{"pages":[],"posts":[{"title":"AWS KMS를 사용하여 Spring 프로퍼티 암호화 하기","text":"AWS KMS를 사용하여 DB의 계정 정보를 암호화하여 Spring에서 사용하는 방법을 알아보겠습니다. 내가 작성한 Spring 코드를 Github에 공개하고 싶은데 민감한 계정 정보, DB 연결 정보는 공개하고 싶지 않을 때 유용하게 사용할 수 있습니다. 1. IAM 유저 생성먼저, AWS IAM - Users에 들어가서 아무 권한을 할당받지 않은 유저를 하나 생성합니다. 이 유저에 우리가 앞으로 생성할 KMS 키에 대한 권한만 부여해줘서, 만약 이 유저의 권한이 탈취되어도 다른 AWS 서비스는 건드리지 못하게 하기 위함입니다. 그리고 아래와 같이 ~/.aws/credentials 파일에 위에서 생성한 유저의 이름과 Access Key와 Secret Key를 입력합니다. ~/.aws/credentials123[kms_test_user]aws_access_key_id = ABCDEFGHIJKLMNOPQRSTaws_secret_access_key = e3wTLBH9MI1a ... jT6C+BjY+D 2. 키 생성AWS KMS - Customer managed keys에 들어가서 Create key 버튼을 클릭합니다. 대칭키를 생성할건지 비대칭키를 생성할건지 선택하는 단계입니다. 일단 대칭키로 선택하고 다음으로 넘어갑니다. 별명, 설명, 태그를 입력하는 단계입니다. 적당한 별명을 하나 적고 다음으로 넘어갑니다. 키를 관리할 유저를 고르는 단계입니다. 아까 생성한 유저를 선택하고 다음으로 넘어갑니다. 키를 사용할 유저를 고르는 단계입니다. 아까 생성한 유저를 선택하고 다음으로 넘어갑니다. 마지막으로 입력한 내용을 리뷰하는 단계를 거치면 키 생성이 완료됩니다. 3. 암호화, 복호화본인이 개발하고 있는 Spring 프로젝트에 AWS KMS SDK를 추가합니다. build.gradle1implementation 'com.amazonaws:aws-java-sdk-kms:1.12.233' 다음과 같은 코드를 사용하면 데이터를 암호화, 복호화 할 수 있습니다. 여기서 profile은 ~/.aws/credentials 파일에 입력해두었던 유저의 이름입니다. KmsUtils.java1234567891011121314151617181920212223242526272829public class KmsUtils { public static String encrypt(String profile, String keyId, String text) { AWSKMS kmsClient = AWSKMSClientBuilder.standard() .withCredentials(new ProfileCredentialsProvider(profile)) .withRegion(Regions.AP_NORTHEAST_2) .build(); EncryptRequest request = new EncryptRequest(); request.withKeyId(keyId); request.withPlaintext(ByteBuffer.wrap(text.getBytes(StandardCharsets.UTF_8))); byte[] cipherBytes = kmsClient.encrypt(request).getCiphertextBlob().array(); return Base64.encodeBase64String(cipherBytes); } public static String decrypt(String profile, String keyId, String cipherBase64) { AWSKMS kmsClient = AWSKMSClientBuilder.standard() .withCredentials(new ProfileCredentialsProvider(profile)) .withRegion(Regions.AP_NORTHEAST_2) .build(); DecryptRequest request = new DecryptRequest(); request.withKeyId(keyId); request.withCiphertextBlob(ByteBuffer.wrap(Base64.decodeBase64(cipherBase64))); byte[] textBytes = kmsClient.decrypt(request).getPlaintext().array(); return new String(textBytes); }} 4. DB 연결 정보 암호화하여 사용하기다음과 같이 properties 파일에 암호화된 값을 입력하여 안전하게 DB 연결 정보를 관리할 수 있습니다. application.properties12345kms.keyId=00000000-0000-0000-0000-000000000000kms.profile=kms_test_userkms.url=IBsYxPm5o6lw37yCx6 ... SbfYupHPnup/vp2ioj5v01SX5Bkms.username=tsa7mB2AAAAYj ... +AgEQgB98PVs+Ui03bAzuhhHJYkms.password=BeAgEAMFkGCSq ... WLqaZZOHusbcdDDOLM3RWCZw== KmsProperties.java123456789@Data@ConfigurationProperties(&quot;kms&quot;)public class KmsProperties { private String keyId; private String profile; private String url; private String username; private String password;} DatabaseConfiguration.java123456789101112131415@Configuration@EnableConfigurationProperties(KmsProperties.class)public class DatabaseConfiguration { @Bean public DataSource dataSource(KmsProperties properties) { String profile = properties.getProfile(); String keyId = properties.getKeyId(); return DataSourceBuilder.create() .url(KmsUtils.decrypt(profile, keyId, properties.getUrl())) .username(KmsUtils.decrypt(profile, keyId, properties.getUsername())) .password(KmsUtils.decrypt(profile, keyId, properties.getPassword())) .build(); }} 5. 특정 IP만 암/복호화 허용하기키를 사용할 때 등록되어 있는 IP에서 온 암/복호화 요청만 허용하게 설정해줄 수 있습니다. AWS KMS 콘솔에서 아까 생성한 키를 클릭하여 자세한 정보 화면으로 들어갑니다. 여기서 Switch to policy view 버튼을 클릭합니다. 그럼 키 정책이 담겨 있는 JSON 파일이 뜹니다. 여기서 다음과 같이 각 Statement 마다 Condition을 추가해줍니다. Key Policy JSON File1234567891011121314151617181920{ &quot;Sid&quot;: &quot;Allow use of the key&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::000000000000:user/kms_test_user&quot; }, &quot;Action&quot;: [ &quot;kms:Encrypt&quot;, &quot;kms:Decrypt&quot;, &quot;kms:ReEncrypt*&quot;, &quot;kms:GenerateDataKey*&quot;, &quot;kms:DescribeKey&quot; ], &quot;Resource&quot;: &quot;*&quot;,+ &quot;Condition&quot;: {+ &quot;IpAddress&quot;: {+ &quot;aws:SourceIp&quot;: &quot;1.2.3.4/32&quot;+ }+ }} 이렇게 설정해 두면 위에서 써놓은 IP 주소에서만 이 KMS 키를 사용할 수 있습니다. Ref 공식문서 - AWS KMS SDK AWS KMS 어렵지 않아요. spring boot에서 aws kms를 이용해 프로퍼티값 암호화 하기","link":"/AWS/AWS-KMS/"},{"title":"Android에 푸쉬 알림 보내기 - 클라이언트편","text":"Node.js와 Google Firebase를 사용하여 Android에 푸쉬 알림 보내는 방법을 알아보겠습니다 (클라이언트 편) 1. 안드로이드 프로젝트 생성 안드로이드 프로젝트를 만드는 법은 굳이 설명하지 않도록 하겠습니다.여기서 저 Package Name은 밑에서 쓰이므로 기억해두세요. 2. Firebase 프로젝트 생성 먼저 Firebase Console로 들어갑니다. 그런 다음 프로젝트 추가 버튼을 누릅니다. 프로젝트 이름을 입력하여 프로젝트를 만듭니다.저는 간단하게 FCM-Example로 했습니다. 3. Firebase 프로젝트에 안드로에드 앱 추가 프로젝트가 생성되고 난 뒤 나타난 화면에서 안드로이드 버튼을 눌러 앱을 추가합니다. 안드로이드 프로젝트를 생성할 때 나왔던 패키지 이름을 입력합니다.앱 닉네임은 적당히 적고 앱 등록을 누릅니다. 그럼 이렇게 구성파일을 다운로드 하라고 하는데 google-services.json 파일을 다운로드 한 다음 아래와 같이 app 폴더 안에 넣어줍니다. 위의 지시사항 대로 Firebase SDK를 gradle을 통해 추가 합니다. 앱을 실행하여 제대로 설정이 되었는지 확인합니다. 앱이 실행이 되면 자동으로 아래와 같이 바뀝니다. 만약 앱을 실행 시켜도 콘솔로 이동하는 버튼이 활성화 되지 않는 다면 몇 번 더 앱을 재실행 시켜주시면 됩니다. 위와 같이 하면 다음과 같이 앱이 제대로 추가 되어 있는 것을 볼 수 있습니다. 4. 안드로이드 프로젝트에 코드 작성1implementation 'com.google.firebase:firebase-messaging:17.3.3' 위의 코드를 통해 fireabse messaging 라이브러리를 설치 합니다 2019-01-02 기준 라이브러리 버전은 17.3.1 입니다. 현재 버전을 알고 싶으면 여기서 확인해 주세요 MyFirebaseInstanceIDService.kt를 만들고 아래의 코드를 입력합니다. MyFirebaseMessagingService.kt123456789101112131415161718192021package kim.hyunsub.fcm_exampleimport com.google.firebase.messaging.FirebaseMessagingServiceimport com.google.firebase.messaging.RemoteMessageclass MyFirebaseMessagingService : FirebaseMessagingService() { override fun onNewToken(token: String?) { super.onNewToken(token) println(&quot;Refreshed token: &quot; + token!!) } override fun onMessageReceived(remoteMessage: RemoteMessage?) { super.onMessageReceived(remoteMessage) remoteMessage?.notification?.let { noti -&gt; println(&quot;title : ${noti.title}&quot;) println(&quot;body : ${noti.body}&quot;) } }} onNewToken(token: String?) 기기의 Firebase Token 값이 변하면(생성되면) 이 메서드가 호출이 됩니다. 이 함수에서 본인의 서버로 token 값을 보내는 코드를 넣어서 이 토큰 값을 가지고 앱에 푸쉬 알림을 보내시면 됩니다. onMessageReceived(remoteMessage: RemoteMessage?) Firebase 서버에서 푸쉬알림을 받으면 이 메서드가 호출이 됩니다. 이 메서드에서 사용자에게 알림을 보내시면 됩니다. AndroidManifest.xmlAndroidManifest.xml의 application태그 안에 아래를 입력합니다. 123456&lt;service android:name=&quot;.MyFirebaseMessagingService&quot;&gt; &lt;intent-filter&gt; &lt;action android:name=&quot;com.google.firebase.MESSAGING_EVENT&quot;/&gt; &lt;/intent-filter&gt;&lt;/service&gt; 위의 코드를 전부 작성하고 앱을 실행하면 이렇게 창에 기기 token 값이 생성되서 나옵니다.기기에 따라 몇 초에서 몇 분정도는 걸릴 수 있습니다. 5. 기기 토큰 값 확인MainActivity.kt1234567891011FirebaseInstanceId.getInstance().instanceId .addOnCompleteListener(OnCompleteListener { task -&gt; if (!task.isSuccessful) { print(&quot;get token failed ${task.exception}&quot;) return@OnCompleteListener } val token = task.result!!.token print(&quot;get token : $token&quot;) }) 토큰 값이 생성된 이후에 다시 기기의 token 값을 알아내려면 위의 코드를 통해 알아내시면 됩니다.","link":"/Android/Firebase-Push-Notification-1/"},{"title":"Android에 푸쉬 알림 보내기 - 서버편","text":"Node.js와 Google Firebase를 사용하여 Android에 푸쉬 알림 보내는 방법을 알아보겠습니다 (서버 편) 1. Firebase 콘솔에서 비공개 키 받기 프로젝트 콘솔페이지에서 설정 - 프로젝트 설정으로 들어갑니다. 서비스 계정 탭으로 이동합니다. 새 비공개 키 생성을 클릭하여 json 파일을 다운로드 받습니다. 2. Node.js 프로젝트에 Firebase 추가1$ npm install firebase-admin --save 위의 명령어를 통해 npm 프로젝트에 firebase 패키지를 설치 합니다. 12345678var admin = require('firebase-admin');var serviceAccount = require('path/to/serviceAccountKey.json');admin.initializeApp({ credential: admin.credential.cert(serviceAccount), databaseURL: 'https://&lt;DATABASE_NAME&gt;.firebaseio.com'}); 위의 코드를 통해 admin을 초기화 합니다. 3. 개별 기기로 메시지 전송123456789101112131415161718192021// Registration Token 은 안드로이드 앱에서 나온 Token 입니다.var registrationToken = 'YOUR_REGISTRATION_TOKEN';// See documentation on defining a message payload.var message = { data: { score: '850', time: '2:45' }, token: registrationToken};// 메시지를 보냅니다.admin.messaging().send(message) .then((response) =&gt; { // Response is a message ID string. console.log('Successfully sent message:', response); }) .catch((error) =&gt; { console.log('Error sending message:', error); }); 성공적으로 완료되면 send() 메소드는 메시지 ID 문자열을 projects/{project_id}/messages/{message_id} 형식으로 반환합니다. 그렇지 않은 경우 오류가 표시됩니다. 설명 및 해결 단계가 포함된 전체 오류 코드 목록은 Admin FCM API 오류를 참조하세요. 자세한 문서","link":"/Android/Firebase-Push-Notification-2/"},{"title":"Android에서 네이버 지도 SDK 사용하기","text":"Android에서 네이버 지도 SDK 사용하기 1. 앱 등록하기 NaverMapExample 이라는 이름으로 프로젝트를 하나 만들었습니다. minimum SDK는 19로 했습니다. 네이버 개발자 센터에 가서 로그인을 하고 어플리케이션 등록을 누릅니다. 어플리케이션 이름은 실제 사용자들이 보게 될 서비스 이름입니다. 뭐 우리는 이 앱을 실제로 서비스하지 않으므로 적당히 입력합니다.사용 API에서 지도(모바일)을 선택하고 Android 환경을 추가한 뒤 안드로이드 앱 패키지 이름을 입력한 뒤 등록합니다. 등록을 하고 나면 위와 같이 Client ID와 Client Secret을 볼 수 있습니다. Client ID만 쓰이므로 Client Secret는 신경 쓰지 말도록 합시다. 2. 준비하기App Level의 build.gradle의 dependencies에 아래의 내용을 추가합나디. 1implementation 'com.naver.maps.open:naver-map-api:2.1.2@aar' AndroidManifest.xml의 manifest 태그 안에 아래의 내용을 추가합니다. 12345&lt;uses-permission android:name=&quot;android.permission.INTERNET&quot; /&gt; &lt;!--반드시 추가--&gt;&lt;uses-permission android:name=&quot;android.permission.ACCESS_FINE_LOCATION&quot;/&gt; &lt;!--위치정보활용시 추가--&gt;&lt;uses-permission android:name=&quot;android.permission.ACCESS_COARSE_LOCATION&quot;/&gt; &lt;!--위치정보활용시 추가--&gt;&lt;uses-permission android:name=&quot;android.permission.ACCESS_WIFI_STATE&quot;/&gt; &lt;!--WIFI 상태활용시 추가--&gt;&lt;uses-permission android:name=&quot;android.permission.CHANGE_WIFI_STATE&quot;/&gt; &lt;!--WIFI 상태활용시 추가--&gt; 3. 네이버 지도 사용하기3.1. Activity에서 네이버 지도 사용하기NMapView : 안드로이드 ViewGroup 클래스를 상속받은 클래스로서 지도 데이터를 화면에 표시합니다. activity_main.xml 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;LinearLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;match_parent&quot; android:orientation=&quot;vertical&quot;&gt; &lt;com.nhn.android.maps.NMapView android:id=&quot;@+id/map_view&quot; android:layout_width=&quot;360dp&quot; android:layout_height=&quot;360dp&quot;/&gt;&lt;/LinearLayout&gt; MainActivity.kt 1234567891011class MainActivity : NMapActivity() { private val clientId = &quot;YOUR_CLIENT_ID&quot; override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) map_view.setClientId(clientId) map_view.isClickable = true }} NMapView는 clickable이 false로 초기화 되어 있으므로 지도 이동 및 확대가 전혀 되지 않습니다. 따라서 isClickable을 true로 만들어 주어야 합니다. 앱을 실행하면 아래와 같이 나옵니다. NMapActivity는 AppCompatActivity를 상속하는 것이 아니므로 툴바가 나오지 않는다는 사실을 알 수 있습니다. 3.2. Fragment에서 네이버 지도 사용하기Fragment에서 NMapView를 사용하려면 아래와 같이 모든 lifecycle 마다 NMapContext를 호출해야 합니다. MainFragment.kt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class MainFragment : Fragment() { private val clientId = &quot;YOUR_CLIENT_ID&quot; private lateinit var mMapContext: NMapContext override fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle?): View? { return inflater.inflate(R.layout.activity_main, container, false) } override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) mMapContext = NMapContext(super.getActivity()) mMapContext.onCreate() } override fun onActivityCreated(savedInstanceState: Bundle?) { super.onActivityCreated(savedInstanceState) map_view.setClientId(clientId) mMapContext.setupMapView(map_view) } override fun onStart() { super.onStart() mMapContext.onStart() } override fun onResume() { super.onResume() mMapContext.onResume() } override fun onPause() { super.onPause() mMapContext.onPause() } override fun onStop() { mMapContext.onStop() super.onStop() } override fun onDestroy() { mMapContext.onDestroy() super.onDestroy() }} 4. 지도 배율 설정하기 앱 실행 화면을 보시면 너무 지도 요소들이 작은 것을 알 수 있습니다. 가져온 지도의 1픽셀을 기기 화면의 1픽셀에 대응시켜서 보여주기 때문입니다. 따라서 NMapView의 다음 메서드를 이용하여 지도를 적절히 확대시켜야 합니다. 1boolean setScalingFactor(float scalingFactor, boolean mapHD) 하지만 여기서의 scalingFactor는 단순히 확대 시키는 비율이므로 화면의 density에 따라 적절히 정해주어야 합니다. 다음 예시를 보시면 이해가 되실것입니다. scalingFactor를 3.0으로 했을 경우 xxhdpi (1920 x 1080) xxxhdpi (2560 * 1440) scalingFactor를 context.resources.displayMetrics.density로 했을 경우 xxhdpi (1920 x 1080) xxxhdpi (2560 * 1440) mapHD의 비교 mapHD = true mapHD = false 5. 지도에 마커 표시하기NMapResourceProvider는 지도 위의 오버레이 객체 드로잉에 필요한 리소스 데이터를 제공하기 위한 추상 클래스입니다. 따라서 지도에 뭔가를 표시하려면 이를 상속하는 클래스를 만들어서 사용해야 합니다. 그러므로 아래 코드를 복사하여 CustomResourceProvider를 만듭니다. 여기 있는 메서드들은 앞으로 차차 채워나갈 것입니다. CustomResourceProvider.kt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class CustomResourceProvider(context: Context): NMapResourceProvider(context) { override fun getLocationDot(): Array&lt;Drawable&gt;? { return null } override fun getDrawableForMarker(p0: Int, p1: Boolean, p2: NMapOverlayItem?): Drawable? { return null } override fun getCalloutBackground(p0: NMapOverlayItem?): Drawable? { return null } override fun getCalloutRightButton(p0: NMapOverlayItem?): Array&lt;Drawable&gt;? { return null } override fun getCalloutTextColors(p0: NMapOverlayItem?): IntArray? { return null } override fun findResourceIdForMarker(markerId: Int, focused: Boolean): Int { return 0 } override fun getCalloutRightButtonText(p0: NMapOverlayItem?): String? { return null } override fun getCalloutRightAccessory(p0: NMapOverlayItem?): Array&lt;Drawable&gt;? { return null } override fun getDirectionArrow(): Drawable? { return null } override fun getParentLayoutIdForOverlappedListView(): Int { return 0 } override fun getOverlappedListViewId(): Int { return 0 } override fun getLayoutIdForOverlappedListView(): Int { return 0 } override fun getListItemLayoutIdForOverlappedListView(): Int { return 0 } override fun getListItemTextViewId(): Int { return 0 } override fun getListItemTailTextViewId(): Int { return 0 } override fun getListItemImageViewId(): Int { return 0 } override fun getListItemDividerId(): Int { return 0 } override fun setOverlappedListViewLayout(listView: ListView, itemCount: Int, width: Int, height: Int) { } override fun setOverlappedItemResource(poiItem: NMapPOIitem, imageView: ImageView) { }} 지도 위에 핀으로 쓸 이미지를 하나 가져옵니다. 저는 대충 24dp 크기 짜리 벡터 이미지를 하나 가져왔습니다. CustomResourceProvider의 findResourceIdForMarker를 아래와 같이 바꿉니다. 123override fun findResourceIdForMarker(markerId: Int, focused: Boolean): Int { return R.drawable.pin} MainActivity의 onCreate를 다음과 같이 수정합니다. MainActivity.kt 12345678910override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) map_view.setClientId(clientId) map_view.isClickable = true map_view.setScalingFactor(resources.displayMetrics.density, true) // 여기 부터 아래 내용 입력 시작} 1val resourceProvider = CustomResourceProvider(this) 지도 위의 오버레이 객체 드로잉에 필요한 리소스 데이터를 제공하는 CustomResourceProvider를 생성합니다. 1val overlayManager = NMapOverlayManager(this, map_view, resourceProvider) 지도 위에 표시되는 오버레이 객체를 관리하는 NMapOverlayManager를 생성합니다. 1val poiData = NMapPOIdata(10, resourceProvider) 지도 위에 표시되는 POI 아이템을 관리하는 NMapPOIdata를 생성합니다. 10은 전체 POI 아이템 갯수를 의미합니다. 1234poiData.beginPOIdata(2)poiData.addPOIitem(126.977983, 37.565568, &quot;서울광장&quot;, 123, 0)poiData.addPOIitem(126.976715, 37.575994, &quot;광화문&quot;, 123, 0)poiData.endPOIdata() NMapPOIdata에 데이터를 넣을 때는 꼭 beginPOIdata를 호출해야 합니다.beginPOIdata 파라미터는 앞으로 넣을 POIItem의 갯수입니다.123은 markderId이고 NMapResourceProvider에서 사용합니다.0은 tag이고 마커를 선택했을 때 호출되는 콜백 인터페이스에서 사용됩니다. 1val poiDataOverlay = overlayManager.createPOIdataOverlay(poiData, null) 여러 개의 오버레이 아이템을 포함할 수 있는 NMapPOIdataOverlay를 만듭니다. 123Handler().postDelayed({ poiDataOverlay.showAllPOIdata(0)}, 400) NMapPOIdataOverlay의 showAllPOIdata를 호출하여 모든 POIItem을 화면에 표시합니다. showAllPOIdata의 파라미터는 지도의 축척을 의미 합니다. 0인 경우에는 모든 마커를 표시하는 적절한 축척을 자동으로 지정하여 보여줍니다. 그런데 이 부분을 바로 실행할 경우 아래와 같이 나와서 Handler를 사용해서 실행했습니다. onStart나 onResume에서 실행해도 마찬가지라서 이와 같이 했습니다. 왜 이런지는 저도 잘 모르겠네요;; Handler를 사용한 경우 Handler를 사용하지 않은 경우","link":"/Android/Naver-Map/"},{"title":"개발자를 위한 비트코인 해부학","text":"개발자의 시각에서 정확한 비트코인의 동작 방식을 알아보겠습니다. 비트코인이란?네트워크로 연결된 여러 노드에서 동일한 송금 장부를 유지하고 작성하는 컴퓨터 프로그램블록체인이란?송금 장부를 저장하는 방식1. 비트코인 노드비트코인 노드는 비트코인 데몬을 실행하고 있고, 비트코인의 모든 송금 장부(=블록체인)을 저장하고 있는 컴퓨터를 의미합니다. 트랜잭션을 생성하고, 이렇게 생성된 트랜잭션을 모아서 Block을 만들고(채굴), 이렇게 만들어진 Block을 검증하고, 승인하는 주체입니다. 따라서 전체 비트코인 네트워크에서 각각의 노드는 어느 Block을 거절하고 승인할 지 결정할 수 있는 민주 시민의 역할을 하고 있다고 볼 수 있습니다. 현재 비트코인 블록체인 크기2022년 1월 기준 비트코인 전체 블록체인의 크기는 385GB입니다. 비트코인 데몬을 실행하면 모든 블록체인이 다른 비트코인 노드로 부터 다운로드 됩니다. 여기에서 실시간 그래프를 볼 수 있습니다. 사람들이 노드를 유지하는 이유이렇게 방대한 양의 블록체인을 가지고 있는 노드를 사람들은 왜 유지하고 있을까요? 노드를 유지하고 있는 사람을 크게 3가지로 나눌 수 있습니다. (출처) 1) 송금, 채굴 중개 사업자송금, 채굴등 모든 비트코인 관련 기능은 노드 밖에 할 수 없습니다. 다르게 말하면 비트코인의 모든 블록체인을 다운로드 받지 않는 이상 송금이나 채굴을 할 수 없다는 것입니다. 비트코인을 송금하려는 모든 사람이 수 백 GB에 달하는 모든 블록체인을 가지고 있기에는 무리가 있기 때문에, 송금을 중개를 해주는 서비스가 존재하고 있고, 이런 서비스들이 노드를 유지하고 있습니다. 2) 비트코인 부자들하지만 이런 송금 중개를 해주는 서비스들은 기술적으로 사용자의 IP를 알아낼 수 있습니다. 또한 송금을 하기 위해서는 Private Key의 사용이 필수적입니다. 따라서 많은 양의 비트코인을 소유하고 있는 사용자는 해킹의 위협 때문에 믿을 수 없는 송금 중개 서비스의 사용을 꺼릴 수 있습니다. 이들은 독자적인 노드를 유지하여 개인 정보의 보안을 유지하고자 할 것입니다. 3) 열렬한 비트코인 신봉자들수 백 GB이 데이터는 어찌보면 하드디스크 하나에 다 들어갈 수 있을 만큼 작다고 볼 수 있습니다. 따라서 한 개인이 유지하고 있기에는 그렇게 부담스러운 크기는 아닙니다. 탈중앙화 금융에 매료된 사람들이나 블록체인에 대하여 연구하는 사람들은 범지구적인 블록체인 네트워크의 일원이 되고 싶어할 수도 있습니다. DNS Seeds처음 비트코인 데몬을 실행하면 그동안 쌓여왔던 블록체인을 다른 노드로 부터 다운로드 받으려고 할 것입니다. 그러기 위해서는 인터넷에 존재하는 다른 노드의 IP 주소를 알아야 합니다. 비트코인은 다른 노드의 IP 주소를 DNS Query를 이용하여 얻어오는데, 이 때 사용하는 DNS를 DNS Seeds라고 부릅니다. 이러한 DNS Seeds는 비트코인 코드에 하드코딩 되어 있으며 Github 코드 에서 확인할 수 있습니다. 2. 송금 (Transaction)송금은 특별한 과정이 필요하지 않습니다. 그냥 철수가 영희에게 50 비트코인을 송금한다고 다른 노드에게 알려주고, 그 정보를 수신한 어떤 노드가 송금 정보(=Transaction)를 포함하고 있는 Block을 만들어주기만 하면 됩니다. Memory Pool각 노드들은 네트워크에 연결되어 있는 다른 노드로 부터 트랜잭션 정보를 전달 받습니다. 이렇게 전달 받았지만 아직 Block으로 생성되지 않은 트랜잭션을 임시로 저장해두는 장소를 Memory Pool이라고 합니다. 실시간 Memory Pool을 이 사이트에서 볼 수 있습니다. 트랜잭션의 승인 (Confirmation)Memory Pool에 있는 트랜잭션은 아직 완료된 것이 아닙니다. 즉, 아직 비트코인이 완전히 상대방에게 넘어간 것이 아닙니다. 이 트랜잭션이 포함된 Block이 생성되어야 합니다. 이 트랜잭션이 포함된 Block이 생성되면 한 번 승인 되었다라고 표현합니다. 그 뒤에 Block이 하나 더 연결되면 두 번, 그 뒤에 Block이 하나 더 연결 되면 세 번, 이런식으로 우리는 어떤 트랜잭션이 몇 번 승인 되었다고 말할 수 있습니다. 금액에 따라 송금이 완료되었다고 판단할 만한 승인 횟수는 다릅니다. 적은 금액은 한 두번이면 충분하지만, 많은 금액은 6번은 필요합니다. 어떤 금액에 몇 번 승인이 필요한지는 각 노드마다 다르게 판단합니다. 비트코인 계좌 (지갑)비트코인 데몬은 따로 계좌 목록 데이터를 가지고 있지 않습니다. 때문에 명시적으로 계좌를 만들어서 등록하는 과정이 없습니다. 그냥 랜덤으로 생성한 문자열을 하나 만들고 이 문자열에 누군가 송금해주면 해당 계좌가 세상에 알려지는 겁니다. 그리고 비대칭 암호화를 이용한 디지털 서명 기술을 이용하여 해당 계좌의 돈을 함부로 인출할 수 없게 해놨습니다. 비트코인은 랜덤한 32byte 값(Private Key)을 생성한 다음, Secp256k1 라는 타원곡선 암호(Elliptic curve cryptography)를 이용하여 Public Key를 생성하는 방식을 사용합니다. 여기서 생성한 Public Key가 계좌 번호이고 Private Key가 계좌 비밀번호입니다. UTXO 비트코인은 ‘계좌 잔액’ 이란 개념이 없습니다. ‘다 쓴 금액’과 ‘아직 안 쓴 금액’만 있을 뿐입니다. 위 그림에서 빨간색이 ‘다 쓴 금액’ 파란색이 ‘아직 안 쓴 금액’을 의미합니다. UTXO는 아직 안 쓴 금액을 의미합니다. 위 그림과 같이 트랜잭션에서 보내는 금액(Input)은 정확히 예전에 받았던 금액(Output)과 일치합니다. 각 Input은 해당 금액을 받았던 트랜잭션의 ID를 가지고 있어서 돈의 출처를 알 수 있습니다. 위 그림을 보면 알 수 있듯이 어떤 금액을 보내기 위해서는 그보다 많은 총액을 가지고 있는 Input들을 사용합니다. 이 트랜잭션에서 사용된 모든 Input들은 ‘다 쓴 금액’ 처리가 되고, 보내고 남은 금액은 또 다른 Output으로 생성됩니다. 비트코인 데몬은 어떤 계좌가 어떤 UTXO를 가지고 있는지 인덱싱 해둔 데이터를 가지고 있습니다. 이 것을 UTXO Set라고 부릅니다. 트랜잭션을 빠르게 검증하기 위해서 사용합니다. UTXO Set은 비트코인 데몬을 처음 실행하면 다른 노드로부터 블록체인 데이터를 받을 때 생성합니다. 2022년 1월 기준 UTXO 수는 8천만개에 달하고, UTXO Set의 데이터 양은 4.5GB 정도 합니다. 여기에서 UTXO의 수와 UTXO Set의 데이터 양을 실시간 그래프로 볼 수 있습니다. 3. Block에 담긴 데이터 최초 Block의 데이터를 살펴보면서 비트코인 Block에는 정확히 어떤 데이터가 담겨있나 알아보겠습니다. 여기 나 여기 를 보면 비트코인 최초 Block에 어떤 데이터가 담겨있는지 자세히 알 수 있습니다. 각 값의 이름과 의미 이름 값 Hash 각 Block을 식별할 수 있는 고유 값. 모든 Block은 서로 다른 Hash를 가지고 있습니다. Confirmations 이 Block 이후에 블록체인에 생성된 Block의 수 Timestamp Block이 생성된 시간 Height 이 Block이 몇 번째 Block인지를 나타내는 값 Difficulty 밑에서 자세히 설명 Merkle root 밑에서 자세히 설명 Version 이 Block의 버전. 버전에 따라 Block에는 다른 형식의 데이터가 담겨 있습니다. Bits 밑에서 자세히 설명 Weight 2017년에 새롭게 등장한 값으로 Block의 크기를 의미. 추후에 자세히 설명 Size Block의 바이트 크기 Nonce 밑에서 자세히 설명 Transaction Volume 트랜잭션의 총 비트코인 양 Block Reward 이 Block을 채굴한 노드에게 주어진 보상 Fee Reward 모든 트랜잭션의 수수료 합 Merkle Root Merkle Root를 알기 위해서는 먼저 Merkle Tree를 알아야합니다. Merkle Tree는 Binary Hash Tree라고도 불리웁니다. 방대한 데이터의 무결성을 빠르게 검증하기 위해서 생겨난 자료 구조입니다. Merkle Tree의 Leaf는 어떤 한 데이터의 Hash 값이고, 각 부모 노드들은 두 자식 노드를 더한 값의 Hash 값입니다. 이런식으로 각 노드를 합치는 것을 반복한 후 남은 하나의 Hash값을 Merkle Root라고 부릅니다. 비트코인에서는 한 Block 안에 있는 모든 트랜잭션을 요약하기 위한 용도로 쓰입니다. 비트코인에서 Merkle Tree의 Leaf 값들은 특정 트랜잭션을 두 번 SHA256을 한 값입니다. 하나의 트랜잭션을 조금만 수정해도 Merkle Root는 크게 달라지므로, Merkle Root 값을 가지고 Block 안에 있는 모든 트랜잭션의 무결성을 보장할 수 있습니다. 그냥 모든 Transaction을 한 번에 Hash하면 되지 왜 이런 방식을 쓸까요? 만약에 그런 방식을 사용한다면 특정 트랜잭션을 검증하기 위해서 같은 Block에 있는 다른 모든 트랜잭션의 데이터가 필요합니다. 하지만 Merkle Tree를 사용한다면 위 그림과 같이 몇 개의 Hash 값만 가지고 손 쉽게 특정 트랜잭션을 검증할 수 있게 됩니다. 출처1, 출처2 4. Block 생성 (채굴)Memory Pool에서 Block으로 만들기를 원하는 트랜잭션을 가지고 적절한 Hash 값만 찾아내면 바로 Block을 생성해 낼 수 있습니다. Block은 그냥 각 값들을 열거해 놓은 데이터 모음에 불과하기 때문입니다. 하지만 이 Hash 값을 생성해 내는 것이 어렵습니다. Block Hash 생성 방법위에서 언급된 바와 같이 Block Hash는 무작위로 생성된 값이 아닙니다. 아래의 6개의 값을 바탕으로 생성합니다. 이름 설명 크기 Version Block의 버전 4 바이트 PrevBlockHash 이전 Block의 해시 값 32 바이트 MerkleRoot Merkle Root 값 32 바이트 Time 1970-01-01T00:00 UTC 부터 지금까지 지난 시간 (단위: 초) 4 바이트 Bits Block 생성 난이도를 조절하는 값으로 밑에서 자세히 설명 4 바이트 Nonce 조건을 Hash값을 생성하기 위해 조절되는 값으로 밑에서 자세히 설명 4 바이트 이렇게 총 80바이트를 두 번 SHA256 해서 생성해 냅니다. hash = SHA256(SHA256(Version + PrevBlockHash + MerkleRoot + Time + Bits + Nonce)) Target그런데 비트코인은 정말 특이한 규칙을 가지고 있습니다. Block Hash는 특정 target 값 보다 작은 값을 가지고 있어야 한다는 것입니다. 예를 들어 target이 0x010000 인 경우 0x012345은 target 보다 큰 값이므로 Hash로 사용할 수 없고 0x001234은 target 보다 작은 값이므로 사용할 수 있습니다. 이 target 값은 Block Hash를 만들 때 쓰이는 값 중에 하나인 Bits 값을 통해 다음 규칙에 따라 만들어 집니다. x = Bits의 1번째 바이트y = Bits의 2 ~ 4번째 바이트target = y * (1 &lt;&lt; (8 * (x - 3))) ex)Bits = 0x12345678x = 0x12y = 0x345678target = 0x0000000000000000000000000000345678000000000000000000000000000000 이렇게 계산된 target 값 보다 Block Hash 값이 작아야만 합니다. Nonce이렇게 특정 target 값보다 Block Hash 값을 작게 만들어 주기 위해서 Nonce 값을 바꿔가며 일일히 Block Hash를 계산합니다. Nonce 값은 4바이트 이므로 총 4,294,967,296 개의 후보가 있습니다. 이런 조건을 만족하는 Nonce를 찾는 것이 바로 채굴입니다.사실 채굴은 어려운 수학문제를 푼다기 보다는 많은 복권을 긁는 행위에 더 가까운 것입니다!따라서 복잡한 단일 연산을 빠르게 처리하는 CPU보다 간단한 연산을 병렬로 처리하는 것에 특화된 GPU가 채굴 부품으로써 각광을 받게 된 것입니다. Nonce의 모든 후보를 체크해봐도 target 보다 작은 Hash 값을 찾을 수 없을 수도 있습니다. 이런 경우에는 Block 생성 시간을 의미하는 Time 값을 변경하거나, 트랜잭션 몇 개를 memory pool에서 바꿔서 MerkleRoot 값을 변경해가며 다시 조건을 만족하는 Nonce 값을 찾습니다. 이 gist 코드는 이러한 Hash 생성 방법을 Python3으로 작성한 것입니다. 출처 Hash Rate 해시 레이트는 1초에 얼마나 많은 해시 연산을 할 수 있는가에 대한 척도입니다. 1GH/s 는 1초에 10억 번 해시 연산을 할 수 있다는 의미입니다. 보통 이 단위로 채굴 기기들의 성능을 다룹니다. 여기에 따르면 RTX3090의 비트코인 해시레이트는 4.85 GH/s 라고 합니다. 위 그림은 비트코인 네트워크에 연결된 모든 노드의 해시 레이트 추이를 나타냅니다. 2022년 1월 현재 전체 해시 레이트는 200EH/s입니다. 1초에 200,000,000,000,000,000,000 번 해시 연산을 할 수 있다는 뜻입니다! 여기에서 비트코인 전체 해시 레이트를 실시간으로 볼 수 있습니다. Difficulty Difficulty는 얼마나 Block 생성이 어려운지, 즉, 조건을 만족하는 Hash를 찾는 것이 얼마나 어려운지를 측정하는 척도입니다. 아래와 같이 계산합니다. Difficulty = 최초 Block의 target 값 / 현재 Block의 target 값 최초 Block의 Bits 값은 0x1D00FFFF 이고, 2022년 1월 기준 현재 Bits 값은 0x170B8C8B 이므로, 현재 Difficulty는 24,371,874,614,345 입니다! 비트코인이 처음 생겼을 때 보다 Block을 생성하는 것이 24조배 어려워졌다는 것을 의미합니다. 비트코인은 Block을 10분에 한 번 생성되는 것을 지향하기 때문에 이 Difficulty 값을 조절해서 Block 생성 간격을 일정하게 만듭니다. Miner가 많이 몰려서 Block이 10분 보다 빨리 생성되면 Difficulty를 올립니다. 그리고 그 반대의 경우도 마찬가지 입니다. 하지만 Difficulty가 1보다는 작아지지 않게 설정되어 있습니다. Difficulty 값은 매 2016 Block마다 조절됩니다. 2016이라는 숫자는 2주 동안 생성되는 Block의 수 입니다. Difficulty 값을 2주 / 직전 2016개의 Block이 생성되는데 든 총 시간 으로 조정하고 Difficulty 값을 통해 다음 2016개의 Block의 target 값을 정합니다. 여기에서 현재 Difficulty 값을 실시간으로 볼 수 있습니다. 출처1, 출처2, 출처3 Mining Pool의 등장 남들보다 더 빠르게 조건을 만족하는 Hash 값을 찾아야 채굴 보상을 받을 수 있는 비트코인 특성상 가능한 많은 컴퓨팅 자원을 가지고 있는 사람이 유리합니다. 그래서 사람들은 힘을 합쳐서 Nonce 값을 구간 별로 나눠서 조건에 맞는 Hash를 찾습니다. 보통 채굴한다는 사람들은 전부 이런 Mining Pool에서 Nonce 값 범위를 지정 받아서 조건에 맞는 Hash를 찾는 것입니다. 따라서 모든 Blockchain을 다운로드 받을 필요도 없고, 휴대폰이나 웹 브라우저로도 채굴이 가능한 것입니다. 무조건 강력한 컴퓨팅 자원을 가지고 있는 Mining Pool만 채굴 보상을 독점하는 것도 아닙니다. 채굴이라는 행위 자체가 복권을 긁는 것과 같아서 약소한 Mining Pool도 우연히 강력한 Mining Pool 보다 조건에 맞는 Hash를 빨리 찾을 수 있기 때문입니다. 각 Mining Pool 마다 채굴 보상을 나누는 규칙이 다릅니다. 자세한 정보는 이 글을 읽어 주세요. 여기에서 실시간으로 각 Mining Pool의 성능과 점유율을 확인할 수 있습니다. 현재 채굴 수익성 현재 그래픽카드 최고 라인업 중에 하나인 RTX3090을 1000대 샀다고 가정해 봅시다. 여기에 따르면 RTX3090의 비트코인 해시레이트는 4.85 GH/s 라고 합니다. 따라서 1000대의 총 해시레이트는 4.85 TH/s 이고, 이 값을 이 사이트에 입력하면 예상 수익을 계산할 수 있습니다. 위 사진에서 볼 수 있듯이 전기세 무료에, Mining Pool 수수료를 전혀 내지 않는다고 가정해도 한 달에 31달러밖에 벌 수 없습니다 (2022년 1월 기준). 비트코인의 채굴 난이도는 시간이 지날 수록 점점 높아지니, 비트코인의 가격이 많이 오르지 않는 이상 이 금액은 점점 줄어들겁니다. 반감기 비트코인은 21만개의 Block이 생성될 때 마다 채굴 보상이 반으로 줄어들게 설계되어 있습니다. 이걸 시간으로 환산하면 대략 4년정도 입니다. 처음에는 채굴 보상이 50 BTC이었지만 지금은 3번의 반감기를 거쳐 6.25 BTC로 줄어들었습니다. 주기적으로 채굴 보상이 줄어드니 시간이 아무리 지나도 시중에 2100만개 이상의 비트코인이 풀리지 않을 것을 알 수 있습니다. 2022년 1월 현재 약 1900만개의 비트코인이 시중에 존재하고 있습니다. 5. Block File Format출처1, 출처2, 출처3, Github 비트코인 최초 Block은 파일에 정확히 아래와 같이 저장되어 있습니다. 12345678910111213141516171819F9 BE B4 D9 1D 01 00 00 01 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00 00 3B A3 ED FD7A 7B 12 B2 7A C7 2C 3E 67 76 8F 61 7F C8 1B C388 8A 51 32 3A 9F B8 AA 4B 1E 5E 4A 29 AB 5F 49FF FF 00 1D 1D AC 2B 7C 01 01 00 00 00 01 00 0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00 00 00 00 FF FFFF FF 4D 04 FF FF 00 1D 01 04 45 54 68 65 20 5469 6D 65 73 20 30 33 2F 4A 61 6E 2F 32 30 30 3920 43 68 61 6E 63 65 6C 6C 6F 72 20 6F 6E 20 6272 69 6E 6B 20 6F 66 20 73 65 63 6F 6E 64 20 6261 69 6C 6F 75 74 20 66 6F 72 20 62 61 6E 6B 73FF FF FF FF 01 00 F2 05 2A 01 00 00 00 43 41 0467 8A FD B0 FE 55 48 27 19 67 F1 A6 71 30 B7 105C D6 A8 28 E0 39 09 A6 79 62 E0 EA 1F 61 DE B649 F6 BC 3F 4C EF 38 C4 F3 55 04 E5 1E C1 12 DE5C 38 4D F7 BA 0B 8D 57 8A 4C 70 2B 6B F1 1D 5FAC 00 00 00 00 F9 BE B4 D9 비트코인 Block은 Script 필드를 제외한 모든 필드가 Little-Endian 으로 저장되어 있습니다. variable bytes는 다음 규칙으로 값을 읽습니다 첫 바이트가 253보다 작을 경우, 그냥 그 값을 읽습니다. 첫 바이트가 253일 경우, 그 다음 바이트 값을 읽습니다. 첫 바이트가 254일 경우, 그 다음 4바이트 값을 읽습니다. 첫 바이트가 255일 경우, 그 다음 8바이트 값을 읽습니다. 이를 해석하면 다음과 같습니다 Magic Number0xD9B4BEF9 으로 하드코딩 되어 있습니다. 테스트 네트워크의 경우에는 0xDAB5BFFA 입니다. Github 코드에서 확인할 수 있습니다. Block을 파일 시스템에 저장할 때, 한 파일에 한 Block을 저장하는 것이 아니라 여러 Block을 저장하기 때문에 각 Block을 구분하기 위하여 고안되었습니다. 일반적으로 한 파일이 128MB가 넘어가면 다음 Block 부터는 다른 파일에 저장합니다. 출처1, 출처2 Lock Time해당 트랜잭션이 언제부터 valid 한지를 나타내는 필드입니다. 즉 예약 송금을 할 수 있다는 의미입니다. 이 시기가 되기 전까지는 해당 트랜잭션은 Memory Pool에만 있고, 새로 채굴되는 블럭에 포함될 수 없습니다. 값 의미 0 즉시 송금 &lt; 500,000,000 이 숫자 만큼의 블록이 쌓인 후 송금 &gt;= 500,000,000 해당 날짜가 지난 후 송금. 1970-01-01T00:00 UTC 부터 지금까지 지난 시간(초)를 의미합니다. 출처 Sequence예약 송금을 한 후, 송금 금액을 변경하고 싶을 수 있습니다. 그럴 때는 원래 트랜잭션보다 더 높은 Sequnce 값을 가지고 있는 새로운 트랜잭션을 다른 노드에 전파하면 기존의 예약된 트랜잭션을 덮어쓸 수 있습니다. 일반적으로 예약 송금을 하지 않을 때는 Lock Time 값을 0으로, Sequnce 값을 MAX_INT 값으로 지정해 놓습니다. 추가적인 수수료 없이 다른 노드에 전파된 트랜잭션을 마음껏 수정할 수 있다 보니, 악의적인 사용자가 이 기능을 이용하여 막대한 트래픽을 발생시켜서 비트코인 네트워크를 마비시킬 수 있습니다. 따라서 중간에 기능을 막아두었다가, 트랜잭션을 수정할 때 수수료를 더 내야하는 구조로 변경되었습니다. 이를 Opt-in RBF 라고 합니다 더 자세한 정보는 이 글과 이 글을 읽어주세요 Appendix 사토시 나카모토가 쓴 비트코인 논문 링크 Mastering Bitcoin - Andreas M. Antonopoulos (O’Reilly) Mastering Bitcoin - Andreas M. Antonopoulos (Github)","link":"/Blockchain/Bitcoin/"},{"title":"Docker Hub에 도커 이미지 배포해보기","text":"Github Repository와 연동하여 나만의 Docker Image를 Docker Hub에 배포하는 법을 알아보도록 하겠습니다. 1. Repository 생성1.1. Docker Hub Repository 생성Docker Hub에 접속하여 회원가입을 합니다. 회원 가입 후 상단의 Repositories 탭에 들어간 후 Create Repository 버튼을 클릭합니다. Repository 이름은 docker-hub-test로 하고 Visibility는 Public, Build Settings는 건들지 않고 Create 버튼을 클릭합니다. 1.2. Github Repository 생성Github에 docker-hub-test라는 이름의 레파지토리를 하나 생성한 후 아래의 Dockerfile과 README.md 파일을 넣습니다. Dockerfile 12FROM alpineENTRYPOINT echo &quot;hello, world!&quot; README.md 1README!!! 2. Docker Hub와 Gitub 연동 레파지토리에서 Builds 탭에 들어가서 Link to GitHub 버튼을 누릅니다. Github 탭의 Connect 버튼을 누릅니다. 버튼을 누르면 Github에 로그인 하라는 화면이 뜰텐데 로그인 한 후 Authorize Docker Hub Builder 페이지가 나오면 Authorize 버튼을 눌러 권한을 승인해 줍니다. 이런 과정을 통해 Docker Hub가 내 Github Repository에 접근하여 Push가 발생할 때마다 Docker Image를 빌드하여 Docker Hub에 푸시할 수 있습니다. 다시 레파지토리에서 Builds 탭에 들어가면 위와 같이 Connected 라고 뜰텐데 이 버튼을 클릭합니다. Source Repository는 우리가 방금 만들었던 Github Repository를 선택합니다. Autotest는 Docker Hub가 Github Repository에 Pull Request가 들어왔을 경우 설정한 Shell Script를 실행해 테스트를 할 수 있게 해주는 것입니다. 일단 Off로 설정해둡니다. 자세한 정보는 문서를 참고해주세요. Repository Link는 Dockerfile의 FROM에 적혀있는 베이스 이미지가 공식 이미지가 아닐 경우 이 이미지가 업데이트 되면 다시 빌드하게 해주는 것입니다. 일단 Off로 설정해둡니다. 이 단계에서 가장 중요한 Build Rule을 설정해야 합니다. 이 설정을 통해 언제 어떻게 이미지 태그를 붙일것인지 설정할 수 있습니다. 위의 시나리오를 참고하여 설정해 주길 바랍니다. 밑의 Build Environment Variable을 설정하여 빌드할 때 환경 변수를 설정할 수 있습니다. 설정이 다 끝난 후 Save and Build 버튼을 눌러서 이미지를 빌드하도록 하겠습니다. 이미지 빌드가 끝난 후 위의 화면에서 SUCCESS 라고 뜨면 우리의 첫 도커 이미지가 세상에 나온 것입니다! 3. 실행해보기이제 Docker가 설치된 아무 컴퓨터에서 아래의 명령어를 입력하여 우리의 도커 이미지가 잘 실행되는지 알아봅시다. 1234567$ docker run [username]/docker-hub-testUnable to find image '[username]/docker-hub-test:latest' locallylatest: Pulling from [username]/docker-hub-testaad63a933944: Already existsDigest: sha256:3522bf472b4bc691a7b967ea5d32a94fdb8797202b126bffd494fe8c93b3e6aaStatus: Downloaded newer image for [username]/docker-hub-test:latesthello, world! 위와 같이 도커 이미지를 다운로드 받은 후 hello, world!가 뜨면 성공입니다. 4. Docker Hub Readme위와 같이 Github 레파지토리와 연동하여 이미지를 푸쉬할 경우 Docker Hub는 자동적으로 Dockerfile이 있는 폴더 에서 README.md 파일을 가져와 Docker Hub 레파지토리의 Readme에 덮어씌웁니다. Docker Hub의 레파지토리에 가시면 우리가 Github에 올려뒀던 README.md 파일의 내용이 뜨는 것을 볼 수 있습니다.","link":"/Docker/Docker-Hub/"},{"title":"MySQL Backup Docker Image","text":"mysqldump, AWS CLI, crontab을 이용하여 주기적으로 MySQL을 백업하는 Docker Image를 만들어 보겠습니다. backup.sh123TODAY=$(date +&quot;%Y-%m-%d&quot;)mysqldump -h database -u $MYSQL_USER -p$MYSQL_PASSWORD $MYSQL_DATABASE &gt; &quot;$MYSQL_DATABASE-$TODAY.sql&quot;aws s3 cp &quot;$MYSQL_DATABASE-$TODAY.sql&quot; s3://my-s3-bucket-name Dockerfile123456789101112131415161718192021222324FROM python:3.7-alpineWORKDIR /rootRUN apk update# TimezoneRUN apk add --no-cache tzdataENV TZ='Asia/Seoul'# aws cliRUN pip3 install awscli --upgrade --userRUN ln -s /root/.local/bin/aws /usr/bin/aws# mysqldumpRUN apk add --no-cache mysql-client# crontabRUN echo &quot;0 6 * * * /root/backup.sh&quot; | crontab -ADD backup.sh /root/backup.shRUN chmod +x /root/backup.shENTRYPOINT crond -f -L /dev/stdout Docker Command12345678$ docker run -d \\ -e MYSQL_USER=[MySQL User] \\ -e MYSQL_PASSWORD=[MySQL Password] \\ -e MYSQL_DATABASE=[MySQL Database] \\ -e AWS_ACCESS_KEY_ID=[AWS Access Key ID] \\ -e AWS_SECRET_ACCESS_KEY=[AWS Secret Access Key] \\ -e AWS_DEFAULT_REGION=[AWS Default Region] \\ mysql-backup","link":"/Docker/Mysql-Backup/"},{"title":"Server-Sent Event","text":"Nodejs에서 Sever-Sent Event를 사용하는 법을 알아보겠습니다. 또한 Nginx를 통해 통신할 경우 추가로 해주어야 하는 설정을 알아보겠습니다. 1. 시작1.1. ssestream 설치1$ npm install ssestream 1.2. Server-Side서버에 아래의 코드를 작성합니다. 12345678910111213141516171819import * as express from 'express';import SSEStream from 'ssestream';const app = express();app.get('/sse', (req, res) =&gt; { res.setHeader('Access-Control-Allow-Origin', '*'); const stream = new SSEStream(req); stream.pipe(res); setInterval(() =&gt; { stream.write({ data: Date.now().toString(), }); }, 100);});app.listen(8080); 여기서 10번째 줄에 data에는 string을 넣어도 되고 object를 넣어도 됩니다. ssestream 내부에서 object를 JSON.stringify 함수를 통해 string으로 바꿔줍니다. 1.3. Client-Side1234const es = new EventSource('http://localhost:8080/sse');es.onmessage = (event) =&gt; { console.log(event);} 2. Cookie 전송 [Client-Side] EventSource 생성할 때 옵션으로 { withCredentials: true }를 넣어줍니다. [Server-Side] Access-Control-Allow-Origin 헤더를 *이 아닌 특정 도메인으로 설정합니다. [Server-Side] Access-Control-Allow-Credentials 헤더의 값을 true로 해줍니다. 3. 각 이벤트마다 ID 부여서버에서 SSEStream에 write할 때 id를 넣어주면 됩니다. 1234stream.write({ id: Date.now().toString(), data: 'Hello, World!',}); 클라이언트에서는 event.lastEventId를 통해 접근합니다. 123es.onmessage = (event) =&gt; { console.log(event.lastEventId, event.data);} 4. 각 이벤트마다 타입 부여서버에서 SSEStream에 write할 때 event를 넣어주면 됩니다. 12345stream.write({ id: Date.now().toString(), data: 'Hello, World!', event: (Math.random() &gt; 0.5) ? 'event1' : 'event2',}); 클라이언트에서는 onmessage가 아닌 addEventListener를 통해 이벤트를 받습니다. 123456es.addEventListener('event1', (event) =&gt; { console.log('event1', event.lastEventId, event.data);});es.addEventListener('event2', (event) =&gt; { console.log('event2', event.lastEventId, event.data);}); 5. Client에서 연결 종료1es.close(); 서버에서 Client가 연결을 종료한 것을 알아내려면 아래와 같이 하면 됩니다. 123req.socket.on('close', () =&gt; { // 연결 종료}); Server에서도 연결을 종료할 시 위의 콜백함수가 호출되는 한계가 있습니다. 6. Server에서 연결 종료123req.socket.end();// 또는res.socket.end(); Client에서 서버가 연결을 종료한 것을 알아내려면 아래와 같이 하면 됩니다. 123456es.onerror = (event) =&gt; { if (event.eventPhase === es.CLOSED) { es.close(); // 연결 종료 }} 여기서 es.close()를 하지 않으면 Client에서 무한정으로 Server에 접속 시도를 하게 됩니다. 7. Nginx를 통해 통신 (proxy_pass를 사용하는 경우)1proxy_buffering off; 를 location block에 추가해주면 됩니다. 또는 1X-Accel-Buffering: no 를 Server에서 보내는 Response에 헤더로 추가하면 됩니다.","link":"/Node/Server-Sent-Event/"},{"title":"ESLint + Typescript","text":"Typescript 프로젝트에서 airbnb 스타일로 eslint를 적용해 보자! 1. ESLint 설치npm install -D eslint 명령어로 간단히 eslint를 설치할 수 있습니다. npx eslint --init 명령어로 .eslintrc.json 을 생성할 수 있습니다. 만약 React 프로젝트일 경우 그에 맞게 선택해 주시면 됩니다. 제가 선택한 옵션은 다음과 같습니다. 12345678910111213141516171819202122232425262728293031323334353637383940$ npx eslint --init? How would you like to use ESLint? To check syntax only To check syntax and find problems ❯ To check syntax, find problems, and enforce code style? What type of modules does your project use? JavaScript modules (import/export) CommonJS (require/exports) ❯ None of these? Which framework does your project use? React Vue.js ❯ None of these? Does your project use TypeScript? (y/N) y? Where does your code run? (Press &lt;space&gt; to select, &lt;a&gt; to toggle all, &lt;i&gt; to invert selection)❯◯ Browser ◉ Node? How would you like to define a style for your project? ❯ Use a popular style guide Answer questions about your style Inspect your JavaScript file(s)? Which style guide do you want to follow?❯ Airbnb: https://github.com/airbnb/javascript Standard: https://github.com/standard/standard Google: https://github.com/google/eslint-config-google? What format do you want your config file to be in? JavaScript YAML ❯ JSON@typescript-eslint/eslint-plugin@latest eslint-config-airbnb-base@latest eslint@^5.16.0 || ^6.8.0 eslint-plugin-import@^2.20.1 @typescript-eslint/parser@latest? Would you like to install them now with npm? (Y/n) Y 그러면 아래와 같은 .eslintrc.json 파일이 생성됩니다. 123456789101112131415161718192021{ &quot;env&quot;: { &quot;es6&quot;: true, &quot;node&quot;: true }, &quot;extends&quot;: [ &quot;airbnb-base&quot; ], &quot;globals&quot;: { &quot;Atomics&quot;: &quot;readonly&quot;, &quot;SharedArrayBuffer&quot;: &quot;readonly&quot; }, &quot;parser&quot;: &quot;@typescript-eslint/parser&quot;, &quot;parserOptions&quot;: { &quot;ecmaVersion&quot;: 2018 }, &quot;plugins&quot;: [ &quot;@typescript-eslint&quot; ], &quot;rules&quot;: {}} 다음과 같은 명령어로 eslint를 실행할 수 있습니다. 123$ npx eslint 'src/**/*.ts' # 단순히 문법 검사만 함$ npx eslint 'src/**/*.{ts,tsx}' # tsx 파일도 검사함$ npx eslint 'src/**/*.ts' --fix # 고칠수 있는 부분은 고치고 고칠 수 없는 부분은 출력함 2. Typescript용 airbnb 스타일 설치하기실제로 eslint 명령어를 수행하면 Type을 import한 부분에 대해서는 사용하지 않았다는 아래와 같은 에러를 출력합니다. 123import { SomeType } from 'some-package';const something: SomeType | null = null; 1error 'SomeType' is defined but never used no-unused-vars 이러한 에러가 발생하는 이유는 .eslintrc.json를 생성할 때 설치된 airbnb 스타일이 Typescript용이 아니기 때문입니다. 따라서 Typescript용 airbnb 스타일을 설치 해야 합니다. 2.1. React 프로젝트가 아닌 경우1$ npm install -D eslint-config-airbnb-typescript 위와 같이 설치 한 다음 .eslintrc.json을 다음과 같이 수정합니다. 1234567 &quot;extends&quot;: [- &quot;airbnb-base&quot;+ &quot;airbnb-typescript/base&quot; ], &quot;parserOptions&quot;: {+ &quot;project&quot;: &quot;./tsconfig.json&quot; }, 2.2. React 프로젝트인 경우12345$ npm install eslint-config-airbnb-typescript \\ eslint-plugin-jsx-a11y \\ eslint-plugin-react \\ eslint-plugin-react-hooks@^1.7.0 \\ --save-dev 위와 같이 설치 한 다음 .eslintrc.json을 다음과 같이 수정합니다. 1234567 &quot;extends&quot;: [- &quot;airbnb-base&quot;+ &quot;airbnb-typescript&quot; ], &quot;parserOptions&quot;: {+ &quot;project&quot;: &quot;./tsconfig.json&quot; }, 이제 다시 eslint 명령어를 수행하면 아까와 같은 에러가 더이상 발생하지 않는 것을 알 수 있습니다. 3. Typescript 절대 경로 Import일반적으로 Typescript 프로젝트에서는 import * as ssl from '../../../services/ssl' 과 같은 import 경로를 피하기 위해 절대 경로 import를 많이 사용합니다. 아래와 같이 tsconfig.json 을 수정하면 import * as ssl from '@src/services/ssl' 처럼 절대 경로로 import 할 수 있습니다. 123456{ &quot;compilerOptions&quot;: { &quot;baseUrl&quot;: &quot;.&quot;, &quot;paths&quot;: {&quot;@src/*&quot; : [&quot;./src/*&quot;]} }} 하지만 eslint 에서는 이 경로를 인식하지 못해서 아래와 같은 에러가 발생합니다. 1error Unable to resolve path to module '@src/services/ssl' import/no-unresolved 이러한 문제를 해결 하기 위해서는 eslint-import-resolver-typescript라는 패키지를 설치해 주셔야 합니다. 1$ npm i -D eslint-import-resolver-typescript 위와 같이 설치 한 다음 .eslintrc.json에 다음을 추가합니다. 1234567{ &quot;settings&quot;: { &quot;import/resolver&quot;: { &quot;typescript&quot;: {} } }} 4. TL; DR4.1. React 프로젝트가 아닌 경우Package 설치1234567$ npm install eslint \\ eslint-plugin-import \\ @typescript-eslint/eslint-plugin \\ @typescript-eslint/parser \\ eslint-config-airbnb-typescript \\ eslint-import-resolver-typescript \\ --save-dev Project Root에 .eslintrc.json 생성1234567891011121314151617181920212223242526272829303132{ &quot;env&quot;: { &quot;es6&quot;: true, &quot;node&quot;: true }, &quot;extends&quot;: [ &quot;airbnb-typescript/base&quot; ], &quot;globals&quot;: { &quot;Atomics&quot;: &quot;readonly&quot;, &quot;SharedArrayBuffer&quot;: &quot;readonly&quot; }, &quot;parser&quot;: &quot;@typescript-eslint/parser&quot;, &quot;parserOptions&quot;: { &quot;ecmaVersion&quot;: 2018, &quot;project&quot;: &quot;./tsconfig.json&quot; }, &quot;plugins&quot;: [ &quot;@typescript-eslint&quot; ], &quot;rules&quot;: { &quot;radix&quot;: &quot;off&quot;, &quot;no-console&quot;: &quot;off&quot;, &quot;@typescript-eslint/no-use-before-define&quot;: &quot;off&quot;, &quot;@typescript-eslint/no-unused-vars&quot;: [&quot;error&quot;, { &quot;args&quot;: &quot;none&quot; }] }, &quot;settings&quot;: { &quot;import/resolver&quot;: { &quot;typescript&quot;: {} } }} 4.2. React 프로젝트인 경우Package 설치12345678910$ npm install eslint \\ eslint-plugin-import \\ eslint-plugin-jsx-a11y \\ eslint-plugin-react \\ eslint-plugin-react-hooks@^1.7.0 \\ @typescript-eslint/eslint-plugin \\ @typescript-eslint/parser \\ eslint-config-airbnb-typescript \\ eslint-import-resolver-typescript \\ --save-dev Project Root에 .eslintrc.json 생성1234567891011121314151617181920212223242526272829303132{ &quot;env&quot;: { &quot;browser&quot;: true, &quot;es6&quot;: true }, &quot;extends&quot;: [ &quot;airbnb-typescript&quot; ], &quot;globals&quot;: { &quot;Atomics&quot;: &quot;readonly&quot;, &quot;SharedArrayBuffer&quot;: &quot;readonly&quot; }, &quot;parser&quot;: &quot;@typescript-eslint/parser&quot;, &quot;parserOptions&quot;: { &quot;ecmaFeatures&quot;: { &quot;jsx&quot;: true }, &quot;ecmaVersion&quot;: 2018, &quot;project&quot;: &quot;./tsconfig.json&quot; }, &quot;plugins&quot;: [ &quot;react&quot;, &quot;@typescript-eslint&quot; ], &quot;rules&quot;: { }, &quot;settings&quot;: { &quot;import/resolver&quot;: { &quot;typescript&quot;: {} } }}","link":"/Node/eslint/"},{"title":"Nodejs Logging (Morgan, Winston)","text":"Nodejs에서 자주 사용하는 로깅 라이브러리인 Morgan과 Winston을 알아보겠습니다. 1. Morgan1234567891011121314151617181920212223242526272829const rfs = require('rotating-file-stream');function fileName(time: Date | null, index: number): string { if (time) { return `${dateToString(time).split(' ')[0]}.log`; } return `${dateToString(new Date()).split(' ')[0]}.log`;}const consoleFormat = '[:date] &lt;:remote-addr&gt; (:user-id) :method :status :response-time ms &quot;:url&quot;';export const consoleLogger = morgan(consoleFormat);const accessLogStream = rfs(fileName, { interval: '1h', path: logDirectory, immutable: true,});const fileFormat = '[:date] &lt;:remote-addr&gt; (:user-id) :method :status :response-time ms &quot;:url&quot; &quot;:user-agent&quot;';export const fileLogger = morgan(fileFormat, { stream: accessLogStream, skip (req, res) { if (req.user_id) { return req.user_id === 1 || req.user_id === 4; } return false; },}); 2. Winston1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import { createLogger, format, transports } from 'winston';import { TransformableInfo } from 'logform';const consoleFormat = format.printf((info: TransformableInfo) =&gt; { return `[${info.timestamp}] [${info.level}] ${info.message}`;});const fileFormat = format.printf((info: TransformableInfo) =&gt; { const payload = info.payload; return `[${info.timestamp}] [${info.level}] ${info.message} ${payload ? JSON.stringify(payload) : ''}`;});const consoleTransport = new transports.Console({ level: 'info', format: format.combine( format.colorize(), consoleFormat, ),})const dailyRotateFileTransport = new transports.DailyRotateFile({ level: 'debug', dirname: './log', filename: '%DATE%.log', datePattern: 'YYYY-MM-DD', maxSize: '10m',})export const logger = createLogger({ format: format.combine( format.timestamp({ format: () =&gt; moment().tz('Asia/Seoul').format('YYYY-MM-DD HH:mm:ss'), }), fileFormat, transports: [consoleTransport, dailyRotateFileTransport], )})export function logRequest(req: IncommingMessage, res: ServerResponse) { let ip = req.connection.remoteAddress; const method = req.method; const url = req.url; req['_startAt'] = process.hrtime(); res.on('finish', () =&gt; { const [sec, ns] = process.hrtime(req['_startAt']); const responseTime = Math.floor(sec * 1e3 + ns * 1e-6); const status = res.statusCode; })}","link":"/Node/logging/"},{"title":"tsconfig.json 파헤치기","text":"tsconfig.json 파일에는 정말 많은 옵션들이 있습니다. 그 중에 제가 자주 사용하는 옵션들을 설명해 보겠습니다. esModuleInterop12345{ &quot;compilerOptions&quot;: { &quot;esModuleInterop&quot;: true }} ES6 문법에서는 * 형태로 import 한 것을 함수로 사용할 수 없습니다. 예를 들어서 다음과 같은 코드는 정상적으로 Javascript로 Transpile 되지만 ES6 문법에 맞지 않는 것이죠. 1234567// before transpileimport * as moment from 'moment';moment(); // ES6 문법에 맞지 않음!// after transpileconst moment = require('moment');moment(); Javascript와의 호환성을 위해 * 형태로 import 해야하는 module들을 default 형태로 import 할 수 있게 해주는 옵션이 esModuleInterop 입니다. 아래와 같은 방식으로 동작한다고 생각하시면 됩니다. 1234567// before transpileimport moment from 'moment';moment(); // ES6 문법에 맞는다!// after transpileconst moment = __importDefault(require('moment'));moment.default(); 위에서 __importDefault 함수가 default가 있는 module은 그대로 두고 없는 module은 default로 바꾸어주는 역할을 합니다.","link":"/Node/tsconfig/"},{"title":"바닥부터 시작하는 Webpack","text":"Webpack과 Babel을 하나도 몰라도 Typescript, React, CSS까지 번들링 하는 법을 알아보겠습니다. Webpack이 뭔지 Babel이 뭔지는 다른 블로그가 설명이 더 잘 되어있으니 여기서는 실습에 집중하겠습니다. 1. 일단 webpack 써보기1234$ mkdir webpack-test$ cd webpack-test$ npm init -y$ npm i -D webpack webpack-cli 위 명령어를 통해 NPM 프로젝트를 만들어 준 후 webpack과 webpack-cli를 설치합니다. src/index.js1console.log(&quot;hello, world!&quot;); src/index.js에 위의 코드를 적습니다. 1$ npx webpack --mode none --entry ./src/index.js -o dist 콘솔창에 위와 같은 명령어를 입력하면 아래와 같은 내용이 적혀있는 파일이 dist/main.js에 생성됩니다. dist/main.js1234/******/ (() =&gt; { // webpackBootstrapconsole.log(&quot;hello, world&quot;);/******/ })(); entry 옵션을 줄 때 src/index.js와 같이 경로를 적으면 파일을 찾지 못합니다. ./src/index.js와 같이 앞에 ./을 붙여야합니다. 옵션을 파일로 주기 (webpack.config.js)그런데 이렇게 일일히 옵션 입력하면 귀찮으니까 보통은 아래와 같이 webpack.config.js를 만들어 줍니다. 위 파일을 생성한 다음에 npx webpack 명령어를 입력하면 위와 같은 동작을 하는 것을 알 수 있습니다. webpack.config.js12345678910const path = require('path');module.exports = { mode: 'development', entry: './src/index.js', output: { filename: 'main.js', path: path.join(__dirname, '.'), },} 여기서 output path에는 절대경로만 들어가야합니다. 2. 여러 파일을 하나로 묶기src/math.js파일을 생성하여 아래와 같이 적습니다. src/math.js1module.exports.add = (a, b) =&gt; a + b; src/index.js123const math = require('./math');console.log(math.add(1, 2)); 그러면 webpack이 알아서 dependency를 추적해서 관련된 모든 파일을 한 파일로 묶어줍니다. 123$ npx webpack$ node dist/main.js3 ES6 모듈 시스템node는 CommonJS 모듈 시스템을 쓰고 있습니다. 따라서 ES6 모듈 시스템을 사용할 수 없었는데 webpack을 사용하면 ES6 모듈 시스템을 쓸 수 있습니다. math.js1export const add = (a, b) =&gt; a + b; index.js123import * as math from './math';console.log(math.add(1, 2)); 1234$ node src/index.js # 에러$ npx webpack$ node dist/main.js # 정상동작3 3. webpack dev server3.1. webpack watch1$ npx webpack --watch 위의 명령어를 사용하면 webpack 데몬이 돌아가면서 파일이 변경될 때마다 자동으로 빌드해줍니다. 3.2. webpack dev server1$ npm i -D webpack-dev-server webpack-dev-server는 웹 브라우저에서 자동으로 live reload를 하게 해줌으로써 생산성을 높일 수 있습니다. dist/index.html12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;webpack-test&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;script src=&quot;./main.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; webpack.config.js123456module.exports = {+ devServer: {+ contentBase: path.join(__dirname, 'dist'),+ hot: true,+ },} 위와 같이 html파일을 만들고 webpack.config.js 파일을 수정한 후 아래의 명령어로 server를 실행시킬 수 있습니다. 1$ npx webpack serve http://localhost:8080 경로에 웹 브라우저로 접속한 후 개발자 도구로 console 창을 확인하면 3이 떠있는 것을 볼 수 있습니다. 여기서 src 폴더의 파일을 수정하면 웹 페이지가 자동으로 새로고짐 됩니다. contentBase 경로가 프로젝트 루트 경로이면 hot reloading이 되지 않습니다. 4. React1$ npm i -D react react-dom @types/react @types/react-dom babel-loader @babel/core @babel/preset-react src/index.js1234567891011import React from 'react'import ReactDOM from 'react-dom'import * as math from './math';const App = () =&gt; { return ( &lt;h1&gt;{math.add(1, 2)}&lt;/h1&gt; )}ReactDOM.render(&lt;App /&gt;, document.getElementById('root')); dist/index.html123456789&lt;html&gt; &lt;head&gt; &lt;title&gt;webpack-test&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; &lt;script src=&quot;./main.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; webpack.config.js12345678910111213141516module.exports = {+ module: {+ rules: [+ {+ test: /\\.(js|jsx)$/,+ exclude: /node_modules/,+ use: {+ loader: 'babel-loader',+ options: {+ presets: ['@babel/preset-react'],+ },+ }+ }+ ]+ },} 5. Typescript12$ npm i -D ts-loader typescript$ npx tsc --init src/index.ts123import * as math from './math.js';console.log(math.add(1, 2)); src/math.ts1export const add = (a: number, b: number) =&gt; a + b; webpack.config.js123456789101112131415161718module.exports = { ...- entry: './src/index.js',+ entry: './src/index.ts', ...+ resolve: {+ extensions: ['.ts', '.js']+ }, module: { rules: [+ {+ test: /\\.ts$/,+ exclude: /node_modules/,+ use: ['ts-loader'],+ }, ] },} webpack은 ts 확장자 파일을 자동으로 인식하지 않기 때문에 위와같이 extension으로 추가해주어야합니다. 6. Typescript + Reacttsconfig.json123456789{ &quot;compilerOptions&quot;: { &quot;target&quot;: &quot;es6&quot;, &quot;module&quot;: &quot;es6&quot;, &quot;jsx&quot;: &quot;react&quot;, &quot;moduleResolution&quot;: &quot;node&quot;, &quot;allowSyntheticDefaultImports&quot;: true }} index.tsx1234567891011import React from 'react';import ReactDOM from 'react-dom';import * as math from './math';const App = () =&gt; { return ( &lt;h1&gt;{math.add(1, 2)}&lt;/h1&gt; )}ReactDOM.render(&lt;App /&gt;, document.getElementById('root')); webpack.config.js1234567891011121314151617181920module.exports = { ...- entry: './src/index.ts',+ entry: './src/index.tsx', ... resolve: {- extensions: ['.ts', '.js']+ extensions: ['.ts', '.tsx', '.js'] }, module: { rules: [ {- test: /\\.ts$/,+ test: /\\.(ts|tsx)$/, exclude: /node_modules/, use: ['ts-loader'], }, ] },} 7. CSS1$ npm i -D style-loader css-loader webpack.config.js1234567891011module.exports = { module: { rules: [+ {+ test: /\\.css$/,+ exclude: /node_modules/,+ use: ['style-loader', 'css-loader'],+ }, ] }} css-loader는 css 파일을 읽는 역할을 하고 style-loader는 읽은 css 파일을 html 파일에 넣어주는 역할을 합니다. css-loader말고 less-loader, sass-loader, postcss-loader 등 많은 loader를 사용할 수 있습니다.","link":"/Node/webpack/"},{"title":"Apache Hadoop 설치하기 (Single Node) (+Docker)","text":"Single Node에서 Apache Hadoop을 설치하는 법을 알아보겠습니다. 그리고 글의 끝에 이 모든 내용을 한 번에 Docker로 올릴 수 있는 Dockerfile도 적어두었습니다. Hadoop을 설치하기 위해서는 Java가 설치되어 있어야 합니다! Java를 설치하는 방법은 인터넷에 많으니 건너 뛰도록 하겠습니다. 1. Hadoop 바이너리 다운로드먼저 Hadoop 공식 홈페이지에서 바이너리를 다운로드 받습니다. 이 글을 작성하는 시점의 최신 버전은 3.3.1 이라서 이 버전으로 다운로드 받도록 하겠습니다. 현재 최신 버전을 알고 싶으면 여기에서 확인해주세요. 아래 명령어를 실행해서 Hadoop 3.3.1 을 다운로드 받고 이를 원하는 폴더에 압축을 풀면 됩니다. 여기서는 /usr/local 폴더에 압축을 푸는 것으로 진행하겠습니다. shell123$ wget https://archive.apache.org/dist/hadoop/core/hadoop-3.3.1/hadoop-3.3.1.tar.gz -P ./$ sudo tar zxf ./hadoop-3.3.1.tar.gz -C /usr/local$ sudo mv /usr/local/hadoop-3.3.1 /usr/local/hadoop 2. SSH Key 생성Hadoop의 각 노드들은 SSH를 이용하여 데이터를 주고 받습니다. 따라서 Hadoop을 설치 하기 전에 SSH를 설정해주어서 다른 노드에 접속할 때 비밀번호가 없이도 접속할 수 있게 해주어야 합니다. 여기서는 싱글노드라서 localhost에 접속할 것이고 localhost에 접속할 때도 ssh key는 필요합니다. 아래 명령어를 통해 SSH Key를 생성합니다. shell123$ ssh-keygen -t rsa -f ~/.ssh/id_rsa -P ''$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys$ chmod 400 ~/.ssh/id_rsa 그 다음 ssh localhost 명령어를 통해 localhost에 ssh 접속이 잘 되는지 확인합니다. 3. 환경 변수 설정~/.bashrc 파일 맨 밑에 아래 내용을 입력하여 환경 변수를 설정해줍니다. 만약에 본인이 zsh를 쓰고 있으면 ~/.zshrc 파일을 사용하면 됩니다. 만약 본인이 사용하고 있는 shell을 모른다면 echo $0 명령어를 사용하여 알 수 있습니다. .bashrc123export HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 위 내용을 입력한 후 source ~/.bashrc 명령어를 통해 환경 변수를 등록합니다. 그 다음에 JAVA_HOME 환경 변수를 아래 명령어를 통해 hadoop에 등록합니다. 아래 명령어를 쓰기 전에 echo $JAVA_HOME 명령어를 통해 제대로 JAVA_HOME 환경 변수가 등록 되어 있는지 확인하고 진행해주세요. shell1$ echo &quot;export JAVA_HOME=$JAVA_HOME&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh 4. 설정 파일 작성Hadoop 설정 파일을 작성합니다. /usr/local/hadoop/etc/hadoop/core-site.xml 파일에 입력합니다. 아래처럼 설정을 함으로써 9000 포트에 hdfs 전용 포트가 열립니다. /usr/local/hadoop/etc/hadoop/core-site.xml12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://0.0.0.0:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 기본적으로 Hadoop의 모든 데이터들은 /tmp/hadoop-${username} 경로에 저장되도록 설정되어 있습니다. 이런 데이터 저장 경로를 변경하고 싶으면 아래의 설정을 추가하면 됩니다. /usr/local/hadoop/etc/hadoop/core-site.xml1234&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/path/to/save/hadoop/data&lt;/value&gt;&lt;/property&gt; core-site.xml 설정 파일에 대한 자세한 정보를 보고 싶으면 여기를 참고해주세요. 5. 실행shell123456789101112# 먼저 Hadoop 파일 시스템(HDFS)을 포맷합니다.$ hdfs namenode -format# HDFS 인스턴스 구동. 이 파일은 $HADOOP_HOME/sbin 경로에 있습니다.$ start-dfs.sh# 아래 명령어를 통해 각 인스턴스가 제대로 떠 있는지 확인할 수 있습니다.$ jps4339 Jps4230 SecondaryNameNode4008 DataNode3819 NameNode 6. 접속 확인http://[하둡을 설치한 서버의 아이피 주소]:9870에 접속하면 Namenode의 정보를 확인할 수 있습니다. http://[하둡을 설치한 서버의 아이피 주소]:9868에 접속하면 Secondary Namenode의 정보를 확인할 수 있습니다. http://[하둡을 설치한 서버의 아이피 주소]:9864에 접속하면 Datanode의 정보를 확인할 수 있습니다. 7. HDFS에 파일을 넣어보기shell12345678910# 먼저 / 경로에 파일이 있는지 확인합니다. 당연히 아무 것도 없는 것이 정상입니다.$ hdfs dfs -ls /# hadoop 설치할 때 받아두었던 압축 파일을 HDFS에 넣어줍니다.$ hdfs dfs -copyFromLocal hadoop-3.3.1.tar.gz /# 다시 / 경로에 파일이 있는지 확인하면 아래와 같이 파일이 있는 것을 확인할 수 있습니다.$ hdfs dfs -ls /Found 1 items-rw-r--r-- 3 ubuntu supergroup 605187279 2021-12-09 14:40 /hadoop-3.3.1.tar.gz hdfs 명령어에 대해 자세히 알고 싶다면 https://blog.voidmainvoid.net/175를 참고해주세요. 8. Docker로 한 번에 하기지금까지 했던 것을 Docker로 한 번에 띄워버릴 수 있습니다. core-site.xml12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://0.0.0.0:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Dockerfile123456789101112131415161718192021222324252627282930313233343536373839FROM ubuntu:20.04RUN apt-get update# Set TimeZone (이 작업이 없으면 openjdk 설치할 때 TimeZone 선택하라면서 막힘)RUN ln -fs /usr/share/zoneinfo/Asia/Seoul /etc/localtimeRUN DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata# 패키지 다운로드RUN apt-get install -y wget vim curl net-tools dnsutilsRUN apt-get install -y openjdk-8-jdk openssh-server# Hadoop 설치RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-3.3.1/hadoop-3.3.1.tar.gz -P ~/Downloads; \\ tar zxf ~/Downloads/hadoop-3.3.1.tar.gz -C /usr/local; \\ mv /usr/local/hadoop-3.3.1 /usr/local/hadoop; \\ rm ~/Downloads/hadoop-3.3.1.tar.gz# ssh 설정RUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P ''; \\ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys; \\ chmod 400 ~/.ssh/id_rsa; \\ echo &quot;Host *\\n StrictHostKeyChecking no&quot; &gt; ~/.ssh/config;# Java 설정ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre# Hadoop 설정ENV HADOOP_HOME=/usr/local/hadoop \\ HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinRUN echo &quot;export JAVA_HOME=$JAVA_HOME&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh; \\ echo &quot;export HDFS_NAMENODE_USER=root&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh; \\ echo &quot;export HDFS_DATANODE_USER=root&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh; \\ echo &quot;export HDFS_SECONDARYNAMENODE_USER=root&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh;COPY core-site.xml /usr/local/hadoop/etc/hadoop/core-site.xmlENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot; , &quot;service ssh start; hdfs namenode -format; start-dfs.sh; tail -f /dev/null&quot;] 10번째 줄: wget, vim, curl, ifconfig, nslookup 등 유틸 설치 23번째 줄: 이 설정을 하지 않으면 hadoop에서 ssh를 사용할 때 정말로 접속하겠냐는 확인이 떠서 중간에 멈춥니다. 33~35번째 줄: hadoop을 root로 실행하기 위해서는 이런 설정이 필요합니다. shell12345# 도커 이미지 빌드$ docker build -t hadoop:single-node .# 도커 컨테이너 실행$ docker run -d -p 9870:9870 -p 9868:9868 -p 9864:9864 hadoop:single-node","link":"/Hadoop/Install-Hadoop-Single-Node/"},{"title":"Bash Hack - 멋있게 Bash 쓰기","text":"자주 마주치는 8가지 상황에서 Bash의 다양한 기능을 이용해 적게 타이핑 하고 문제를 해결해 보자! 1. 깜빡하고 sudo를 입력하지 않음12$ ln -s /path/to/command/file /usr/bin/command$ sudo !! !!는 바로 전에 입력한 명령어를 의미합니다. !-n은 n번째 전에 입력한 명령어를 의미합니다. 따라서 !!와 !-1은 같은 의미입니다. 2. 명령어를 입력하는 데 오타 발생12$ some very long long lung long command$ ^lung^long^ ^old^new^는 가장 최근에 입력한 명령어 중에서 처음으로 찾은 old를 new로 바꾸어 줍니다. 3. 폴더를 만든 후 바로 들어 가기12345$ mkdir verylonglongfoldername &amp;&amp; cd verylonglongfoldername$ mkdir verylonglongfoldername &amp;&amp; cd $_$ ls -al /path/to/list/files$ cd $_ $_는 전에 입력한 명령어 중에 가장 마지막 파라미터를 의미합니다. ls 명령어로 뭐가 있는지 확인하고 그 경로로 들어갈 때도 유용합니다. 4. 파일 이름을 일부분만 수정하기12$ mv /path/to/verylongfilename.txt /path/to/veryshortfilename.txt$ mv /path/to/verylongfilename.txt !#$:s/long/short/ 조금 복잡해 보이는데 분해해서 설명하면 다음과 같습니다. !#: 지금까지 입력란에 입력한 단어들을 의미합니다. 여기서는 mv /path/to/verylongfilename 입니다. $: 앞에서 찾은 단어들 중에 가장 마지막 단어를 의미합니다. :s/old/new/: 앞에서 찾은 단어에서 old를 new로 바꿉니다. 5. 지금까지 입력했던 명령어 검색Ctrl + r을 누른 후 검색어를 입력하면 지금까지 입력했던 명령어 중에 검색어가 존재하는 가장 최근 명령어를 보여줍니다. 여기서 Ctrl + r을 누르면 그 다음 최근 명령어를 보여줍니다. 6. 하나의 명령어를 여러 줄에 입력12345$ docker run \\ -e port=8080 \\ -e host=0.0.0.0 \\ -p 8080:8080 \\ service:latest 위와 같이 역슬래쉬를 입력하고 다음 라인에 명령어를 계속 입력할 수 있습니다. 7. 명령어를 한 번에 여러 개 실행123$ command1 &amp;&amp; command2$ command1 || command2$ command1; command2 &amp;&amp;: command1이 성공하면 command2도 실행됩니다. ||: command1이 실패하면 command2가 실행됩니다. ;: command1의 성공여부와 상관 없이 command2는 실행됩니다. 명령어를 그룹화 하고 싶으면 중괄호를 입력하면 됩니다. 중괄호로 그룹화한 명령어들 중에 가장 마지막 명령어 뒤에 세미콜른은 필수적으로 붙여주어야 합니다. 12$ command1 &amp;&amp; command2 &amp;&amp; command3 &gt; file$ { command1 &amp;&amp; command2 &amp;&amp; command3; } &gt; file 위의 명령어는 command3의 결과만 file에 기록되고 command1과 command2의 결과는 stdout에 출력됩니다. 아래의 명령어는 모든 명령어의 결과가 file에 기록됩니다. 8. 명령어를 백그라운드에서 실행1$ some command &amp; 명령어 뒤에 &amp;을 붙이면 그 명령어는 백그라운드에서 실행됩니다. 하지만 다음 두 가지의 단점이 있습니다. 명령어가 실행된 쉘을 종료하면 백그라운드로 실행되는 명령어도 종료됩니다. 명령어의 output이 현재 쉘에 그대로 출력됩니다. 따라서 nohup을 이용하는 편이 좋습니다. 12$ nohup some command &amp;$ nohup some command &amp; 명령어 앞에 nohup을 뒤에 &amp;를 붙이면 위의 두 가지 단점이 사라집니다. 명령어가 실행된 쉘을 종료해도 백그라운드로 실행되는 명령어는 유지됩니다. 명령어의 output은 명령어가 실행된 경로에서 nohup.out 이라는 파일에 기록됩니다. Appendix1) 현재 사용하고 있는 Shell 이름 알아내기 echo $0: 현재 사용하고 있는 Shell 이름 알아내기 echo $SHELL: 기본으로 설정된 Shell 이름 알아내기 2) 단축키 모음 Ctrl + a, Home: 커서를 맨 앞으로 옮기기 Ctrl + e, End: 커서를 맨 뒤로 옮기기 Ctrl + l: clear 명령어와 같은 동작을 합니다. Ctrl + t: 현재 커서에 있는 글자와 그 앞에 있는 글자의 위치를 바꿉니다 (오타 수정용) Ctrl + r: 지금까지 입력했던 명령어를 역순으로 검색하기 (reverse-search-history) Ctrl + d: 명령창에 아무 글자도 없을 때 이 키를 입력하면 exit 명령어와 같은 동작을 합니다. Ctrl + k: 현재 커서 뒤에 있는 글자 들을 삭제합니다. Ctrl + u: 현재 커서 앞에 있는 글자 들을 삭제합니다. Ctrl + w: 현재 커서 앞에 있는 가장 가까운 공백까지 삭제합니다. Ctrl + y: 최근에 삭제한 글자들을 현재 커서 뒤에 입력합니다. bind -p 명령어로 단축키를 확인할 수 있습니다. 3) 히스토리 이용하기명령어 지칭 !!: 바로 전에 입력했던 명령어 !n: history에서 n번째 명령어 !-n: n번째로 전에 입력했던 명령어 !string: string으로 시작하는 가장 최근 명령어 !?string: string을 포함하는 가장 최근 명령어 ^string1^string2^: 가장 최근 명령어에서 string1을 string2로 바꾼 명령어 !#: 현재 입력란에서 지금까지 입력한 글자들을 지칭함 단어 지칭위의 명령어 지칭을 사용한 후에 :를 입력하여 단어 지칭을 할 수 있습니다. 단어 지칭이 ^, $, *, -, %로 시작하는 경우 :를 생략할 수 있습니다. n: 지칭된 단어들 중에 n번째 단어를 지칭함 (0 부터 시작함) mkdir folder; cd !#:1 docker stop service; docker rm !#:2 $: 지칭된 단어들 중에 마지막 단어를 지칭함 ^: 지칭된 단어들 중에 두번째 단어를 지칭함 (명령어를 제외한 첫 번째 argument) x-y: x번째 부터 y번째 까지의 단어 -y: 0번째 부터 y번째 까지의 단어 *: 첫 번째 단어를 제외한 모든 단어 x*, x-: x-$와 같은 의미 명령어 지칭이 없이 단어 지칭을 사용하는 경우 바로 전에 입력했던 명령어에서 단어 지칭을 합니다. 단어 수정optional word designator 다음에 단어 수정자를 한 개 이상 나열할 수 있습니다. 각각은 : 뒤에 옵니다. h: 가장 마지막 pathname component를 삭제합니다. t: 가장 마지막 pathname component만 남깁니다. r: 확장자를 삭제합니다. e: 확장자만 남깁니다. s/old/new/: 처음 등장하는 old를 new로 바꿉니다. / 대신 다른 구분자를 사용할 수 있고 역슬래쉬를 통해 구분자를 이스케이프 할 수 있습니다. 만약 &amp;가 new에 등장합니다면 이는 old로 치환됩니다. &amp; 또한 역슬래쉬로 이스케이프 할 수 있습니다. 맨 마지막 구분자가 입력 라인의 가장 마지막 글자라면 생략할 수 있습니다. a, g: 치환을 전체 라인에 대해 적용합니다. s나 &amp; 앞에서 사용합니다. 예) gs/old/new/, g&amp; G: 앞으로 등장하는 모든 치환을 모든 라인에 대해 적용합니다. q: 모든 단어를 한 번에 따옴표로 감쌉니다. x: 모든 단어를 각각 따옴표로 감쌉니다. 단어들은 공백, 탭, 개행으로 구분합니다. &amp;: 가장 최근에 실행한 치환을 다시 반복합니다. p: 새로운 명령어를 출력하고 실행하지는 않습니다. 4) quote(‘)와 double quote(“)의 차이quote(‘)quote(‘) 사이의 어떤 특수문자도 아무런 처리를 하지 않습니다. 따라서 backslash(\\)로 quote를 escape 할 수 없습니다. 123456789$ echo '$PORT'$PORT$ echo '!!'!!# quote 문자열에서 quote를 출력하고 싶으면 아래처럼 해야 합니다.$ echo 'I'\\''m human'I'm human double quote(“)double quote(“) 사이의 특수문자(!, $, # 등)는 기존 처리방법대로 처리합니다. 1234567$ PORT=8080$ echo &quot;$PORT&quot;8080$ echo &quot;!!&quot;echo 8080 참고 자료 https://ss64.com/bash/syntax-keyboard.html http://tldp.org/LDP/abs/html/special-chars.html http://www.gnu.org/software/bash/manual/bash.html https://www.lesstif.com/system-admin/bash-readline-line-reverse-search-6717494.html","link":"/Server/Bash-Hack/"},{"title":"AWS Cloud9 사용해보기","text":"클라우드 IDE인 Cloud9을 통해 언제 어디서나 웹 브라우저를 통해 내 환경에서 개발하는 방법을 알아보겠습니다. 1. 환경 만들기 Cloud9 콘솔에 들어가서 Create environment를 클릭합니다. Name에 WorkSpace를 적고 Next Step을 클릭합니다. 각종 설정을 하고 다음으로 넘어갑니다. Environment Type : 새로운 EC2를 만들 것인지 아니면 기존에 있던 서버에 Cloud9환경을 추가할지 선택합니다. Cloud9은 무슨 새로운 패키지를 설치하는 것이 아니라 .c9 폴더에 Cloud9 환경 정보를 저장해 놓고 Cloud9에서 서버에 접속해 이 폴더를 사용하는 것 뿐입니다. 환경을 삭제하려면 단순히 .c9 폴더를 삭제하시면 됩니다. 저는 새로운 EC2를 만드려고 해서 위와 같은 옵션을 선택했습니다. Instance Type : 새로운 EC2의 인스턴스 타입을 선택합니다. 일반적으로 24시간 돌리는 서버와는 다르게 쓰는 만큼만 요금이 나가므로 비교적 비싼 타입을 선택하셔도 됩니다. Platform : 설치할 OS를 선택합니다. 저는 Ubuntu를 선택했습니다. Cost-saving setting : Cloud9 웹 IDE 페이지를 종료한 후 몇 시간 후에 자동으로 EC2를 중지할 지 선택합니다. 최소 단위가 30분이고 저는 이 옵션을 선택했습니다. 앞서 설정한 옵션들을 검토하는 화면입니다. 별 문제 없으면 Create environment를 클릭해서 환경을 만듭니다. 2. 보안그룹 설정 보안 그룹 설정에 본인이 쓰실 포트를 추가해 주시면 됩니다. 싸지방은 포트가 80번과 443번밖에 안 뚫려있기 때문에 저는 80번과 443번을 추가했습니다. 3. Apache2 삭제 Cloud9을 켜신후 서버의 아이피 주소로 들어가 보시면 위와 같이 Apache2 페이지를 보실 수 있습니다. 자동으로 Apache2 웹 서버가 구동되어 있기 때문입니다. 저와 같이 Apache2를 쓰지 않으시는 분은 Apache2를 삭제 하셔서 80번 포트를 아무도 안 쓰게 해주셔야 합니다. 아래의 명령어를 입력해서 Apache2를 삭제합니다. 12$ sudo service apache2 stop$ sudo apt-get purge apache2 apache2-utils 다른 블로그 같은 곳에서 sudo apt autoremove를 하라는 곳도 있을 텐데 제 경험상 이 걸 하면 기본적으로 설치되어 있는 node가 고장났던 경험이 있습니다. 알아서 해보시길 바랍니다. 위의 명령어 만으로 Apache2가 삭제 되지만 완전히 모든 파일이 삭제되지는 않습니다. 만약 모든 파일을 삭제하시길 원하시면 아래와 같은 명령어로 삭제해 주시길 바랍니다. 123ubuntu:~/environment $ whereis apache2apache2: /usr/sbin/apache2 /usr/lib/apache2 /etc/apache2 /usr/share/apache2 /usr/share/man/man8/apache2.8.gzubuntu:~/environment $ sudo rm -rf /etc/apache2 4. 기본적으로 설치되어 있는 것들아래는 기본적으로 설치되어 있는 패키지들 목록입니다. 제가 모르는 것이 더 있을 수 있습니다. 5. 간단한 노드 서버 만들어 보기간단한 노드 서버를 만들어 보도록하겠습니다. 이 과정이 필요 없으신 분들은 건너 뛰셔도 됩니다. 기본적으로 환경 변수 PORT가 8080으로 설정되어 있어 환경 변수를 이용하는 코드를 쓰시는 분들은 주의해주시길 바랍니다. (아래의 명령어 참조) 12ubuntu:~/environment/node $ echo $PORT8080 index.js 파일을 만들고 아래와 같이 작성합니다. 1234567891011const http = require('http');const server = http.createServer(function (req, res) { console.log('hello world'); res.statusCode = 200; res.end('hello world\\n');})server.listen(80, function() { console.log('Server Start!!'); }); 아래의 명령어를 통해 서버를 구동하고 서버의 아이피 주소를 이용하여 접속합니다. 1$ sudo node index.js 6. 단축키Ctrl + S 저장 Alt + W 에디터 탭 닫기 Alt + N 새 파일 Alt + T 새 터미널 주의사항 : Ctrl + W 는 에디터 창 닫기가 아니라 웹페이지 탭 닫기라서 Cloud9이 꺼져버립니다. 위의 모든 단축키는 설정에서 수정하실 수 있습니다. 7. 파일 업로드 &amp; 다운로드 파일 업로드 Cloud9 왼쪽에 뜨는 폴더 Hierarchy에서 원하시는 디렉토리에 드래그 앤 드롭하시면 됩니다. 또는 메뉴의 File - Upload Local Files... 를 이용하시면 됩니다. 파일 다운로드 Cloud9 왼쪽에 뜨는 폴더 Hierarchy에서 원하시는 파일을 마우스 오른쪽 클릭하시면 다운로드 됩니다.","link":"/Server/Cloud9/"},{"title":"VSCode 원격으로 사용해보기","text":"서버에 VSCode를 설치 한 후 브라우저를 통해서 접속할 수 있는 프로젝트가 있습니다. 바로 code-server라는 프로젝트입니다. 이를 사용할 수 있는 방법을 알아보겠습니다. 1. 빠르게 Docker로 시작해 보기1$ docker run -it -p 8080:8080 -v &quot;$PWD:/home/coder/project&quot; codercom/code-server 위의 명령어를 입력한 후 8080 포트를 통해 접속하면 우리에게 익숙한 VSCode 화면을 볼 수 있습니다. 2. 서버에 설치해서 사용하기하지만 위처럼 Docker로 실행하여 code-server의 터미널을 사용하게 되면 컨테이너 내부의 명령어만 사용 할 수 있어서 Host에 있는 명령어를 아무 것도 사용할 수 없게 됩니다. 따라서 서버에 설치를 해서 사용하는 편을 추천드립니다. 서버에 설치해서 사용하는 법을 아래를 따라하면 됩니다. 1) 바이너리 다운로드 먼저 https://github.com/cdr/code-server/releases에 들어간 다음에 위의 다운로드 목록 중에 본인에게 맞는 버전에 마우스 오른쪽 클릭을 누르고 링크를 복사하면 됩니다. 일반적으로 linux-x86_64.tar.gz를 선택하면 됩니다. 그런 다음 아래의 명령어를 입력해서 /usr/local/bin에 바이너리를 넣어주면 됩니다. 아래의 명령어는 예시이며 본인의 상황에 맞게 바꾸면 됩니다. 123$ wget https://github.com/cdr/code-server/releases/download/2.1698/code-server2.1698-vsc1.41.1-linux-x86_64.tar.gz$ tar -xvzf code-server2.1698-vsc1.41.1-linux-x86_64.tar.gz$ mv code-server2.1698-vsc1.41.1-linux-x86_64/code-server /usr/local/bin 2) 서비스에 등록하기서버가 재시작 될 때마다 자동으로 실행되기 위해서 서비스에 등록합니다. /etc/systemd/system/code-server.service 파일을 만드신 후 아래 내용을 작성하면 됩니다. 저는 유저가 hyunsub이므로 아래에 등장하는 모든 hyunsub을 본인의 유저에 맞게 바꾸면 됩니다. 12345678910111213[Unit]Description=code-serverAfter=network.target[Service]Type=simpleUser=hyunsubExecStart=/usr/local/bin/code-server --port 8443 --user-data-dir /var/lib/code-server /home/hyunsubEnvironment=&quot;PASSWORD=some-password&quot;Restart=always[Install]WantedBy=multi-user.target 위의 파일을 한 번 살펴 보겠습니다. --port 8443: code-server가 실행되는 포트입니다. 기본 값은 8080입니다. --user-data-dir /var/lib/code-server: VSCode Extension, 설정, 로그 등이 저장되는 장소 입니다. /home/hyunsub: code-server에 접속했을 때 기본으로 뜨는 경로를 지정할 수 있습니다. Environment=&quot;PASSWORD=some-password&quot;: 환경변수로 비밀번호를 지정함으로써 code-server에 접속할 때 비밀번호를 입력해야만 접속 할 수 있게 합니다. 위 파일 작성 후 아래 명령어를 실행하면 됩니다. 12345$ sudo mkdir /var/lib/code-server$ sudo chown hyunsub:hyunsub /var/lib/code-server$ sudo systemctl daemon-reload$ sudo systemctl enable code-server$ sudo systemctl start code-server 3. Run over HTTPScode-server가 HTTPS 프로토콜 위에 동작하지 않으면 클립보드를 사용할 수 없습니다. 따라서 다른 곳에서 복사한 내용을 붙여넣기 할 수 없게 되는 것입니다. 3.1. 이미 HTTPS 인증서가 있는 경우위에서 작성한 code-server.service 파일에서 ExecStart의 명령어에 --cert와 --cert-key 옵션을 추가해주면 됩니다. --cert에는 인증서 경로를, --cert-key에는 Private Key 경로를 입력해 주면됩니다. 3.2. HTTPS 인증서가 없는 경우code-server.service에서 ExecStart의 명령어에 단순히 --cert 옵션만 추가해 주면됩니다. 그러면 code-server가 알아서 인증서와 Private Key를 생성합니다. 하지만 이런 방식으로 할 경우 웹 브라우저에서 신용할 수 없는 인증서라는 경고 화면을 띄울 텐데 그냥 무시하고 들어가면 정상적으로 사용할 수 있습니다.","link":"/Server/Code-Server/"},{"title":"Terminal 꾸미기","text":"검정 바탕에 흰 글자가 전부인 기본 Terminal은 눈을 침침하게 하고 코딩 의욕을 저하시키는 원인 중에 하나입니다. 다양한 테마와 폰트, 색상 프로필을 통해 터미널을 멋있게 꾸미는 법을 알아보겠습니다. 맥 기본 Terminal의 모습입니다. 검정 바탕에 흰 글자… 정말 재미도 없고 감동도 없는 그런 모습입니다. 이제 이 글을 따라하며 한 단계씩 설정해 나가면 다음과 같은 터미널을 만들 수 있습니다. 1. zsh 설치맥은 Catalina부터 기본 터미널이 zsh로 설정되어 있습니다. 따라서 대부분의 경우 이 단계는 건너뛰어도 됩니다. 터미널에서 echo $0를 입력한 결과값에 zsh가 포함되어 있지 않다면 아래의 명령어로 zsh를 설치하고, 기본 터미널을 zsh로 바꿔줍니다. 12$ brew install zsh # zsh 설치$ chsh -s $(which zsh) # 기본 터미널을 zsh로 변경 2. Oh My ZSH 설치먼저 다양한 테마와 다양한 플러그인을 설치할 수 있는 Oh My ZSH를 설치합니다. 1$ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; 위의 명령어를 통해 Oh My ZSH를 설치하면 아래와 같이 터미널이 조금 달라져 있는 것을 볼 수 있습니다. 3. Oh My ZSH 테마 변경Oh My ZSH는 설정 파일에서 단순히 글자만 바꿈으로써 테마를 변경할 수 있습니다. 일단 여기서는 agnoster 테마를 적용하겠습니다. 더 많은 테마는 여기에서 확인해주세요. 12- ZSH_THEME=&quot;robbyrussell&quot;+ ZSH_THEME=&quot;agnoster&quot; ~/.zshrc 파일을 위와 같이 변경한 후 새 터미널을 열면 아래 사진과 같이 바뀐 테마가 적용 되어 있습니다. 명령어 입력란을 보면 좀 달라져 있는 것을 알 수 있습니다. 하지만 폰트 문제 때문에 위와 같이 뭔가 이상하게 나오는 것을 알 수 있습니다. 4. 폰트 적용https://github.com/powerline/fonts 에 가서 맘에드는 폰트를 설치하길 바랍니다. 저는 Source Code Pro for Powerline을 사용했습니다. 그런 다음 터미널에서 아래와 같이 폰트를 바꾼 후 새 터미널을 열면 제대로 나오는 것을 볼 수 있습니다. 5. 색상 프로필 변경여기까지만 해도 충분히 괜찮지만 검정 바탕에 흰 글자는 여전히 너무 식상합니다. 이제 색상 프로필을 변경함으로써 좀 더 그럴듯하게 만들어줄 것입니다. 먼저 여기서 원하는 프로필을 선택한 후 다운로드 해줍니다. 저는 Solarized Dark 프로필을 사용했습니다. 그런 다음 터미널에서 아래와 같이 프로필을 Import한 후 새 터미널을 열면 그럴듯한 색이 나오는 것을 볼 수 있습니다. 6. powerlevel10k 테마여기까지만 해도 충분이 괜찮지만 powerlevel10k 테마를 통해 명령어 입력란을 좀 더 풍성하게 만들어 줄 수 있습니다. powerlevel10k 테마는 Oh My ZSH 에서 기본으로 지원해 주지 않기 때문에 따로 아래의 명령어를 통해 powerlevel10k 테마를 다운로드 해줍니다. 1$ git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k Source Code Pro for Powerline에 Awesome Font가 추가된 폰트를 여기에서 다운로드한 후 설치합니다. 굳이 이 폰트가 아니더라도 Powerline과 Awesome Font가 추가된 폰트면 다 괜찮습니다. 터미널 설정에 들어가 폰트를 SourceCodePro+Powerline+Awesome Regular로 바꿔줍니다. 다음과 같이 ~/.zshrc 파일을 변경한 후 새 터미널을 열면 아래 사진과 같이 interactive하게 설정을 할 수 있습니다. 12- ZSH_THEME=&quot;agnoster&quot;+ ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; 설정을 다 마치고 나면 다음과 같이 멋있고 풍성한 터미널을 볼 수 있습니다. 위의 사진처럼 powerlevel10k 테마에서는 명령어를 입력한 시간, 명령어가 걸린 시간, 명령어의 exit 값을 알 수 있습니다. 7. Ghostty 터미널Ghostty는 2024년 말 공개된 터미널 에뮬레이터로, GPU 가속 렌더링과 네이티브 UI를 결합하여 빠른 속도와 높은 완성도를 자랑합니다. macOS와 Linux를 모두 지원하며, 설정 파일 하나로 간결하게 관리할 수 있습니다. 설정 파일 위치는 ~/.config/ghostty/config입니다. 테마 및 폰트12345theme = Solarized Dark Higher Contrastfont-family = &quot;SourceCodePro+Powerline+Awesome Regular&quot;font-family = &quot;Apple SD Gothic Neo&quot;font-style-bold = falsefont-style-bold-italic = false theme: 기본 제공 테마 이름을 입력합니다. Solarized Dark Higher Contrast는 Solarized Dark보다 대비가 높아 가독성이 좋습니다. font-family: 우선순위 순서로 여러 개 지정할 수 있습니다. 첫 번째 폰트에 없는 글자(한글 등)는 두 번째 폰트로 자동 대체됩니다. font-style-bold / font-style-bold-italic: false로 설정하면 볼드·이탤릭 서체를 별도로 렌더링하지 않고 일반 서체로 통일합니다. 이 옵션을 설정하지 않으면 ls 명령어 결과가 이상하게 출력됩니다. ls는 기본적으로 디렉토리 이름을 볼드체로 출력하는데, SourceCodePro+Powerline+Awesome Regular는 Regular 웨이트만 존재하는 패치 폰트입니다. 볼드 렌더링이 요청되면 Ghostty가 해당 폰트의 Bold 변형을 찾지 못해, Powerline·Awesome 글리프가 포함되지 않은 시스템의 다른 Source Code Pro Bold 폰트로 대체됩니다. 그 결과 프롬프트의 특수 기호들이 깨지거나, 폰트 메트릭 차이로 인해 글자 간격이 틀어져 보입니다. font-style-bold = false를 설정하면 볼드 요청이 와도 항상 Regular 폰트를 사용하므로 이 문제가 사라집니다. 커서123cursor-style = blockcursor-color = #708284shell-integration-features = no-cursor cursor-style: 커서 모양을 지정합니다. block, bar, underline, block_hollow 중 선택할 수 있습니다. 기본값은 block입니다. Solarized Dark Higher Contrast 테마는 cursor-style을 별도로 정의하지 않으므로 Ghostty 기본값인 block이 그대로 적용됩니다. cursor-color: 커서 색상을 hex 코드로 지정합니다. Solarized Dark Higher Contrast 테마의 기본 커서 색상은 #f34b00(주황빛 빨강)인데, 눈에 너무 튀어서 회색 계열인 #708284로 변경했습니다. shell-integration-features = no-cursor: Ghostty의 쉘 통합 기능 중 커서 제어만 비활성화합니다. zsh/powerlevel10k가 커서를 직접 관리하도록 맡깁니다. 이 옵션을 설정하지 않으면 cursor-style과 cursor-color 설정이 적용되지 않습니다. Ghostty는 쉘 통합(shell integration) 기능을 활성화하면 zsh에 훅(hook)을 자동으로 주입하여, 명령어 실행 중·대기 중 등 쉘의 상태에 따라 동적으로 커서를 변경하는 ANSI 이스케이프 시퀀스를 프롬프트에 삽입합니다. 이 동적 시퀀스가 config 파일의 정적 커서 설정을 덮어쓰기 때문에, 아무리 cursor-style과 cursor-color를 지정해도 반영되지 않는 것입니다. no-cursor를 설정하면 Ghostty가 커서 관련 이스케이프 시퀀스 주입을 생략하므로, config 파일에 지정한 커서 스타일과 색상이 그대로 유지됩니다. 배경 투명도12background-opacity = 0.85background-blur-radius = 20 background-opacity: 배경 투명도를 0~1 사이로 설정합니다. 0.85는 살짝 투명하게 보이는 정도입니다. background-blur-radius: 투명 배경 뒤의 내용을 흐리게 처리하는 블러 반경입니다. macOS의 유리(glass) 효과와 함께 사용하면 깔끔합니다. 키 바인딩1keybind = global:cmd+grave_accent=toggle_quick_terminal global: 접두사를 붙이면 다른 앱이 포커스를 갖고 있어도 동작하는 전역 단축키로 등록됩니다. cmd+grave_accent는 Cmd+`에 해당합니다. toggle_quick_terminal은 화면 상단에서 슬라이드로 내려오는 Quick Terminal을 토글합니다. 어느 앱에서든 빠르게 터미널을 열 수 있어 매우 편리합니다.","link":"/Server/Customize-Terminal/"},{"title":"AWS EC2로 나만의 Proxy 서버 만들기","text":"복잡한 서버 설정 없이 Proxy Server를 만드는 법을 알아봅시다. 우리는 세상 살면서 여러가지 이유로 해외 IP로 접속해야 하는 사이트가 존재합니다. 그게 무슨 사이트인지는 굳이 언급하지 않도록 하겠습니다 ㅎㅎ. 시중에 있는 무료 Proxy는 너무 속도가 느리고 그렇다고 유료 Proxy을 쓰자니 돈이 좀 아까운 느낍이 듭니다. 그래서 EC2로 Proxy Server를 만드는 법을 알려드리도록 하겠습니다. 1. 해외 Region에서 EC2 만들기이건 쉬우므로 건너 뛰도록 하겠습니다. 2. Mac에서 VPN 사용하기먼저 터미널을 켜주신 후 pem 파일이 있는 폴더로 가서 아래 명령어를 입력합니다. 아래 1.2.3.4를 본인 EC2의 IP로 바꿔주시길 바랍니다. 1$ ssh -i key_pair.pem -D 8080 -C -N user@1.2.3.4 위 명령어를 입력한 후 뭔가 실행된 상태로 되어 있으면 정상적으로 proxy tunnel을 생성한겁니다. 앞으로 접속하는 모든 사이트의 패킷들은 이 tunnel을 통과합니다. 2.1. 환경설정에서 Proxy 기능 켜고 끄기네트워크 환경설정에 들어갑니다. 고급 버튼을 누릅니다. 위 그림처럼 따라하시면 됩니다. 적용 버튼을 누르면 Proxy 기능이 켜집니다. Proxy를 끌 때는 저 설정에 들어가서 SOCKS 프록시를 체크해제 하시고 적용하시면 됩니다. 2.2. Command로 Proxy 기능 켜고 끄기근데 이렇게 일일이 환경설정 들어가서 Proxy를 키거나 끄면 너무 귀찮으니까 터미널에서 명령어로 처리하는 법을 알아봅시다. 먼저 본인이 지금 사용하고 있는 네트워크 인터페이스 이름을 알아내야 합니다. 그러기 위해서 아래 명령어를 입력해 그 리스트를 알아냅니다. 1$ networksetup -listnetworkserviceorder 저는 위와 같은 결과가 나왔습니다. 저는 제 맥북을 와이파이로 쓰고 있으므로 저 Wi-Fi가 제 와이파이 네트워크 인터페이스 이름입니다. 기타 다른 네트워크 어댑터를 쓰고 있다면 그에 맞는 인터페이스 이름을 찾으시면 됩니다. 이제 이 이름을 바탕으로 아래 명령어를 실행하시면 됩니다. VPN 켤 때 1$ networksetup -setsocksfirewallproxy &quot;Wi-Fi&quot; localhost 8080 VPN 끌 때 1$ networksetup -setsocksfirewallproxystate &quot;Wi-Fi&quot; off 4. Windows에서 VPN 사용하기제가 아는 선에서는 Mac과 달리 Windows에서 모든 패킷을 Proxy로 우회하는 방법이 없습니다. 만약 그런 방법이 있다면 제보해 주시길 바랍니다. 먼저 putty와 같이 ssh tunnel을 유지시키는 MyEnTunnel을 인터넷에서 다운받습니다. 위와 같이 아이피 주소, 유저 네임, 비밀번호를 입력하고, Enable Dynamic SOCKS를 체크하시고 저 포트를 8080로 바꿔줍니다. 그리고 Connect 버튼을 누르면 위와 같은 메시지가 뜰텐데 그냥 Yes 누르시면 됩니다. status 탭에 이렇게 로그가 뜰텐데 Connectino stable이라고 뜨면 성공적으로 ssh tunnel을 만든겁니다. 그리고 프로그램들을 가상 네트워크 인터페이스 상에 실행시켜줄 SocksCap64를 인터넷에서 다운받습니다. 여기서 예를 누르면 자동으로 웹 브로우저 목록을 불러옵니다. 뭐라는지 잘 모르겠지만 Accept를 눌러줍시다. 이와 같은 화면에서 좌상단의 Proxy 버튼을 누릅니다. 여기서 127.0.0.1과 8080을 입력한 다음에 저장하고 이 프로그램상에서 웹브라우저를 실행시키시면 됩니다.","link":"/Server/EC2-Your-Own-VPN/"},{"title":"EC2 Container Service(ECS) 사용하기 + AutoScaling","text":"EC2 Container Service(ECS) 사용하기 + AutoScaling 기본적인 Docker 사용법은 안다고 가정하겠습니다. (설치, 명령어 등) 현재 저는 접속하면 hello world를 띄워주는 서비스를 제공 하고 있습니다. 이 서비스는 AWS EC2와 nodejs를 사용하고 있습니다. ECS라는 서비스를 사용 하려고 합니다. 1. 도커 이미지 만들기Dockerfile1234567891011121314FROM ubuntu:16.04MAINTAINER Noverish Harold &lt;embrapers263@gmail.com&gt;WORKDIR /rootRUN apt-get updateRUN apt-get install -y nodejsRUN apt-get install -y gitRUN git clone https://github.com/Noverish/simple-nodejsCMD cd /root/simple-nodejs; git pull origin master; nodejs index.jsEXPOSE 80EXPOSE 443 위 처럼 자신이 만들고 싶은 도커 이미지의 Dockerfile을 만듭니다. Dockerfile은 만들 줄 아신다고 생각하겠습니다. 1$ docker build --tag main-image . Dockerfile이 있는 위치에서 위의 명령어를 통해 이름이 main-image인 도커 이미지를 만들었습니다. 2. ECS 설정하기 AWS ECS에 들어가서 시작하기를 누릅니다. 우리는 둘 다 할 것이므로 둘다 체크하고 넘어갑니다. 리포지토리 이름은 그냥 test라고 했습니다. 위의 명령어를 통해 리포지토리에 로그인 하고 이미지를 푸쉬하라는데 밑에 자세히 설명하도록 하겠습니다. 일단 AWS CLI를 설치해야 합니다. 맥 기준으로 1$ brew install awscli 명령어를 통해 AWS CLI를 설치합니다. 다른 OS는 알아서… 그다음에 설정을 해줘야 하는데 aws configure 명령어를 통해 IAM User의 Access ID와 Secret Key를 위 처럼 입력해주시면 됩니다. 이제 AWS CLI 설정은 끝났습니다. 아까 페이지의 명령어를 차례대로 따라하시면 됩니다. 1$ aws ecr get-login --no-include-email --region ap-northeast-2 이 명령어를 통해 로그인을 하면 위와 같이 엄청 긴 명령어가 나옵니다. 만약 위와 같이 에러 메세지가 나온다면 aws configure를 통해 설정 해준 유저에 권한이 없어서 생기는 문제입니다. AWS IAM에서 위의 권한을 유저에 할당하시면 됩니다. 아까의 명령어에서 반환된 결과를 복붙해서 실행합니다. 12$ docker tag main-image:latest 123456789123.dkr.ecr.ap-northeast-2.amazonaws.com/test:latest$ docker push 123456789123.dkr.ecr.ap-northeast-2.amazonaws.com/test:latest 위의 명령어는 예시 입니다. 본인의 명령어는 단계 2: Docker 이미지 빌드, 태그 지정 푸쉬 페이지에서 확인하시기 바랍니다. 우리가 만들어 놓은 도커 이미지 이름은 main-image이고 리포지토리 이름은 test입니다. 이에 주의하면서 위의 명령어를 입력하면 아래와 같이 업로드가 진행됩니다. 위의 명령어를 입력하면 위와 같이 우리가 만들어 놓은 이미지가 리포지토리에 푸쉬됩니다. 그 다음에 페이지의 다음 단계로 넘어갑니다. 작업 정의 이름을 test-task-definition으로 컨테이너 이름을 test-container으로 해줬습니다. 여기서 주의 해야 할 점은 위와 같이 호스트 포트를 꼭 0으로 해주어야 합니다! 0으로 해주지 않으면 뒤에서 할 AutoScaling을 할 때 Load Balancer의 포트 번호가 이미 사용 중이라고 해서 에러가 납니다. 여기서 0의 의미는 자동으로 할당한다는 의미 입니다. 서비스 이름을 test-service라고 지었습니다. 우리가 아까 호스트 포트를 0으로 설정해 놔서 강제로 Load Balancer를 사용 하는 것으로 되어 있습니다. 그리고 넘어갑니다. 자신이 원하는 설정 하시고 넘어갑니다. 인스턴스 시작 및 서비스 실행을 누릅니다 그러면 ELB, VPC, EC2등 다양한 것들을 설정합니다. 이 작업은 몇 분 정도 걸립니다. 작업이 끝나면 서비스 보기를 누릅니다. 위와 같이 나오면 성공입니다. 2. AutoScaling 설정하기 Auto Scaling을 누릅니다. 아직 아무 것도 없다고 뜹니다. 업데이트를 누릅니다. 그냥 넘어갑니다. 여기도 그냥 넘어갑니다. 저는 최소 작업 개수를 1개로 최대 작업 개수를 5개로 했습니다. 다르게 해도 상관 없습니다. 조정 정책 추가를 누릅니다. 정책 이름을 test-policy로 했습니다. 새 경보 생성을 누릅니다. 알람 이름을 test-alarm으로 했습니다. 위와 같이 한 다음에 저장을 누릅니다. 위에서 설정한 값의 의미는 1개의 컨테이너가 1분 이상 평균 CPU 사용량이 1퍼센트가 넘으면 경보가 울린다는 의미입니다. 조정 작업에서 1 추가로 합니다.이 의미는 경보가 울리면 작업을 1개 추가 한다는 의미 입니다. 그리고 다음 단계로 넘어갑니다. 서비스 업데이트를 누릅니다. 위와 같이 설정 작업이 끝날 때 까지 기다립니다. EC2 대쉬보드로 들어가서 로드밸런서의 DNS 주소를 찾습니다. 정상적으로 페이지가 나오는 걸 확인 할 수 있습니다. CloudWatch에 들어가면 위 사진과 같이 우리가 설정해 놓은 알림이 있는 것을 확인 할 수 있습니다. 이제 이 알림이 울리게 하기 위해 아까의 페이지에서 새로고침을 연타합니다! CPU 사용률을 늘려서 AutoScaling이 되는 것을 확인하기 위해서 입니다. (물론 설정해 놓은 사용률이 1%여서 굳이 안해도 경보가 울리긴 합니다.) 그러면 이렇게 경보가 생기는 것을 알 수 있습니다. 이제 ECS 대쉬보드로 넘어가면 AutoScaling이 된 것을 확인 할 수 있습니다.","link":"/Server/ECS-AutoScaling/"},{"title":"Elastic Stack - Elasticsearch 편","text":"정형 및 비정형 데이터를 시각화하고 분석하는데 용이한 Elastic Stack을 사용하는 법을 알아보겠습니다. 이 글은 Ubuntu 18.04.2 LTS 사용자를 대상으로 작성했습니다. 1. 설치하기1.1. Java 설치Elasticsearch는 Java를 사용하므로 먼저 Java를 설치하셔야 합니다. 2019년 4월 16일부터 oracle-jdk를 다운로드 받으려면 무조건 License를 가지고 있어야 합니다. 따라서 OpenJDK를 설치하도록 하겠습니다 1234567# 1. OpenJDK를 설치합니다.$ sudo apt-get install -y openjdk-8-jre$ sudo apt-get install -y openjdk-8-jdk# 2. 설치가 잘 되었나 확인합니다.$ javac -version$ java -version 1.2. Elasticsearch 설치1) Elasticsearch를 다운로드 받습니다.1$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.5.0.deb 다른 버전의 설치 링크를 알고 싶으시다면 https://www.elastic.co/kr/downloads/past-releases를 방문하시면 됩니다. URL의 6.5.0 부분을 원하시는 버전으로 바꾸시면 그 버전이 다운로드 됩니다. 2) dpkg 명령어를 이용해 Elasticsearch를 설치합니다.1$ sudo dpkg -i elasticsearch-6.5.0.deb Elasticsearch는 다음과 같은 위치에 설치 됩니다. 내용 위치 설치 경로 /usr/share/elasticsearch 설정 파일 경로 /etc/elasticsearch 데이터 저장 경로 /var/lib/elasticsearch 로그 저장 경로 /var/log/elasticsearch 실행 파일 경로 /etc/init.d/elasticsearch 3) 서버가 시작 될 때 자동으로 시작되게 설정합니다.1$ sudo systemctl enable elasticsearch.service 4) option : Elasticsearch 삭제 방법 12$ sudo dpkg --purge elasticsearch$ find / -name elasticsearch -exec rm -r &quot;{}&quot; \\; 2. 실행하기12345# Elasticsearch 시작 명령어$ sudo service elasticsearch start# Elasticsearch 중지 명령어$ sudo service elasticsearch stop 아래의 명령어를 통해 정상적으로 실행이 되었는지 알 수 있습니다. Elasticsearch가 시작 되는 데 시간이 좀 걸리므로 1분 정도 지나서 시도해 주시길 바랍니다. 1$ curl http://localhost:9200 3. 데이터 넣기Elasticsearch에 대한 사용법은 다른 포스트에서 다루기로 하고 우리는 일단 ELK 스택을 실습해 보는 것을 목표로 하므로 일단 데이터를 넣어보는 것만 하겠습니다. 1) index 만들기1$ curl -XPUT http://localhost:9200/records?pretty 2) mapping 넣기1$ curl -XPUT http://localhost:9200/records/_mapping/_doc?pretty -d @mapping.txt -H &quot;Content-Type:application/json&quot; mapping.txt 3) document 넣기1$ curl -XPOST http://localhost:9200/records/_doc/_bulk?pretty --data-binary @bulk.txt -H &quot;Content-Type:application/x-ndjson&quot; bulk.txt 이로써 우리는 Elasticsearch를 구동하고 거기에 1000개의 상품 판매 목록 데이터를 집어 넣었습니다. 그러면 다음 포스트에서 Kibana로 이 데이터를 시각화 하는 방법에 대해 알아보도록 하겠습니다.","link":"/Server/ELK-1/"},{"title":"Elastic Stack - Kibana 편","text":"정형 및 비정형 데이터를 시각화하고 분석하는데 용이한 Elastic Stack을 사용하는 법을 알아보겠습니다. 이 글은 Ubuntu 18.04.2 LTS 사용자를 대상으로 작성했습니다. 1. Kibana 설치1) Kibana를 다운로드 받습니다.1$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.5.0-amd64.deb 다른 버전의 설치 링크를 알고 싶으시다면 https://www.elastic.co/kr/downloads/past-releases를 방문하시면 됩니다. URL의 6.5.0 부분을 원하시는 버전으로 바꾸시면 그 버전이 다운로드 됩니다. Kibana 6.0.0 이상의 버전은 64bit 운영체제만 지원합니다 2) dpkg 명령어를 이용해 Kibana를 설치합니다.1$ sudo dpkg -i kibana-6.5.0-amd64.deb Kibana는 다음과 같은 위치에 설치 됩니다. 내용 위치 설치 경로 /usr/share/kibana 설정 파일 경로 /etc/kibana 데이터 저장 경로 /var/lib/kibana 실행 파일 경로 /etc/init.d/kibana 3) 서버가 시작 될 때 자동으로 시작되게 설정합니다.1$ sudo systemctl enable kibana.service 4) option : Kibana 삭제 방법 12$ sudo dpkg --purge kibana$ find / -name kibana -exec rm -r &quot;{}&quot; \\; 2. Elasticsearch와 연동하기Elasticsearch와 Kibana를 같은 서버에서 구동하면 딱히 설정할 게 없지만 다른 서버에서 구동하면 /etc/kibana/kibana.yml을 수정해야 합니다. 파일에서 아래의 부분을 본인의 Elasticsearch 서버의 주소로 바꾸시고 앞의 주석 처리를 없앱니다. 1#elasticsearch.url: &quot;http://localhost:9200&quot; 3. 실행하기12345# Kibana 시작 명령어$ sudo service kibana start# Kibana 중지 명령어$ sudo service kibana stop 아래 링크에 웹브라우저로 접속해서 정상적으로 실행이 되었는지 알 수 있습니다. Kibana 시작 되는 데 시간이 좀 걸리므로 1분 정도 지나서 시도해 주시길 바랍니다. 1$ curl -v http://localhost:5601 아래와 같이 뜨면 실행이 되고 있다는 뜻입니다. 1234567891011121314151617181920Note: Unnecessary use of -X or --request, GET is already inferred.* Rebuilt URL to: http://localhost:5601/* Trying 127.0.0.1...* TCP_NODELAY set* Connected to localhost (127.0.0.1) port 5601 (#0)&gt; GET / HTTP/1.1&gt; Host: localhost:5601&gt; User-Agent: curl/7.58.0&gt; Accept: */*&gt; &lt; HTTP/1.1 302 Found&lt; location: /app/kibana&lt; kbn-name: kibana&lt; kbn-xpack-sig: fd6e658a000b1fdd8c1408d85da429b6&lt; cache-control: no-cache&lt; content-length: 0&lt; connection: close&lt; Date: Tue, 21 May 2019 13:45:43 GMT&lt; * Closing connection 0 4. 로그 보기Kibana는 기본적으로 로그를 파일에 저장하는 것이 아니라 stdout에 출력하므로 로그를 보시려면 다른 방법을 사용해야 합니다 Kibana의 로그를 보고 싶다면 아래의 명령어를 입력하시면 됩니다. 1$ journalctl -u kibana.service -e 또는 Kibana의 설정파일을 수정하시면 됩니다. /etc/kibana/kibana.yml 1logging.dest: /var/log/kibana.log 5. Elasticsearch 데이터 보기 Management - Index Pattern을 클릭합니다. Index Pattenr을 records라고 입력하고 다음으로 넘어갑니다. Time filter field name을 time이라고 설정하고 완료합니다. Discover에 들어가서 우측 상단의 시간을 적절히 조정하시면 우리가 저번에 넣었던 데이터 1000개를 볼 수 있습니다. 6. 80번 포트로 실행시키기/etc/kibana/kibana.yml에서 포트를 그냥 80번으로 변경한 뒤 실행하면 그 포트를 사용하지 못한다는 에러가 발생합니다. sudo service kibana start 명령어가 Kibana를 root 권한으로 실행시키지 않아서 발생하는 에러 입니다. 따라서 service가 Kibana를 root 권한으로 실행시키도록 수정해야 합니다. 123456789101112131415$ sudo vi /etc/kibana/kibana.ymlserver.port: 80로 변경$ sudo vi /etc/systemd/system/kibana.serviceUser=rootGroup=root로 변경$ sudo systemctl daemon-reload$ sudo service kibana start","link":"/Server/ELK-2/"},{"title":"ElasticSearch - Index 편","text":"ElasticSearch - Index 편 모든 문서는 6.5.0 버전 기준으로 작성 되었습니다.1. Index 만들기1$ curl -XPUT http://localhost:9200/twitter 1.1. Index 이름 규칙 소문자만 사용 가능 \\, /, *, ?, &quot;, &lt;, &gt;, |, (space character), ,, # 사용 불가 7.0 이전 버전에서는 :을 사용할 수 있지만 그 이후 버전에서는 사용 불가 -, _, +로 시작할 수 없음 . 또는 .. 으로 지을 수 없음 255 바이트 보다 길게 지을 수 없음 (한글 같은 경우는 한 글자에 1 바이트 보다 크므로 지을 수 있는 글자 수는 더 적음) 1.2. Setting과 함께 Index 만들기12345678910$ curl -XPUT http://localhost:9200/twitter -H &quot;Content-Type:application/json&quot; -d '{ &quot;settings&quot; : { &quot;index&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 } }}' 다음도 가능합니다. 123456{ &quot;settings&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 }} number_of_shards의 기본값은 5입니다. number_of_replicas의 기본값은 1입니다. (각 primary shard 당 1개의 복사본) 1.3. Mapping과 함께 Index 만들기1234567891011$ curl -XPUT http://localhost:9200/test -H &quot;Content-Type:application/json&quot; -d '{ &quot;mappings&quot; : { &quot;_doc&quot; : { &quot;properties&quot; : { &quot;field1&quot; : { &quot;type&quot; : &quot;text&quot; } } } }}' 1.4. 별명 (Aliase)과 함께 Index 만들기12345678910111213$ curl -XPUT http://localhost:9200/test -H &quot;Content-Type:application/json&quot; -d '{ &quot;aliases&quot; : { &quot;alias_1&quot; : {}, &quot;alias_2&quot; : { &quot;filter&quot; : { &quot;term&quot; : {&quot;user&quot; : &quot;kimchy&quot; } }, &quot;routing&quot; : &quot;kimchy&quot; } }}' 위의 Setting, Mapping, Aliase는 동시에 설정 할 수 있습니다.","link":"/Server/ElasticSearch-Index/"},{"title":"내 서버에 Https 적용하기 (with Nginx)","text":"무료 HTTPS 인증서 발급 기관인 Let’s Encrypt를 통해 내 서버에 Https를 적용하는 법을 알아보겠습니다. 인증서를 얻기 위해서는 해당 서버에 SSH로 접속해야 합니다. 1. Certbot 설치Certbot은 Let’s Encrypt의 인증서 발급을 편하게 도와주는 도구 입니다. Certbot을 설치하기 위해서는 다음과 같은 명령어를 입력해 주시길 바랍니다. 12$ sudo apt update$ sudo apt install certbot 2. 인증서 발급인증서 발급은 인증 기관(여기서는 Let’s Encrypt)이 인증서 발급을 요청한 서버가 실제로 이 도메인을 소유하고 있는 지를 검증하기 위해 요청 도메인의 특정 Path에 특정 내용을 담아보라고 요구합니다. 예를 들어 hyunsub.kim에 대한 인증서를 요청하면 인증 기관은 http://hyunsub.kim/1q2w3e4r에 ‘asdfqwer’를 담아보라고 요구하고 이 것을 수행하면 이 도메인을 소유하고 있다고 판단하는 것입니다. 따라서 certbot이 이러한 작업을 할 수 있도록 명령어를 입력하는데 크게 2가지 경우가 있습니다. 1) 80 포트에 돌아가는 웹 서버를 잠시 정지 시킬 수 있는 경우1$ sudo certbot certonly -d www.example.com --standalone certbot이 독자적인 웹 서버를 80 포트에 돌려서 위의 인증 기관의 요구 사항을 수행합니다. 2) 80 포트에 돌아가는 웹 서버를 잠시 정지 시킬 수 없는 경우 (webroot)1$ sudo certbot certonly -d www.example.com --webroot -w /var/www/example 만약에 80 포트에 돌아가는 웹 서버를 정지할 수 없는 경우 webroot 폴더 경로를 지정해 주면서 인증 기관의 요구 사항을 수행합니다. 여기서 w 옵션의 값으로 들어간 /var/www/example 폴더가 webroot 폴더인데 예를 들어 /var/www/example/tmp.txt 에 파일을 생성하면 http://www.example.com/tmp.txt 로 접근할 수 있게 웹 서버 설정이 된 폴더를 webroot 폴더라고 합니다. 3) 80 포트에 돌아가는 웹 서버를 잠시 정지 시킬 수 없는 경우 (Manual)1$ sudo certboy certonly -d www.example.com --manual 아래 nginx 설정처럼 webroot 폴더가 없는 경우 이 방법을 사용하면 됩니다. 아래와 같이 nginx 설정을 통해 인증 기관의 요구 사항을 수행합니다. 123456789101112server { listen 80 default; server_name home.hyunsub.kim; location /.well-known/acme-challenge/D-Vxj9IdTZH1olzPp6ignQFn14GxE3YlzB0cYA6mAEk { return 200 &quot;D-Vxj9IdTZH1olzPp6ignQFn14GxE3YlzB0cYA6mAEk.ZlPIVlhU8ETlWqd4WS3pR7AHmjQaL_HoPLgvdyIBf0c&quot;; } location / { return 301 https://home.hyunsub.kim$request_uri; }} 위 3가지 방법 중에 하나를 수행하고 나면 아래의 경로에 두 파일이 만들어집니다. /etc/letsencrypt/live/{도메인 주소}/fullchain.pem (인증서 + 공개키 파일) /etc/letsencrypt/live/{도메인 주소}/privkey.pem (개인키 파일) 이제 이 두 파일을 Nginx에 적용시켜 보겠습니다. 3. Nginx 설정12345678910111213141516171819202122server { listen 80 default; server_name home.hyunsub.kim; location / { return 301 https://home.hyunsub.kim$request_uri; }}server { listen 443 ssl default; server_name home.hyunsub.kim; ssl_certificate /etc/letsencrypt/live/home.hyunsub.kim/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/home.hyunsub.kim/privkey.pem; location / { root /usr/share/nginx/html; index index.html; try_files $uri $uri/ /index.html; }} 4. 인증서 갱신Let’s Encrypt 인증서는 3개월 후에 만료되므로 주기적으로 갱신시켜주어야 합니다. 인증서 갱신은 인증서 발급과 같은 과정을 거치기 때문에 80 포트를 쓸 수 있냐 없냐로 나뉘어서 명령어를 입력해야 합니다. 1) 80 포트에 돌아가는 웹 서버를 잠시 정지 시킬 수 있는 경우12$ sudo certbot renew --pre-hook &quot;service nginx stop&quot; --post-hook &quot;service nginx start&quot;$ sudo certbot renew --pre-hook &quot;service nginx stop&quot; --post-hook &quot;service nginx start&quot; --dry-run # 갱신 테스트 prehook, posthook을 통해 갱신 전에 웹 서버를 잠시 정지시키고 인증서 갱신한 다음 다시 웹 서버를 실행시킵니다. 2) 80 포트에 돌아가는 웹 서버를 잠시 정지 시킬 수 없는 경우인증서를 발급 받을 때와 같은 명령어를 입력하면 됩니다. 5. Crontab에 등록Crontab에 다음과 같이 추가해서 주기적으로 인증서 갱신을 하도록 합니다. 10 0 1 * * sudo certbot renew ... 매달 1일 0시에 인증서를 갱신한다는 의미입니다. 하지만 인증서 갱신은 인증서 만료 30일 전부터 가능한다는 점 알아두셔야 합니다. 6. 내 사이트 테스트아래 사이트를 통해 내 사이트의 HTTPS를 검증할 수 있습니다. https://www.ssllabs.com/ssltest/ 7. [부록] 와일드카드 인증서와일드카드 인증서에 대한 설명은 생략하도록 하겠습니다. 1$ sudo certbot certonly --manual -d '*.hyunsub.kim' -d hyunsub.kim --preferred-challenges dns 먼저 위 명령어를 입력합니다. 실수로 -d 옵션을 두 개 입력한 것이 아닙니다. 하나는 와일드카드 인증서, 나머지 하나는 hyunsub.kim에 대한 인증서 입니다. 이 두 개는 다른 것이기 때문에 두 개를 꼭 명시해주어야 합니다. 123456789101112131415Saving debug log to /var/log/letsencrypt/letsencrypt.logPlugins selected: Authenticator manual, Installer NoneObtaining a new certificatePerforming the following challenges:dns-01 challenge for hyunsub.kimdns-01 challenge for hyunsub.kim- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -NOTE: The IP of this machine will be publicly logged as having requested thiscertificate. If you're running certbot in manual mode on a machine that is notyour server, please ensure you're okay with that.Are you OK with your IP being logged?- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -(Y)es/(N)o: 그러면 위와 같은 메시지가 나오는데 Let’sEncrypt 쪽에서 인증서 발급 요청을 하는 아이피를 로깅해도 되겠냐고 물어봅니다. 거절하면 중단되기 때문에 Y를 입력하고 엔터를 누릅니다. 123456789- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Please deploy a DNS TXT record under the name_acme-challenge.hyunsub.kim with the following value:CMvPkXSCJK6eMu2oaPtEa2vNEO6SJ_fPtqDU4ZXwhzMBefore continuing, verify the record is deployed.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Press Enter to Continue 위와 같은 메시지가 나올텐데 엔터를 한 번 누릅니다. 123456789- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Please deploy a DNS TXT record under the name_acme-challenge.hyunsub.kim with the following value:YyOb_PjJYJ_JIR-EqkJKhNBHKKqmHhwkapgG6KpURUMBefore continuing, verify the record is deployed.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Press Enter to Continue 이제 엔터를 누르면 안 됩니다!! 또 위와 같은 메시지가 나옵니다. 하나는 *.hyunsub.kim 에 대한 것이고, 나머지 하나는 hyunsub.kim 에 대한 것입니다. 각각의 두 문자열을 본인이 사용하고 있는 DNS 제공 업체의 관리툴에 가서 TXT 타입으로 등록해줍니다. 저는 가비아를 사용하고 있기 때문에 아래와 같이 설정해주었습니다. 해당 내용이 DNS 서버에 반영되기까지 시간이 걸리기 때문에 바로 엔터를 누르면 안 됩니다!!!아래와 같이 dig 명령어를 사용하여 정상적으로 두 문자열이 잘 뜰 때까지 기다립니다. 123456789101112131415161718192021$ dig _acme-challenge.hyunsub.kim txt; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; _acme-challenge.hyunsub.kim txt;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 41154;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;_acme-challenge.hyunsub.kim. IN TXT;; ANSWER SECTION:_acme-challenge.hyunsub.kim. 600 IN TXT &quot;CMvPkXSCJK6eMu2oaPtEa2vNEO6SJ_fPtqDU4ZXwhzM&quot;_acme-challenge.hyunsub.kim. 600 IN TXT &quot;YyOb_PjJYJ_JIR-EqkJKhNBHKKqmHhwkapgG6KpURUM&quot;;; Query time: 130 msec;; SERVER: 210.220.163.82#53(210.220.163.82);; WHEN: Thu Jul 08 19:29:30 KST 2021;; MSG SIZE rcvd: 112 위와 같이 ANSWER SECTION에 두 문자열이 잘 뜨면 작업을 진행하던 터미널에 엔터를 누릅니다. 12345678910111213141516Waiting for verification...Cleaning up challengesIMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/hyunsub.kim/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/hyunsub.kim/privkey.pem Your cert will expire on 2021-07-11. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run &quot;certbot renew&quot; - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le 그러면 위와 같이 뜨면서 와일드카드 인증서 발급이 완료되었습니다! 인증서 갱신인증서 갱신할 때는 인증서 발급할 때와 같은 명령어를 입력하면 됩니다. Certbot이 알아서 처리해줍니다. 8. [부록] 사설 인증서 만들기123456789101112131415161718192021$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout selfsigned.key -out selfsigned.crtGenerating a RSA private key.......+++++.....................................................................................................+++++writing new private key to 'selfsigned.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:KRState or Province Name (full name) [Some-State]:SeoulLocality Name (eg, city) []:SeoulOrganization Name (eg, company) [Internet Widgits Pty Ltd]:HyunsubOrganizational Unit Name (eg, section) []:HyunsubCommon Name (e.g. server FQDN or YOUR name) []:home.hyunsub.kimEmail Address []:embrapers263@gmail.com 여기서 생성된 selfsigned.crt는 인증서, selfsigned.key는 개인 키입니다.","link":"/Server/HTTPS/"},{"title":"APEX로 AWS Lambda 개발하기","text":"APEX라는 프로그램을 사용하여 AWS Lambda를 개발해 서버리스 백엔드를 구현해 보자. 1. APEX 설치하기1$ curl https://raw.githubusercontent.com/apex/apex/master/install.sh | sudo sh macOS, Linux, or OpenBSD에서 위의 명령어를 실행시키면 자동으로 설치가 됩니다. 2. APEX에 AWS 권한 부여하기APEX가 Lambda 함수들을 만들고/지우고/수정하고/실행할 수 있도록 권한을 줘야 합니다. 먼저 APEX가 필요로 하는 권한을 묶은 커스텀 정책을 만듭시다. AWS IAM로 들어가서 정책 생성을 누릅니다. json편집기에 들어가서 밑의 json을 그대로 입력해 줍시다. 그리고 Review Policy를 누릅니다. 123456789101112131415161718192021222324252627282930{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: [ &quot;iam:CreateRole&quot;, &quot;iam:CreatePolicy&quot;, &quot;iam:AttachRolePolicy&quot;, &quot;iam:PassRole&quot;, &quot;lambda:GetFunction&quot;, &quot;lambda:ListFunctions&quot;, &quot;lambda:CreateFunction&quot;, &quot;lambda:DeleteFunction&quot;, &quot;lambda:InvokeFunction&quot;, &quot;lambda:GetFunctionConfiguration&quot;, &quot;lambda:UpdateFunctionConfiguration&quot;, &quot;lambda:UpdateFunctionCode&quot;, &quot;lambda:CreateAlias&quot;, &quot;lambda:UpdateAlias&quot;, &quot;lambda:GetAlias&quot;, &quot;lambda:ListAliases&quot;, &quot;lambda:ListVersionsByFunction&quot;, &quot;logs:FilterLogEvents&quot;, &quot;cloudwatch:GetMetricStatistics&quot; ], &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;*&quot; } ]} 정책의 이름을 APEX-Policy로 짓고 정책을 생성합니다. 그리고 사용자에 이 정책을 추가합니다. 기존 사용자에 정책을 추가하셔도 되고, 새로운 사용자를 만드셔도 됩니다. 사용자를 만들거나 정책을 추가하는 방법은 설명하지 않도록 하겠습니다. 아까 정책을 추가한 사용자의 정보들을 아래의 위치한 파일에 추가합니다. access_key_id와 secret_access_key는 따로 설명하지 않도록 하겠습니다. ~/.aws/credentials 123[example]aws_access_key_id = xxxxxxxxaws_secret_access_key = xxxxxxxxxxxxxxxxxxxxxxxx ~/.aws/config 123[profile example]output = jsonregion = ap-northeast-2 아래의 명령어를 실행합니다. 위에서 입력한 프로필 이름에 따라 아래의 example을 바꿔주시기 바랍니다. 이렇게 하면 APEX가 AWS_PROFILE 환경변수에 접근해서 AWS 기능들을 사용할 수 있게 됩니다. 1$ export AWS_PROFILE=example 3. 프로젝트 만들기프로젝트 폴더를 생성하고 apex init를 입력하고 프로젝트의 이름과 설명을 입력하면 프로젝트가 생성됩니다. 12$ mkdir APEX-Test &amp;&amp; cd APEX-Test$ apex init 프로젝트의 이름은 APEX-Test프로젝트의 설명은 그냥 .으로 했습니다. 이제 프로젝트 폴더를 보면 json파일과 hello라는 함수가 만들어져 있습니다. 4. 프로젝트 구조 살펴보기project.json먼저 project.json 파일을 살펴보겠습니다. role은 배포할 함수의 role을 지정해 줍니다. 이 role은 APEX가 자동으로 생성한 것입니다.memory는 배포할 함수의 메모리 제한(MB)를 나타냅니다.timeout은 배포할 함수의 시간 제한(s)을 나타냅니다.environment는 배포할 함수에서 사용할 환경 변수를 나타냅니다.위의 값들을 자유롭게 본인의 프로젝트에 맞게 변경하시면 됩니다.이 밖에도 다양한 field들이 있습니다. APEX에서는 project.dev.json, project.prod.json과 같이 다중 환경에서 개발 할 수 있는 기능 각각의 함수 폴더 안에 function.json을 넣어서 함수 마다 다른 환경을 만들어 줄 수 있는 기능 등 을 제공하고 있습니다. 자세한 정보는 프로젝트 구조 (영문)를 참고해 주세요 폴더 구조 모든 함수는 functions 폴더안에 존재해야합니다.functions 폴더 밖에 만들어 둔 파일이나 폴더들은 APEX 에서는 무시합니다. functions 폴더의 바로 sub폴더들의 이름이 람다 함수의 이름이 됩니다.특별한 설정을 하지 않는 이상 sub 폴더 안의 index.js가 람다 함수의 실행 파일이 됩니다.각각의 sub 폴더 안에서는 밖에 있는 파일에 접근 할 수 없습니다.함수를 배포 할 때에는 sub 폴더 안의 파일들만을 묶어서 배포하기 때문입니다. 12345678910111213141516171819project.jsonpackage.jsonnode_modules└── ...fun0└── index.jsfunctions├── global.js├── fun1│ ├── index.js│ ├── package.json│ └── node_modules│ └── ...└── fun2 ├── asdf.js └── index.js 위의 프로젝트 구조를 예시로 들어보겠습니다. 프로젝트에는 2개의 함수가 존재합니다. (fun1, fun2) APEX에서 함수를 배포할 때에는 fun0은 무시합니다. fun1, fun2 함수를 실행 시키면 각각의 index.js가 실행됩니다. fun1/index.js 에서는 fun1/node_modules에 접근할 수 있습니다. fun2/index.js 에서는 fun2/asdf.js에 접근할 수 있습니다. fun2 에서는 global.js에 접근할 수 없습니다. fun2 에서는 모든 node_mudules에 접근할 수 없습니다. 만약에 모든 함수에 공통적으로 사용하고 싶은 npm module이 있어도 모든 함수 각각의 폴더 안에 node_mudules를 넣어야 합니다! 이를 해결 하기 위한 방법중에 하나는 WebPack을 사용하는 것인데 다음 시간에 알려드리도록 하겠습니다. 5. 함수 생성하기그냥 단순하게 functions에 폴더 하나 만들어 주고 그 안에 index.js 파일을 만들어 주시면 됩니다. 예시로 test 폴더를 만들고 그 안에 index.js 파일을 만들어서 test라는 함수를 만들었습니다. index.js에는 아래의 코드를 넣었습니다. 1234console.log('test1234')exports.handle = function(e, ctx, cb) { cb(null, { test: 'test' })} 기본적인 람다 코드 작성법이나 설명은 하지 않도록 하겠습니다. 6. 함수 배포하고 실행하기apex deploy는 functions 폴더에 있는 함수들을 모두 배포하는 작업을 합니다.apex deploy {name}처럼 특정 한 함수만 배포 할 수도 있고,apex deploy prefix*, apex deploy *suffix처럼 *을 사용하여 특정 접두어나 접미어를 가진 함수들만 배포 할 수도 있습니다. apex invoke {name}은 {name}이란 이름을 가진 함수를 실행하는 작업을 합니다. 아까 만들었던 test 함수를 배포하고 실행한 결과입니다.AWS Lambda Console 에 들어가보시면 APEX-Test_test라는 함수가 만들어 진 것을 볼 수 있습니다. apex logs {name}를 통해 함수의 로그를 확인 할 수 있습니다.우리가 만들었던 test함수에서 콘솔에 출력했던 test1234를 확인 할 수 있습니다.이 명령어를 통해서 apex invoke를 통해 실행된 것의 로그만 확인 할 수 있는 것이 아니라 모든 로그를 확인할 수 있습니다. log 명령어는 최근 5분 안에 발생했던 로그만 보여줍니다. 더 오래된 명령어를 보고 싶거나 다른 기능의 설명은 여기에 있습니다.","link":"/Server/Lambda-APEX/"},{"title":"달러 기호($)와 함께 세자리수로 한글이 깨지는 경우","text":"''$'\\354\\235\\264\\353\\240\\245\\354\\204\\234''와 같이 달러 기호($)와 함께 세자리수로 한글이 깨지는 경우 해결 방법을 알아보겠습니다. (feat. locale 설정) 1. 현재 설정되어 있는 locale 보기locale 명령어를 입력하여 현재 설정되어 있는 locale 볼 수 있습니다. 12345678910111213141516$ localeLANG=en_US.UTF-8LANGUAGE=LC_CTYPE=&quot;en_US.UTF-8&quot;LC_NUMERIC=&quot;en_US.UTF-8&quot;LC_TIME=&quot;en_US.UTF-8&quot;LC_COLLATE=&quot;en_US.UTF-8&quot;LC_MONETARY=&quot;en_US.UTF-8&quot;LC_MESSAGES=&quot;en_US.UTF-8&quot;LC_PAPER=&quot;en_US.UTF-8&quot;LC_NAME=&quot;en_US.UTF-8&quot;LC_ADDRESS=&quot;en_US.UTF-8&quot;LC_TELEPHONE=&quot;en_US.UTF-8&quot;LC_MEASUREMENT=&quot;en_US.UTF-8&quot;LC_IDENTIFICATION=&quot;en_US.UTF-8&quot;LC_ALL= 아마 위와 같이 나올 것입니다. 지금 locale 설정이 en_US.UTF-8로 설정되어 있는 것을 알 수 있습니다. 이 때문에 한글이 깨지는 것입니다. 2. 사용 가능한 locale 목록 보기locale -a 명령어를 통해 사용 가능한 locale 목록을 볼 수 있습니다. 1234567$ locale -aCC.UTF-8en_USen_US.utf8...POSIX 여기서 ko_KR.UTF-8이 있나 확인 합니다. 만약 존재하지 않다면 다음 명령어를 통해 추가합니다. $ locale-gen ko_KR.UTF-8 설정 가능한 locale 목록은 /usr/share/i18n/SUPPORTED 파일에서 확인할 수 있습니다. 3. Locale charset을 한글로 바꾸기다음 명령어를 통해 Locale charset을 한글로 바꿉니다. $ sudo update-locale LC_CTYPE=ko_KR.UTF-8 그 다음에 ssh 연걸을 끊었다가 다시 연결하면 다음과 같이 한글이 잘 나타나는 것을 알 수 있습니다. Reference 부록. 다양한 LC_* locale 환경 변수들우리는 위에서 LC_CTYPE locale 환경 변수만 변경했습니다. 다른 값들도 변경하면 다양한 방면으로 시스템에 영향을 미칩니다. 예를 들어 LC_TIME=ko_KR.UTF-8로 변경하면 date 명령어의 결과 값이 다음과 같이 달라집니다. shell1234567# LC_TIME=en_US.UTF-8 인 경우$ dateSat Mar 12 12:24:41 KST 2022# LC_TIME=ko_KR.UTF-8 인 경우$ date2022. 03. 12. (토) 12:28:30 KST 아래와 같이 locale 환경 변수는 다양한 kernel 함수에 영향을 미칩니다. LC_TIME : 시간과 날짜의 표현(년, 월, 일에 대한 명칭 등)을 조절합니다. 영향을 미치는 함수: strftime(), strptime() LC_MONETARY : 금액 표현(천단위 구분 문자, 소수점 문자, 금액 표시 문자, 그 위치 등)을 조절합니다. 영향을 미치는 함수: strfmon() LC_NUMERIC : 금액이 아닌 숫자 표현(천단위, 소수점, 숫자 그룹핑 등)을 조절합니다. 영향을 미치는 함수: strtod(), atof(). LC_COLLATE : 문자열의 정렬 순서를 조절합니다. 영향을 미치는 함수: strcoll(), wcscoll(), strxfrm() 더 자세한 정보는 IBM 문서, cpp 문서, 커피닉스 문서를 참고해주세요.","link":"/Server/Locale-Korean/"},{"title":"Docker로 SMTP 서버 직접 구축하기","text":"외부 릴레이(Gmail SMTP 등) 없이 직접 수신 메일 서버에 전달하는 SMTP 서버를 Docker로 구축하는 법을 알아보겠습니다. boky/postfix 이미지를 사용하며, SSL 인증서(Let’s Encrypt)가 이미 발급되어 있다고 가정합니다. 전제 조건 항목 비고 공인 IP ISP에서 포트 25 아웃바운드 차단 여부 확인 필요 (nc -zv smtp.gmail.com 25) SSL 인증서 Let’s Encrypt 등. /etc/letsencrypt/live/DOMAIN/ 위치 가정 도메인 DNS 레코드 직접 편집 가능해야 함 Docker 설치되어 있어야 함 1. DKIM 키 생성123456789mkdir -p /path/to/postfix/opendkim/keys/your.domain# 컨테이너를 현재 유저(uid 1000)로 실행해야 파일 소유자가 root가 되지 않음docker run --rm \\ -u $(id -u):$(id -g) \\ -v /path/to/postfix/opendkim/keys/your.domain:/keys \\ --entrypoint opendkim-genkey \\ instrumentisto/opendkim \\ -b 2048 -d your.domain -s mail -D /keys/ 생성 결과: mail.private — 서명 키 (컨테이너가 사용) mail.txt — DNS에 등록할 공개키 flat 키 파일도 함께 생성boky/postfix는 DKIM 키를 /etc/opendkim/keys/DOMAIN.private 경로(flat 파일)로 탐색한다. 서브디렉터리 구조(hyunsub.kim/mail.private)를 인식하지 못하므로, 아래와 같이 복사본을 만들어둔다. 12cp /path/to/postfix/opendkim/keys/your.domain/mail.private \\ /path/to/postfix/opendkim/keys/your.domain.private 2. OpenDKIM 설정 파일 생성boky/postfix가 시작 시 이 파일들에 내용을 추가(append) 한다. 빈 파일로 시작해도 되고, 아래처럼 기본값을 미리 채워도 된다. opendkim/KeyTable1mail._domainkey.your.domain your.domain:mail:/etc/opendkim/keys/your.domain.private opendkim/SigningTable1*@your.domain mail._domainkey.your.domain opendkim/TrustedHosts1234127.0.0.1localhost172.16.0.0/12192.168.0.0/16 주의: 세 파일 모두 컨테이너 시작 시 스크립트가 내용을 추가하므로 볼륨 마운트를 :ro로 걸면 안 된다. 3. docker-compose.yml1234567891011121314151617181920212223postfix: image: boky/postfix container_name: postfix restart: always hostname: mail.your.domain ports: - &quot;127.0.0.1:587:587&quot; # 외부에서 접근 불가, 로컬 앱 전용 environment: - HOSTNAME=mail.your.domain - ALLOWED_SENDER_DOMAINS=your.domain - POSTFIX_mynetworks=127.0.0.0/8 172.16.0.0/12 192.168.0.0/16 - POSTFIX_smtp_tls_security_level=may - POSTFIX_smtp_tls_cert_file=/certs/live/your.domain/fullchain.pem - POSTFIX_smtp_tls_key_file=/certs/live/your.domain/privkey.pem - POSTFIX_smtpd_tls_cert_file=/certs/live/your.domain/fullchain.pem - POSTFIX_smtpd_tls_key_file=/certs/live/your.domain/privkey.pem - POSTFIX_smtpd_tls_security_level=may volumes: - /etc/letsencrypt:/certs:ro # live/ 내 파일이 ../../archive/ 심볼릭링크이므로 상위 디렉터리 전체 마운트 - ./postfix/opendkim/keys:/etc/opendkim/keys # :ro 금지 (chown 필요) - ./postfix/opendkim/KeyTable:/etc/opendkim/KeyTable # :ro 금지 (startup 시 append) - ./postfix/opendkim/SigningTable:/etc/opendkim/SigningTable - ./postfix/opendkim/TrustedHosts:/etc/opendkim/TrustedHosts 주의사항 — DKIM_AUTOGENERATEboky/postfix의 스크립트는 이 변수를 bash -n 조건으로 체크한다. 1if [ -n &quot;$DKIM_AUTOGENERATE&quot; ]; then # 값이 &quot;false&quot;여도 non-empty이므로 true! 따라서 DKIM_AUTOGENERATE=false로 설정하면 자동생성이 비활성화되지 않는다. 비활성화하려면 변수 자체를 아예 선언하지 않아야 한다. 4. DNS 레코드 등록 타입 이름 값 A mail 서버 공인 IP MX @ mail.your.domain. (우선순위 10, 끝에 . 필수) TXT @ v=spf1 a:mail.your.domain ~all TXT mail._domainkey mail.txt 파일 내용 (아래 참고) TXT _dmarc v=DMARC1; p=none; rua=mailto:admin@your.domain DKIM 공개키 (TXT 레코드) 포맷DNS 레지스트라에 따라 TXT 값이 250자 이하인 문자열만 허용하는 경우가 있다. 2048비트 키는 전체 길이가 400자 이상이므로 반드시 multi-line(따옴표로 분리) 으로 입력해야 한다. mail.txt 파일 내용에서 따옴표와 공백을 정리해 하나의 문자열로 합친 뒤, 250자 단위로 나눈다: 123full = 'v=DKIM1; k=rsa; p=&lt;공개키&gt;'parts = [full[i:i+250] for i in range(0, len(full), 250)]print(' '.join(f'&quot;{p}&quot;' for p in parts)) 출력 예시 (DNS 레지스트라 입력란에 그대로 붙여넣기): 1&quot;v=DKIM1; k=rsa; p=...첫번째 250자...&quot; &quot;...나머지...&quot; PTR 레코드 (역방향 DNS)ISP 또는 호스팅사에 공인IP → mail.your.domain PTR 레코드 설정을 요청해야 한다. 없으면 Gmail 등 주요 메일 서버에서 스팸으로 분류될 수 있다. 5. 컨테이너 시작 및 검증123456789101112131415161718# 포트 25 아웃바운드 확인 (차단 시 직접 발송 불가)nc -zv smtp.gmail.com 25# 컨테이너 기동docker compose up -d postfixdocker logs postfix# 정상 기동 확인 로그 키워드# ✅ DKIM-Signature field added (s=mail, d=your.domain)# ✅ supervisord: opendkim/postfix entered RUNNING state# 테스트 메일 발송swaks --to test@gmail.com --from noreply@your.domain \\ --server localhost:587 --tls-optional \\ --header &quot;Subject: DKIM test&quot;# 메일 발송 로그 확인docker logs postfix 2&gt;&amp;1 | grep -E &quot;(DKIM|status=sent|status=bounced)&quot; 6. Spring Boot 연동application-prod.yml: 1234567891011121314spring: mail: host: localhost port: 587 properties: mail: smtp: starttls: enable: true required: false auth: false # localhost 전용이라 인증 불필요 connectiontimeout: 5000 timeout: 10000 writetimeout: 10000 7. 인증서 갱신 시 자동 reload/etc/letsencrypt/renewal-hooks/post/reload-postfix.sh: 12#!/bin/bashdocker exec postfix postfix reload 1sudo chmod +x /etc/letsencrypt/renewal-hooks/post/reload-postfix.sh 트러블슈팅 증상 원인 해결 컨테이너가 재시작 반복 DKIM_AUTOGENERATE=false로 설정했는데 자동생성 시도 → read-only 마운트에 쓰기 실패 DKIM_AUTOGENERATE 변수 자체를 제거 TrustedHosts: Read-only file system startup 스크립트가 파일에 append 시도 KeyTable, SigningTable, TrustedHosts 마운트에서 :ro 제거 Skipping DKIM for domain + KeyTable 비어있음 boky/postfix는 /etc/opendkim/keys/DOMAIN.private flat 경로만 탐색 keys/your.domain/mail.private를 keys/your.domain.private로 복사 key data is not secure WARNING 키 파일 권한이 opendkim 사용자 관점에서 느슨함 기능 무관, 무시해도 됨 /etc/letsencrypt/live/ 인증서 읽기 실패 live/ 내 파일이 ../../archive/를 가리키는 심볼릭링크 /etc/letsencrypt 전체를 마운트 (하위 디렉터리만 마운트하면 링크 깨짐) 이 글은 Claude Code (Model: Claude Sonnet 4.6 / claude-sonnet-4-6)가 생성했습니다.","link":"/Server/SMTP/"},{"title":"Spring Boot - GraphQL","text":"GraphQL이 무엇인지, 문법은 어떻게 되는지는 다루지 않고 Spring Boot에서 어떻게 GraphQL을 사용하는지에 대한 것만 알아보겠습니다. 1. 준비1.1. 디펜던시 추가 Gradle Maven build.gradle1234dependencies { implementation 'com.graphql-java:graphql-spring-boot-starter:5.0.2' implementation 'com.graphql-java:graphql-java-tools:5.2.4'} pom.xml123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-java-tools&lt;/artifactId&gt; &lt;version&gt;5.2.4&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.2. 모델 추가앞으로 쓰일 User 모델입니다. User.java123456@Datapublic class User { private long id; private String name; private int age;} 1.3. Repostory 추가실습을 위해 만든 데이터를 저장하는 클래스입니다. 나중에 실제 데이터베이스를 사용할 때는 그에 맞는 클래스를 생성해서 쓰면 됩니다. UserRepository1234567@Repositorypublic class UserRepository { public ArrayList&lt;User&gt; users = new ArrayList&lt;&gt;(Arrays.asList( new User(1, &quot;John&quot;, 20), new User(2, &quot;James&quot;, 22) ));} 2. schema 파일 생성resources 폴더에 아래의 파일을 추가합니다. 파일명을 어느 이름으로 해도 상관없습니다. index.graphqls1234567891011121314151617181920type User { id: ID! name: String! age: Int!}input UserInput { id: ID! name: String! age: Int!}type Query { getUsers: [User]! getUser(id: ID!): User}type Mutation { addUser(input: UserInput!): User!} 3. GraphQLQueryResolver다음과 같이 GraphQLQueryResolver를 상속한 클래스를 하나 만들어 우리가 위에서 정의한 Query를 처리합니다. 여기서 각 메서드의 이름은 스키마에서 정의한 이름과 정확히 일치해야합니다. 1234567891011121314151617import com.coxautodev.graphql.tools.GraphQLQueryResolver;@Componentpublic class UserQueryResolver implements GraphQLQueryResolver { @Autowired private UserRepository repository; public List&lt;User&gt; getUsers() { return repository.users; } public Optional&lt;User&gt; getUser(long id) { return repository.users.stream() .filter(v -&gt; v.getId() == id) .findAny(); }} curl 명령어를 통해 아래와 같은 결괏값을 받을 수 있습니다. 1234567891011121314151617181920212223# getUsers$ curl -XPOST localhost:8080/graphql \\ -d '{&quot;query&quot;: &quot;query { getUsers { id, name, age } }&quot;}'{&quot;data&quot;:{&quot;getUsers&quot;:[{&quot;id&quot;:&quot;1&quot;,&quot;name&quot;:&quot;John&quot;,&quot;age&quot;:20},{&quot;id&quot;:&quot;2&quot;,&quot;name&quot;:&quot;James&quot;,&quot;age&quot;:22}]}}# getUser$ curl -XPOST localhost:8080/graphql \\ -d '{ &quot;query&quot;: &quot;query($id: ID!) { getUser(id: $id) { id, name, age } }&quot;, &quot;variables&quot;: { &quot;id&quot;: 2 } }'{&quot;data&quot;:{&quot;getUser&quot;:{&quot;id&quot;:&quot;2&quot;,&quot;name&quot;:&quot;James&quot;,&quot;age&quot;:22}}}# getUser (없는 데이터를 요청한 경우)$ curl -XPOST localhost:8080/graphql \\ -d '{ &quot;query&quot;: &quot;query($id: ID!) { getUser(id: $id) { id, name, age } }&quot;, &quot;variables&quot;: { &quot;id&quot;: 100 } }'{&quot;data&quot;:{&quot;getUser&quot;:null}} 4. GraphQLMutationResolverUserInput 클래스를 만듭니다. Mutation의 input으로 쓰이는 클래스는 파라미터가 없는 생성자가 있어야합니다. 1234567@Getter@NoArgsConstructorpublic class UserInput { private long id; private String name; private int age;} 다음과 같이 GraphQLMutationResolver를 상속한 클래스를 하나 만들어 우리가 위에서 정의한 Mutation를 처리합니다. 여기서 각 메서드의 이름은 스키마에서 정의한 이름과 정확히 일치해야합니다. 12345678910111213import com.coxautodev.graphql.tools.GraphQLMutationResolver;@Componentpublic class UserMutationResolver implements GraphQLMutationResolver { @Autowired private UserRepository repository; public User addUser(UserInput input) { User newUser = new User(input.getId(), input.getName(), input.getAge()); repository.users.add(newUser); return newUser; }} curl 명령어를 통해 아래와 같은 결과값을 받는 것 을 볼 수 있습니다. 1234567$ curl -XPOST localhost:8080/graphql \\ -d '{ &quot;query&quot;: &quot;mutation($input: UserInput!) { addUser(input: $input) { id, name, age } }&quot;, &quot;variables&quot;: { &quot;input&quot;: { &quot;id&quot;: 3, &quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 24 } } }'{&quot;data&quot;:{&quot;addUser&quot;:{&quot;id&quot;:&quot;3&quot;,&quot;name&quot;:&quot;Tom&quot;,&quot;age&quot;:24}}} 5. GraphiQL GraphiQL은 위와 같이 GraphQL을 테스트 할 수 있는 웹 페이지를 현재 프로젝트에 추가해 줍니다. 먼저 디펜던시를 추가한 후, /graphiql 경로로 들어가면 위와 같은 화면을 볼 수 있습니다. Gradle Maven build.gradle123dependencies { implementation 'com.graphql-java:graphiql-spring-boot-starter:5.0.2'} pom.xml1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphiql-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 6. GraphQL SPQRGraphQL SPQR은 간단히 말해서 Annotation 만으로 GraphQL 스키마 파일(*.graphqls)을 자동으로 생성해주는 라이브러리입니다. GraphQL을 이용하여 개발을 하다보면 수정이 일어날 때 마다 GraphQL 스키마 파일과 스프링 내부적으로 사용하는 클래스에 2번씩 수정해야 합니다. 이 라이브러리를 사용하면 이러한 수고를 덜 수 있습니다. 위에서 설치한 디펜던시를 전부 지우고 아래의 디펜던시를 추가합니다. 보시다시피 이 라이브러리의 버전은 아직 0.0.6으로 초기 단계입니다. 따라서 릴리즈하는 프로젝트에는 적합하지 않습니다. Gradle Maven build.gradle1234dependencies { implementation 'io.leangen.graphql:graphql-spqr-spring-boot-starter:0.0.6' implementation 'org.springframework.boot:spring-boot-starter-web'} pom.xml1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.leangen.graphql&lt;/groupId&gt; &lt;artifactId&gt;graphql-spqr-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.0.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 다음과 같이 클래스에는 @GraphQLApi 어노테이션을, 각 메서드에는 @GraphQLQuery, @GraphQLMutation을 붙이면 끝입니다! UserGraphQL.java1234567891011121314151617181920212223242526272829import io.leangen.graphql.annotations.GraphQLMutation;import io.leangen.graphql.annotations.GraphQLQuery;import io.leangen.graphql.spqr.spring.annotations.GraphQLApi;@Component@GraphQLApipublic class UserGraphQL { @Autowired private UserRepository repository; @GraphQLQuery public List&lt;User&gt; getUsers() { return repository.users; } @GraphQLQuery public Optional&lt;User&gt; getUser(long id) { return repository.users.stream() .filter(v -&gt; v.getId() == id) .findAny(); } @GraphQLMutation public User addUser(UserInput input) User newUser = new User(input.getId(), input.getName(), input.getAge()); repository.users.add(newUser); return newUser; }} 그러면 다음과 같은 스키마가 자동으로 생성됩니다. 1234567891011121314151617181920type Mutation { addUser(input: UserInputInput): User}type Query { user(id: Long!): User users: [User]}type User { age: Int! id: Long! name: String}input UserInputInput { age: Int! id: Long! name: String} 보면 함수들이 이름이 살짝씩 바뀌어 있고, input 타입도 이름이 바뀌어 있는 것을 볼 수 있습니다. 이제 다양한 어노테이션을 추가해가면서 스키마를 커스터마이징 할 수 있습니다. 6.1. IDEapplication.properties1graphql.spqr.gui.enabled=true 위의 설정을 한 후 /ide 경로로 접속하면 아래와 같이 GraphQL을 테스트 할 수 있는 웹 페이지를 볼 수 있습니다.","link":"/Spring/Spring-GraphQL/"},{"title":"Spring Boot - JWT","text":"Spring에서 JWT를 다루는 법을 알아보겠습니다 1. 준비build.gradle12345dependencies { implementation 'io.jsonwebtoken:jjwt-api:0.11.2' implementation 'io.jsonwebtoken:jjwt-jackson:0.11.2' runtimeOnly 'io.jsonwebtoken:jjwt-impl:0.11.2'} MyPayload.java12345678@Data@NoArgsConstructor@AllArgsConstructorpublic class MyPayload { private Integer id; private String name; private boolean isMale;} 2. 키 생성1234567// 새로운 키 생성SecretKey key = Keys.secretKeyFor(SignatureAlgorithm.HS256);String secret = Encoders.BASE64.encode(key.getEncoded());// 이미 있는 키 사용String secret = &quot;verylonglonglonglonglonglonglonglonglonglongsecretkey&quot;;SecretKey key = Keys.hmacShaKeyFor(secret.getBytes()); 3. 토큰 생성12345678910String subject = &quot;Foo&quot;;MyPayload payload = new MyPayload(1, &quot;Bar&quot;, true);Date expireDate = new Date(System.currentTimeMillis() + SESSION_TIME * 1000);String token = Jwts.builder() .setSubject(subject) .setExpiration(expireDate) .claim(&quot;payload&quot;, payload) .signWith(key) .compact(); io.jsonwebtoken:jjwt-jackson를 설치하면 JJWT가 자동으로 Jackson 라이브러리를 가져다쓰기 때문에 따로 JSON 매핑을 위한 작업을 하지 않아도 됩니다. 3.1. 커스텀 ObjectMapper 추가만약에 일반적인 JSON 매핑작업이 아니라 커스텀으로 만들어 놓은 ObjectMapper를 사용하고 싶다면 아래와 같이 serializeToJsonWith를 추가하면 됩니다. 1234567String token = Jwts.builder() .setSubject(subject) .setExpiration(expireDate)+ .serializeToJsonWith(new JacksonSerializer&lt;&gt;(objectMapper)) .claim(&quot;payload&quot;, payload) .signWith(key) .compact(); 4. 토큰 검증123456789Claims claims = Jwts.parserBuilder() .setSigningKey(key) .build() .parseClaimsJws(token) .getBody();String subject = claims.getSubject();Date expireDate = claims.getExpiration();MyPayload payload = new ObjectMapper().convertValue(claims.get(&quot;payload&quot;), MyPayload.class); 토큰이 만료된 경우 parseClaimsJws 메서드에서 io.jsonwebtoken.ExpiredJwtException이 발생합니다. 5. 조건을 추가한 토큰 검증123456Claims claims = Jwts.parserBuilder() .setSigningKey(key)+ .requireSubject(&quot;Foo&quot;) // 이렇게 조건을 추가할 수 있다. .build() .parseClaimsJws(token) .getBody(); 위와 같이 토큰을 검증할 때 조건을 추가할 수 있습니다. 다음과 같이 거의 모든 Claim에 조건을 추가할 수 있습니다. 이러한 기능은 Authorization에 유용합니다. 만약 해당 Claim이 없는 경우 io.jsonwebtoken.MissingClaimException이, 해당 Claim이 다른 경우 io.jsonwebtoken.IncorrectClaimException이 발생합니다. 5.1. 사용가능한 토큰 검증 조건 목록12345678requireId(String id);requireSubject(String subject);requireAudience(String audience);requireIssuer(String issuer);requireIssuedAt(Date issuedAt);requireExpiration(Date expiration);requireNotBefore(Date notBefore);require(String claimName, Object value);","link":"/Spring/Spring-JWT/"},{"title":"Spring Boot - JDBC (with MySQL)","text":"Spring Boot에서 JDBC를 사용하여 MySQL을 다루는 법을 알아보겠습니다. 1. JDBC없이 MySQL 접속하기이 부분은 그냥 흥미용으로 읽고 넘어가면 됩니다. build.gradle123dependencies { runtimeOnly 'mysql:mysql-connector-java'} 123456789101112131415161718192021try { String url = &quot;jdbc:mysql://localhost:3306/mydb?serverTimezone=Asia/Seoul&quot;; String username = &quot;username&quot;; String password = &quot;password&quot;; Connection conn = DriverManager.getConnection(url, username, password); PreparedStatement pstmt = conn.prepareStatement(&quot;SELECT * from mytable&quot;); ResultSet res = pstmt.executeQuery(); while(res.next()) { int id = res.getInt(&quot;id&quot;); String name = res.getString(&quot;name&quot;); System.out.println(id + &quot; &quot; + name); } res.close(); pstmt.close(); conn.close();} catch (SQLException ex) { ex.printStackTrace();} 2. Dependency 추가build.gradle1234dependencies { implementation 'org.springframework.boot:spring-boot-starter-jdbc' runtimeOnly 'mysql:mysql-connector-java'} 3. 설정application.properties1234spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/mydb?serverTimezone=Asia/Seoulspring.datasource.username=usernamespring.datasource.password=password 만역에 본인 DB의 TimeZone이 Asia/Seoul이 아닐 경우 본인의 것에 맞게 수정하면 됩니다. 4. JdbcTemplate12@Autowiredprivate JdbcTemplate jdbcTemplate; 특별한 설정 없이 바로 JdbcTemplate 빈을 불러오면 사용할 수 있습니다. 1234567891011121314151617// Select Many RowsList&lt;Map&lt;String, Object&gt;&gt; results = jdbcTemplate.queryForList(&quot;SELECT * FROM table&quot;);// Select One RowMap&lt;String, Object&gt; result = jdbcTemplate.queryForMap(&quot;SELECT * FROM table&quot;);// Select Scalar valueint count = jdbcTemplate.queryForObject(&quot;SELECT COUNT(*) FROM table&quot;, Integer.class);// Insertint insertedRows = jdbcTemplate.update(&quot;INSERT INTO test (id, title) VALUES (1, 'title')&quot;);// Updateint updatedRows = jdbcTemplate.update(&quot;UPDATE test SET title='title2' WHERE id=1&quot;);// Deleteint deletedRows = jdbcTemplate.update(&quot;DELETE FROM test WHERE id=1&quot;); 5. 쿼리문에 변수 사용하기1jdbcTemplate.queryForList(&quot;SELECT * FROM table WHERE id = ?&quot;, 1); 쿼리문에 ?로 PlaceHolder를 지정해놓고 바인딩될 변수를 위와 같은 방법으로 동적으로 넘겨줄 수 있습니다. 이러한 방법은 위에서 한 모든 CRUD에 적용할 수 있습니다. 자세한 사용 방법은 문서 를 참고해주세요. 6. NamedParameterJdbcTemplate123456String sql = &quot;SELECT * FROM table WHERE id = :id&quot;;Map&lt;String, Object&gt; params = new HashMap&lt;&gt;();params.put(&quot;id&quot;, 1);namedParameterJdbcTemplate.queryForList(sql, params) NamedParameterJdbcTemplate를 이용하면 :변수명 형태로 PlaceHolder를 지정할 수 있습니다. 변수를 순서가 아닌 이름으로 넘겨줄 수 있다는 큰 장점이 있습니다. 이러한 방법은 위에서 한 모든 CRUD에 적용할 수 있습니다. 자세한 사용 방법은 문서 를 참고해주세요. 7. 객체로 데이터 받아오기SELECT 문으로 데이터를 받아올 때 매번 Map&lt;String, Object&gt; 형식으로 받아오면 일일히 변환해주는 것도 귀찮습니다. 이제부터 소개할 방법을 사용하면 객체로 데이터를 받아올 수 있게 됩니다. 1234567891011121314public class DemoData { private int id; private String name; public DemoData() {} public void setId(int id) { this.id = id; } public void setName(String name) { this.name = name; }} 앞으로 사용할 객체의 스키마는 위의 클래스를 사용하겠습니다.ㄴ 7.1. RowMapper123456789public class DemoDataRowMapper implements RowMapper&lt;DemoData&gt; { @Override public DemoData mapRow(ResultSet rs, int rowNum) throws SQLException { DemoData d = new DemoData(); d.setId(rs.getInt(&quot;id&quot;)); d.setName(rs.getString(&quot;name&quot;)); return d; }} 1List&lt;DemoData&gt; results = jdbcTemplate.query(&quot;SELECT * FROM table&quot;, new DemoDataRowMapper()); RowMapper 인터페이스를 구현함으로써 query의 결과값을 객체로 받을 수 있습니다. 7.2. Java 8 Lambda123456List&lt;DemoData&gt; results = jdbcTemplate.query(&quot;SELECT * FROM table&quot;, (rs, rowNum) -&gt; { DemoData d = new DemoData(); d.setId(rs.getInt(&quot;id&quot;)); d.setName(rs.getString(&quot;name&quot;)); return d;}); Java 8의 Lambda를 이용하여 RowMapper 인터페이스를 구현하지 않고 Mapping을 할 수 있습니다. 7.3. BeanPropertyRowMapper12RowMapper&lt;DemoData&gt; rowMapper = new BeanPropertyRowMapper&lt;&gt;(DemoData.class);List&lt;DemoData&gt; results = jdbcTemplate.query(&quot;SELECT * FROM table&quot;, rowMapper); 클래스의 멤버 변수 이름과 Table Column 이름이 동일하다면 따로 RowMapper를 구현하지 않고 BeanPropertyRowMapper를 사용하면 자동으로 매핑해줍니다. 단 여기는 다음과 같은 제약사항이 따릅니다. Table Column 이름은 snake_case로, 클래스의 멤버 변수의 이름은 camelCase로 되어있어야 합니다. 클래스에는 인수가 없는 생성자와 모든 Property에 대한 setter 메서드가 존재해야 합니다. String, int, double, java.util.Date 등 Java 기본 데이터 타입만 지원합니다. 7.4. ResultSetExtractorRowMapper는 각각의 Row마다 매핑 메서드가 실행되는 반면 ResultSetExtractor는 직접 ResultSet을 순회하면서 List를 만든 후 반환합니다. 자세한 정보는 다른 곳을 찾아보세요. 7.5. RowCallbackHandlerRowMapper는 Object를, ResultSetExtractor는 List&lt;Object&gt;를 반환하는 반면 RowCallbackHandler는 메서드 내에서 ResultSet를 처리한 후 아무 것도 반환하지 않습니다. 자세한 정보는 다른 곳을 찾아보세요. 8. 기타JdbcTemplate.batchUpdatebatchUpdate 메서드를 사용하면 많은 양의 SQL 구문을 한 번에 처리할 수 있습니다. JdbcTemplate.executeexecute 메서드를 사용하면 DDL 구문을 처리할 수 있습니다. 9. 기본 실행 SQL 파일resources 루트 경로에 schema.sql, data.sql을 둠으로써 어플리케이션이 실행될 때 마다 실행될 SQL 구문을 지정할 수 있습니다. 일반적으로 schema.sql에는 DDL을 data.sql에는 DML을 작성합니다. 이 기능을 사용하려면 application.properties 파일에 아래와 같은 프로퍼티를 설정해야 합니다. application.properties1spring.datasource.initialization-mode=always 자세한 정보는 공식 문서 를 참고해 주세요.","link":"/Spring/Spring-Jdbc/"},{"title":"Spring Boot - Spring Data JPA (with MySQL)","text":"Spring Boot에서 Spring Data JPA를 사용하여 MySQL을 다루는 법을 알아보겠습니다. 1. Dependency 추가build.gradle1234dependencies { runtimeOnly 'mysql:mysql-connector-java' implementation 'org.springframework.boot:spring-boot-starter-data-jpa'} 2. 설정application.properties1234spring.jpa.hibernate.ddl-auto=createspring.datasource.url=jdbc:mysql://localhost:3306/mydb?serverTimezone=Asia/Seoulspring.datasource.username=usernamespring.datasource.password=password spring.jpa.hibernate.ddl-auto의 값의 의미는 다음과 같다. none: 아무 것도 하지 않는다. (MySQL에서의 기본값) validate: DB와 엔티티의 스키마가 맞지 않으면 다른 부분을 출력하고 종료한다. update: 변경된 엔티티의 스키마를 적용한다. create: SessionFactory가 시작될 때, 기존의 테이블을 drop하고 새 테이블을 생성한다. create-drop: create의 동작 + SessionFactory가 종료될 때, 테이블을 drop 한다. 만약 본인 DB의 TimeZone이 Asia/Seoul가 아닐 경우 이에 맞게 바꿔주어야합니다. 3. Entity 클래스 정의12345678910111213@Entity@Table(name=&quot;user&quot;)public class UserEntity { @Id @GeneratedValue private Integer id; @Column(nullable=false) private String name; @Column(columnDefinition=&quot;DATETIME(0) default CURRENT_TIMESTAMP&quot;) private OffsetDateTime dateTime;} 컬럼 타입에 Integer, String 뿐만 아니라 Double, Char, Boolean등 다양한 Java Primitive Type을 사용할 수 있습니다. String은 컬럽 타입이 VARCHAR(255)로 자동으로 지정됩니다. @Table이나 @Column에서 name을 명시헤주지 않으면 해당 클래스나 멤버 변수의 이름을 snale_case로 변환한 것을 사용합니다. nullable을 false로 지정해주지 않으면 true가 기본값입니다. columnDefinition을 통해 컬럼 타입을 직접 지정해줄 수 있습니다. LocalDateTime, OffsetDateTime, ZonedDateTime은 별다른 지정이 없어도 DATETIME으로 타입이 지정됩니다. 하지만 기본으로 소수점 6자리를 사용하도록 설정되기 때문에 저는 저렇게 길이를 0으로 명시하는 편입니다. columnDefinition 뒤에 default [값]을 붙여서 기본값을 지정해줄 수 있습니다. 컬럼이 시간, 날짜와 관련된 타입일 경우 CURRENT_TIMESTAMP를 붙여서 레코드가 생성될 때의 시간을 기본값으로 할 수 있습니다. 3.1. Default가 정의된 컬럼 다루기123@Column()@ColumnDefault(&quot;100&quot;)private Integer score; // DB에 NULL이 저장됨! 위와 같이 Default값이 설정되어 있는 컬럼에 아무 것도 넣지 않고 save를 하면 DB에는 기본값인 100이 아니라 NULL값이 저장되어 있습니다. 이는 Hibernate가 INSERT 구문을 생성할 때 아래와 같이 모든 값을 명시하기 때문입니다. 123INSERT INT user (name, score) VALUE ('John', NULL) -- Hibernate가 생성한 QueryINSERT INT user (name) VALUE ('John') -- 이렇게 되어야 score에 기본값이 들어간다. 이 문제를 해결하기 위해 2가지 방법이 있습니다. 1) @DynamicInsert 사용하기단순히 엔티티 클래스에 @DynamicInsert 어노테이션을 사용하면 됩니다. @DynamicInsert 어노테이션은 값이 NULL인 컬럼을 INSERT 구문에 넣지 않는 기능을 합니다. 추가로 @DynamicUpdate 어노테이션도 존재하는데 이름에서 유추할 수 있듯이 값이 NULL인 컬럼을 UPDATE 구문에 넣지 않는 기능을 합니다. 123456@Entity@Table(name=&quot;user&quot;)@DynamicInsertpublic class UserEntity { } 아래와 같이 엔티티를 save한 후에도 DB에는 기본값이 저장되어 있으나, 엔티티 객체에서는 해당 컬럼의 값이 그대로 null로 남아있다는 단점이 있습니다. 1234repository.save(entity);// DB에는 기본값인 100이 저장되어 있으나 여기서는 null이 저장되어 있음Integer score = entity.getScore(); // null 2) 엔티티 클래스 멤버 변수에 기본값 주기@ColumnDefault 어노테이션을 사용하지 않고 단순히 Java 코드 단계에서 기본값을 정의합니다. DB 스키마에서 기본값을 확인할 수 없는 단점이 있습니다. 123456@Entity@Table(name=&quot;user&quot;)public class UserEntity { @Column() private Integer score = 100;} 3.2. NOT NULL과 Default를 동시에 정의하기어떤 컬럼을 NOT NULL로 선언하고 해당 컬럼에 기본값을 주고 싶은 경우가 종종있습니다. 하지만 아래와 같이 컬럼을 정의하고 해당 컬럼의 값을 넣지 않고 save를 하면 org.hibernate.PropertyValueException이 발생합니다. 123@Column(nullable=false)@ColumnDefault(&quot;100&quot;)private Integer score; 이 컬럼의 nullable 여부는 INSERT 구문을 생성하기 전에 확인하는데 default값은 INSERT 구문이 실행해야 집어넣어지기 때문입니다. 이 문제를 해결하기 위해서는 아래와 같이 2가지 방법이 있습니다. 1234567// 방법 1@Column(columnDefinition=&quot;INT default 100 NOT NULL&quot;)private Integer score;// 방법 2@Column(nullable=false)private Integer score = 100; columnDefinition에서 컬럼의 타입과 기본값과 NOT NULL을 동시에 정의하는 방법 Java 코드에 직접 기본값을 정의 4. Repository 인터페이스 정의123public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt; {} 실수로 내용을 빼먹은 것이 아닙니다. 단순히 JpaRepository만 상속하면 준비는 끝났습니다. JpaRepository를 상속할 때 두번째 제너릭은 Entity의 @Id 컬럼의 타입을 넣어주면 됩니다. 5. 간단한 CRUD5.1. SELECT12345678@Beanprivate UserRepository userRepository;public void select() { List&lt;UserEntity&gt; list = repository.findAll(); Optional&lt;UserEntity&gt; user = repository.findById(1); long num = repository.count();} 5.2. INSERT123456789@Beanprivate UserRepository userRepository;public void insert() { UserEntity user = new UserEntity(&quot;John&quot;); repository.save(user); int id = user.getId(); // 1} repository에 save한 후 생성된 기본키를 가져올 수 있습니다. 5.3. UPDATE12345public void update() { UserEntity user = repository.findById(1).orElseThrow(() -&gt; new IllegalArgumentException(&quot;Not Found&quot;)); user.setName(&quot;James&quot;); repository.save(user);} 5.4. DELETE1234567public void delete() { UserEntity user = userRepository.findById(1).orElseThrow(() -&gt; new IllegalArgumentException(&quot;Not Found&quot;)); userRepository.delete(user); // 또는 repository.deleteById(1);} 6. 조건절이 있는 CRUD6.1. 메서드명으로부터 쿼리 생성12345public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt; { List&lt;UserEntity&gt; findByName(String name); List&lt;UserEntity&gt; findByNameAndScore(String name, int score); long countByName(String name);} 이렇게 단순히 UserRepository 인터페이스에 메서드를 추가해주는 것으로 검색 쿼리를 생성할 수 있습니다. And, Or, Between, LessThan, Like 등 다양한 조건을 추가해줄 수 있습니다. 자세한 정보는 문서 를 참고해 주세요 메서드의 파라미터 이름이 메서드 이름에 적힌 프로퍼티 이름과 같아야할 필요는 없습니다. 단순히 메서드의 파라미터의 순서대로 바인드되기 때문입니다. 6.2. @Query 어노테이션하지만 위와 같은 방법으로는 SELECT 쿼리만 사용할 수 있을뿐만 아니라 소괄호를 통한 연산 우선순위를 적용하는 등 복잡한 쿼리를 할 수 없습니다. 이제부터 직접 쿼리문을 작성하는 방법을 소개하겠습니다. 1234567891011121314public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt; { @Query(&quot;SELECT u FROM UserEntity u WHERE u.name = :name&quot;) List&lt;UserEntity&gt; findByName(String name); @Transactional @Modifying @Query(&quot;UPDATE UserEntity u SET u.name = :to WHERE u.name = :from&quot;) Integer updateName(String from, String to); @Transactional @Modifying @Query(&quot;DELETE FROM #{#entityName} u WHERE u.name = :name&quot;) Integer deleteByName(String name);} 여기서 @Query 어노테이션 안에 있는 쿼리문은 일반적인 SQL이 아니라 JPQL이라는 Java 독자적인 쿼리 언어입니다. 여기서 테이블명이나 컬럼명 대신 엔티티 클래스 이름과 앤티티 클래스의 멤버 번수 이름을 사용합니다. UPDATE나 DELETE같이 DB를 변경해야하는 경우에는 반드시 @Transactional, @Modifying 어노테이션을 사용해야합니다. 자세한 사용방법은 여기를 참고해 주세요. JPQL에 엔티티 클래스 이름을 직접 집어넣지 않고 #{#entityName}을 넣음으로써 현재 레포지토리의 엔티티 클래스 이름을 자동으로 바인딩 할 수 있습니다. 6.3. Example하지만 위와같은 방법으로는 검색 조건의 수가 유동적인 경우에는 사용할 수 없습니다. 예시 엔티티를 생성해서 비교하는 벙법을 소개하겠습니다. 1234UserEntity toCompare = new UserEntity(1, &quot;John&quot;, null);Example&lt;UserEntity&gt; ex = Example.of(toCompare);List&lt;UserEntity&gt; users = repository.findAll(ex);users.forEach(System.out::println); 예시 엔티티를 생성해서 Example.of메서드로 예시를 생성하고 findAll이나 findOne 메서드에 넘겨주면 됩니다. 예시 엔티티에서 값이 NULL아닌 모든 프로퍼티와 비교합니다. ExampleMatcher를 통해 문자열의 대소문자 구분 없음, 정규표현식, 포함관계의 비교를 할 수 있습니다. 자세한 사용방법은 여기를 참고해 주세요. 6.4. JpaSpecificationExecutor하지만 위와같은 방법으로는 검색 조건의 수가 유동적이며 범위 검색이 있는 경우에는 사용할 수 없습니다. Repository에 JpaSpecificationExecutor를 상속하는 방법을 소개하겠습니다. 123public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt;, JpaSpecificationExecutor&lt;UserEntity&gt; {} 먼저 Repository 인터페이스에 추가로 JpaSpecificationExecutor를 상속합니다. 123456789public class UserSpecs { public static Specification&lt;UserEntity&gt; withName(final String name) { return (root, query, builder) -&gt; builder.equal(root.get(&quot;name&quot;), name); } public static Specification&lt;UserEntity&gt; withScoreRange(final Integer from, final Integer to) { return (root, query, builder) -&gt; builder.between(root.get(&quot;score&quot;), from, to); }} 먼저 각각의 조건 항을 정의하는 메서드를 만듭니다. 여기서 builder는 CriteriaBuilder라는 클래스인데 equal, between 뿐만 아니라 여러분이 생각할 수 있는 모든 것이 다 있습니다. 자세한 정보는 문서를 확인해주세요. 1234567891011Specification&lt;UserEntity&gt; spec = Specification.where(null);if (map.containsKey(&quot;name&quot;)) { spec = spec.and(UserSpecs.withName(map.get(&quot;name&quot;)));}if (map.containsKey(&quot;fromScore&quot;) &amp;&amp; map.containsKey(&quot;toScore&quot;)) { spec = spec.and(UserSpecs.withScoreRange(map.get(&quot;fromScore&quot;), map.get(&quot;toScore&quot;)));}List&lt;UserEntity&gt; users = repository.findAll(spec); 위와같이 파라미터로 넘어온 map에 키가 있는지 없는지 체크하면서 조건을 추가해줄 수 있습니다. 여기서 and 메서드말고 or, not 메서드도 사용할 수 있습니다. 7. 정렬과 페이징처리7.1. Repository의 기본 메서드1) 정렬1List&lt;UserEntity&gt; list = repository.findAll(Sort.by(Sort.Direction.ASC, &quot;score&quot;)); Sort 클래스를 통해 정렬을 할 수 있습니다. 2) 페이징123List&lt;UserEntity&gt; list = repository.findAll(PageRequest.of(0, 10));List&lt;UserEntity&gt; list = repository.findAll(PageRequest.of(0, 10, Sort.by(Sort.Direction.ASC, &quot;score&quot;))); PageRequest 클래스를 통해 페이징 처리를 할 수 있습니다. 페이지는 0부터 시작합니다. 세번째 파라미터로 Sort 객체를 넘겨줌으로써 페이징 처리와 정렬을 동시에 할 수 있습니다. 자세한 정보는 PageRequest 공식 문서, Sort 공식 문서 를 참고해 주세요. 7.2. 메서드명으로부터 쿼리 생성1) 정렬1234public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt; { List&lt;UserEntity&gt; findByNameOrderByScoreDesc(String name); List&lt;UserEntity&gt; findByName(String name, Sort sort);} 메서드 명뒤에 OrderBy[프로퍼티이름][Desc|Asc]를 붙이면 됩니다. 아니면 Sort 객체를 맨 끝에 파라미터로 추가하면 됩니다. 2) 페이징12345public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt; { List&lt;UserEntity&gt; findByName(String name, Pageable pageable); Slice&lt;UserEntity&gt; findByName(String name, Pageable pageable); Page&lt;UserEntity&gt; findByName(String name, Pageable pageable);} Pageable 객체를 맨 끝에 파라미터로 추가하면 됩니다. 반환 타입도 위의 3개중에 아무거나 하나 고르면 JPA가 다 알아서 합니다. SlicehasNext, hasPrevious, isFirst, isLast의 메서드를 통해 다음, 이전 페이지가 존재하는지, 현재 페이지가 처음이나 마지막인지 알 수 있습니다. 이외에도 다양한 기능을 제공하니 자세한 정보는 문서 를 확인해 주세요. Page위의 Slice를 상속받은 것이기 때문에 Slice의 모든 기능을 쓸 수 있습니다. 내부적으로 COUNT 함수를 실행시켜서 검색 결과의 총 갯수를 구합니다. COUNT 함수의 오버헤드가 큰 경우 별로 추천하지 않습니다. getTotalElements, getTotalPages 메서드를 통해 전체 결과 갯수, 전체 페이지 수를 구할 수 있습니다. 자세한 정보는 문서 를 확인해 주세요. 7.3. @Query 어노테이션1234public interface UserRepository extends JpaRepository&lt;UserEntity, Integer&gt; { @Query(&quot;SELECT u FROM UserEntity u WHERE u.name = :name&quot;) Page&lt;UserEntity&gt; findByName(String name, Pageable pageable);} 위에서 한 것 처럼 Sort나 Pageable객체를 파라미터에 추가하고 반환 타입을 List, Slice, Page로 바꾸면 JPA가 다 알아서 합니다. 7.4. Example1List&lt;UserEntity&gt; users = repository.findAll(ex, PageRequest.of(0, 10)); 메서드 파라미터 뒤에 Sort나 Pageable객체를 넘겨주면 됩니다. 7.5. JpaSpecificationExecutor1List&lt;UserEntity&gt; users = repository.findAll(spec, PageRequest.of(0, 10)); 마찬가지로 메서드 파라미터 뒤에 Sort나 Pageable객체를 넘겨주면 됩니다. 8. 기본 실행 SQL 파일resources 폴더 루트에 import.sql 이라는 파일이 있으면 Hibernate는 Entity를 통한 Table 자동 생성이 끝난 후 이 파일을 실행합니다. 보통 실행때마다 데이터를 Drop하는 설정에서 요긴하게 쓰입니다. Appendixdependency에 JPA를 포함했지만 JPA 설정을 건너뛰고 싶은 경우1234spring.autoconfigure.exclude= \\ org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration, \\ org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration, \\ org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration","link":"/Spring/Spring-Jpa/"},{"title":"알 수 없는 ConsoleAppender 에러가 발생하는 경우","text":"ch.qos.logback.core.ConsoleAppender[console] - Appender [console] failed to append. io.mockk.MockKException: No other calls allowed in stdObjectAnswer than equals/hashCode/toString 에러가 나는 경우 원인을 알아보도록 하겠습니다. 어느날 아래와 같은 에러가 나면서 테스트 일부가 실패하는 현상이 나타났습니다. ERROR in ch.qos.logback.core.ConsoleAppender[console] - Appender [console] failed to append. io.mockk.MockKException: No other calls allowed in stdObjectAnswer than equals/hashCode/toString Stacktrace 에는 제가 작성한 코드가 존재하지 않고, 구글에 검색해도 해결 방법을 전혀 찾을 수 없었습니다. 정말 긴 시간의 삽질을 한 이후에 원인을 찾을 수 있었습니다. 원인알고보니 다른 테스트에서 Calendar.getInstance()를 mockking 한 것이 원인이었습니다. 아래와 같은 과정 때문에 해당 에러가 발생한 것으로 추정됩니다. 특정 테스트에서 Calendar.getInstance()가 mock을 반환하도록 설정 이 설정을 해제 하지 않은 상태로 테스트 종료 그 후의 다른 테스트에서 Spring Context를 로드하는 과정에서 Calendar.getInstance()를 실행하여 Calendar 인스턴스를 생성함. 하지만 그 인스턴스는 mock 객체였고 이 mock 객체를 사용하는 과정에서 위와 같은 에러가 발생 재현 방법logback-spring.xml1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;appender name=&quot;CONSOLE&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;Pattern&gt; %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n &lt;/Pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; CustomTest.kt1234567891011class CustomTest: FreeSpec({ beforeSpec { mockkStatic(Calendar::class) val calendar = mockk&lt;Calendar&gt;() every { Calendar.getInstance() } returns calendar } &quot;test&quot; { Thread.sleep(1000) }}) TmpApplicationTest.kt1234567@SpringBootTestclass TmpApplicationTest { @Test fun contextLoads() { println(&quot;contextLoads&quot;) }} 위와 같이 코드를 작성하고 CustomTest가 TmpApplicationTest 보다 먼저 실행되면 TmpApplicationTest에서 위와 같은 에러가 발생합니다. 결론static 메서드의 mockking은 신중하게 다뤄야하고, 꼭 테스트가 종료될 때 모든 mock을 해제 해야 합니다.","link":"/Spring/Spring-Mockk-Error/"},{"title":"Spring Boot - MyBatis (with MySQL)","text":"Spring Boot에서 MyBatis를 사용하여 MySQL을 다루는 법을 알아보겠습니다. 1. Dependency 추가 Gradle Maven build.gradle1234dependencies { implementation 'org.mybatis.spring.boot:mybatis-spring-boot-starter:2.1.4' runtimeOnly 'mysql:mysql-connector-java'} pom.xml123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2. 설정application.properties12345spring.datasource.url=jdbc:mysql://localhost:3306/mydb?serverTimezone=Asia/Seoulspring.datasource.username=usernamespring.datasource.password=passwordmybatis.mapper-locations=mapper/**/*.xml 3. model 선언User.java123456public class User { private long id; private String name; private int age; private OffsetDateTime date;} MyBatis에서 Mapper가 반환하는 클래스의 경우에는 위와 같이 Setter나 AllArgsConstructor가 없어도 가능합니다 4. 파라미터가 없는 CRUDUserMapper.java12345678910111213import org.apache.ibatis.annotations.Mapper;@Mapperpublic interface UserMapper { List&lt;User&gt; getUserList(); User getUser(); // Optional&lt;User&gt; getUser(); 도 가능합니다. int countUser(); int insert(); // 추가된 row 수를 반환합니다. int update(); // 변경된 row 수를 반환합니다. int delete(); // 삭제된 row 수를 반환합니다. // 반환타입에 int 대신 void 또는 long을 사용해도 됩니다.} resources 폴더에 mapper 폴더를 생성하고 아래의 파일을 추가합니다. user-mapper.xml1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.example.demo.UserMapper&quot;&gt; &lt;select id=&quot;getUserList&quot; resultType=&quot;com.example.demo.User&quot;&gt; SELECT * FROM user &lt;/select&gt; &lt;select id=&quot;getUser&quot; resultType=&quot;com.example.demo.User&quot;&gt; SELECT * FROM user LIMIT 1 &lt;/select&gt; &lt;select id=&quot;countUser&quot; resultType=&quot;int&quot;&gt; SELECT COUNT(*) FROM user &lt;/select&gt; &lt;insert id=&quot;insert&quot;&gt; INSERT INTO user (name, age, date) VALUES ('James', 22, NOW()) &lt;/insert&gt; &lt;update id=&quot;update&quot;&gt; UPDATE user SET age = 23 &lt;/update&gt; &lt;delete id=&quot;delete&quot;&gt; DELETE FROM user WHERE ![CDATA[ age &gt; 20 ]] &lt;/delete&gt;&lt;/mapper&gt; SQL문 안에 &lt; 나 &gt;를 사용하고 싶은 경우 21번째 줄의 ![CDATA[ age &gt; 20 ]]와 같이 CDATA 섹션을 사용해야 합니다. mybatis.type-aliases-package=com.example.demo 설정을 사용하면 4번째 줄의 resultType을 앞의 패키지 명을 생략하여 User처럼 쓸 수 있습니다. 5. 파라미터가 있는 CRUD1) 파라미터가 하나이고 자바 기본 데이터 타입인 경우 (String, Integer 등)UserMapper.java1234@Mapperpublic interface UserMapper { User getUserByName(String name);} user-mapper.xml123&lt;select id=&quot;getUserByName&quot; resultType=&quot;com.example.demo.User&quot;&gt; SELECT * FROM user WHERE #{any_name}&lt;/select&gt; mapper java 파일에서 선언되어 있는 파라미터의 이름과 관계없이 아무 이름을 mapper xml 파일에 적으면 됩니다. 어차피 파라미터가 하나 뿐이기 때문에 MyBatis가 이름과 관계없이 파라미터를 넣어줍니다. 2) 파라미터가 하나이고 커스텀으로 생성된 Object인 경우Pagination.java1234public class Pagination { private int limit; private int offset;} UserMapper.java12345@Mapperpublic interface UserMapper { List&lt;User&gt; getUserWithSearchQuery(Pagination pagination); int insertUser(User user);} user-mapper.xml123456&lt;select id=&quot;getUserWithSearchQuery&quot; resultType=&quot;com.example.demo.User&quot;&gt; SELECT * FROM user LIMIT #{limit} OFFSET #{offset}&lt;/select&gt;&lt;insert id=&quot;insertUser&quot; parameterType=&quot;com.example.demo.User&quot;&gt; INSERT INTO user (name, age, date) VALUES (#{name}, #{age}, #{date})&lt;/insert&gt; 위와 같이 Object의 프로퍼티 명을 그대로 적으면 됩니다. MyBatis에서 Mapper가 파라미터로 사용하는 클래스의 경우에는 위와 같이 Getter가 없어도 가능합니다 insert, update, delete 태그에서는 일반적으로 위와 같이 parameterType를 사용해주는데 이는 생략해도 됩니다. 3) 파라미터가 여러 개인 경우mapper java 파일에서 메서드의 파라미터 명은 컴파일 할 때 변경될 수 있기 때문에 MyBatis에서는 이를 사용하지 않습니다. 따라서 아래와 같이 @Param 어노테이션을 이용해 변수 명을 명시적으로 정의합니다. 또한 파라미터가 Object인 경우 @Param으로 지정한 파라미터 명.프로퍼티 명으로 값을 넣어줄 수 있습니다. UserMapper.java12345678910import org.apache.ibatis.annotations.Param;@Mapperpublic interface UserMapper { List&lt;User&gt; getUserByNameAndAge( @Param(&quot;name&quot;) String name, @Param(&quot;age&quot;) int age, @Param(&quot;pagination&quot;) Pagination pagination );} user-mapper.xml123&lt;select id=&quot;getUserByNameAndAge&quot; resultType=&quot;com.example.demo.User&quot;&gt; SELECT * FROM user WHERE name = #{name} AND age = #{age} LIMIT #{pagination.limit} OFFSET #{pagination.offset}&lt;/select&gt; 6. #{변수명}과 ${변수명}우리는 지금까지 SQL에 값을 삽입할 때 #{변수명} 만 사용했습니다. 하지만 MyBatis에서는 ${변수명}으로도 값을 삽입할 수 있습니다. 이 두 방식에 차이를 알아보겠습니다. 1) #{변수명}12SELECT * FROM user WHERE age = #{age} -- mapper xml에 작성한 sqlSELECT * FROM user WHERE age = ? -- MyBatis에 의해 변환된 sql mapper xml 파일에 적혀있는 #{변수명}은 MyBatis가 위와 같이 java.sql.PreparedStatement의 바인드 변수로 변환됩니다. 그런 다음 PreparedStatement의 API를 이용해 값을 집어 넣습니다. 보통 값을 넣을 때 사용합니다. 2) ${변수명}12SELECT * FROM user ORDER BY ${column} -- mapper xml에 작성한 sqlSELECT * FROM user ORDER BY age -- MyBatis에 의해 변환된 sql 해당 변수를 그대로 문자열로 대체합니다. SQL Injection과 같은 공격에 노출될 수 있으므로 사용에 유의하셔야합니다. 보통 테이블 명이나 컬럼 명을 넣을 때 사용합니다. 7. 결과값 매핑데이터베이스의 컬럼명과 자바에서 사용하는 객체의 프로퍼티 명이 다른 경우가 종종 있습니다. 이럴 경우 다음과 같이 이 둘을 매핑시켜줄 수 있습니다. 1) snake_case, camelCase일반적으로 데이터베이스에서는 snake_case를, 자바에서는 camelCase를 사용합니다. 다음과 같은 설정을 하면 MyBatis에서 자동으로 이 둘을 매핑시킵니다. application.properties1mybatis.configuration.map-underscore-to-camel-case=true 2) 그 외의 경우user-mapper.xml123456789101112&lt;mapper namespace=&quot;com.example.demo.UserMapper&quot;&gt; &lt;resultMap id=&quot;userResultMap&quot; type=&quot;com.example.demo.User&quot;&gt; &lt;id column=&quot;user_id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;username&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;age&quot; property=&quot;age&quot; /&gt; &lt;result column=&quot;reg_date&quot; property=&quot;regDate&quot; /&gt; &lt;/resultMap&gt; &lt;select id=&quot;getUser&quot; resultMap=&quot;userResultMap&quot;&gt; SELECT * FROM user &lt;/select&gt;&lt;/mapper&gt; resultMap 태그에 id를 정의하고 이를 각 구문 태그의 resultMap에 사용함으로써 매핑을 시켜줍니다. (2, 9번째 줄) Primary Key에는 id 태그를, 그 외의 컬럼에는 result 태그를 사용합니다. (3 ~ 6번째 줄) column에는 데이터베이스 컬럼명을, property에는 자바 객체의 프로퍼티명을 적습니다. (3 ~ 6번째 줄) 데이터베이스 컬럼명과 자바 객체의 프로퍼티명이 같거나, 바로 위에서 설명한 map-underscore-to-camel-case 설정으로 자동으로 매핑이 가능할 경우 굳이 명시하지 않아도 됩니다. (5 ~ 6번째 줄은 생략가능)","link":"/Spring/Spring-MyBatis/"},{"title":"Spring Boot - 커스텀 Tomcat 에러 페이지 설정하기","text":"Spring에서 커스텀 Tomcat 에러 페이지를 설정 하는 법을 알아보겠습니다. 1. Tomcat 에러 페이지 Spring Boot 프로젝트에서 http://localhost:8080/[] 로 접근하면 위와 같은 페이지를 볼 수 있습니다. 이 페이지는 우리가 늘 보던 Whitelabel 에러 페이지와는 다른 모습입니다. HTTP/1.1 명세에 따르면 URI 경로에 포함할 수 없는 특수 문자들이 정의되어 있는데, 이러한 특수 문자를 포함하여 HTTP 요청을 보내면 스프링 컨테이너로 전달되기 전 Tomcat 단에서 에러 페이지를 응답하기 때문에 그렇습니다. 따라서 Spring 에러 페이지 설정과는 다른 방법의 설정이 필요합니다. 2. 커스텀 Tomcat 에러 페이지 정의아래와 같이 ErrorReportValve 를 상속하여 커스텀 Tomcat 에러 페이지를 정의합니다. 1234567891011121314151617181920import java.io.IOException;import java.io.Writer;import org.apache.catalina.connector.Request;import org.apache.catalina.connector.Response;import org.apache.catalina.valves.ErrorReportValve;public class CustomTomcatErrorValve extends ErrorReportValve { protected void report(Request request, Response response, Throwable throwable) { if (!response.setErrorReported()) return; try { Writer writer = response.getReporter(); writer.write(&quot;&lt;h1&gt;This is custom tomcat error page!&lt;/h1&gt;&quot;); response.finishResponse(); } catch (IOException e) { e.printStackTrace(); } }} line 9: 이 메서드에서 에러 페이지를 응답했다는 것을 기록하기 위한 용도 입니다. 이미 응답이 된 경우에는 바로 return 합니다. line 14: 응답할 에러 페이지를 정의합니다. 3. 커스텀 Tomcat 에러 페이지 등록아래와 같이 TomcatWebSocketServletWebServerCustomizer를 상속하여 위에서 정의한 커스텀 Tomcat 에러 페이지를 등록하는 Bean을 생성합니다. 1234567891011121314151617181920212223import org.apache.catalina.Container;import org.apache.catalina.core.StandardHost;import org.springframework.boot.autoconfigure.websocket.servlet.TomcatWebSocketServletWebServerCustomizer;import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;import org.springframework.stereotype.Component;@Componentpublic class TomcatErrorPageCustomizer extends TomcatWebSocketServletWebServerCustomizer { @Override public void customize(TomcatServletWebServerFactory factory) { factory.addContextCustomizers((context) -&gt; { Container parent = context.getParent(); if (parent instanceof StandardHost) { ((StandardHost)parent).setErrorReportValveClass(&quot;kim.hyunsub.demo.CustomTomcatErrorValve&quot;); } }); } @Override public int getOrder() { return 100; }} line 14: 위에서 정의한 CustomTomcatErrorValve의 패키지 경로를 입력합니다. line 21: Spring에서 정의하는 TomcatWebServerFactoryCustomizer가 먼저 설정되어야 하므로 0을 초과하는 값으로 우선순위를 정해줍니다. Github 코드 자세한 정보는 여기를 확인해주세요. 4. 결과위와 같이 설정하고 다시 http://localhost:8080/[]에 접속하면 아래와 같이 우리가 설정한 커스텀 Tomcat 에러 페이지를 볼 수 있습니다. 부록. URI 경로에 특수 문자 허용HTTP/1.1 명세에 따르면 URI 경로에 포함할 수 없는 특수 문자들이 정의되어 있습니다. 이러한 특수 문자를 Spring Configuration Property로 허용해 줄 수 있습니다. 12server.tomcat.relaxed-path-chars=&quot;,&lt;,&gt;,[,\\,],^,`,{,|,}server.tomcat.relaxed-query-chars=&quot;,&lt;,&gt;,[,\\,],^,`,{,|,} 자세한 정보는 Spring 공식 문서와 Tomcat 공식 문서를 확인해주세요","link":"/Spring/Spring-Tomcat-Error-Page/"},{"title":"Spring Boot - WebSocket","text":"Spring Boot 에서 WebSocket을 Controller 처럼 사용하는 법을 알아보겠습니다. 0. 들어가며STOMPSTOMP는 HTTP처럼 Text 기반 메시징 프로토콜입니다. HTTP 프로토콜 같다고 생각하시면 됩니다. spring-web이 HTTP의 path를 기반으로 @RequestMapping에 라우팅을 해준다면spring-websocket은 STOMP의 destination을 기반으로 @MessageMapping에 라우팅 해줍니다. SockJS SockJS는 WebSocket을 사용할 수 없는 브라우저에서도 다른 방법을 이용해 WebSocket과 같은 동작을 수행할 수 있게 해주는 라이브러리입니다. 하지만 위 사진에서 알 수 있듯이 요즘 모든 브라우저에서는 WebSocket을 지원하기 때문에, 굳이 SockJS를 사용할 필요가 없습니다. 따라서 우리 예제에서는 SockJS를 사용하지 않습니다. 만약 필요하다면 이 글을 전부 따라하신 다음에 별도 가이드를 참고해주세요. 1. Dependency 추가build.gradle123dependencies { implementation 'org.springframework.boot:spring-boot-starter-websocket'} 2. Spring Boot 설정WebSocketConfiguration.java123456789@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfiguration implements WebSocketMessageBrokerConfigurer { @Override public void registerStompEndpoints(StompEndpointRegistry registry) { registry.addEndpoint(&quot;/socket&quot;) .setAllowedOriginPatterns(&quot;*&quot;); }} 실제 /socket 경로로 웹 소켓 연결을 합니다. nginx 에서 웹 소켓 설정을 할 때 이 경로로 하시면 됩니다. WebSocketController.java1234567@Controllerpublic class WebSocketController { @MessageMapping(&quot;/ping&quot;) public String ping(WebSocketPayload payload) { return payload.toString(); }} 여기서 /ping은 HTTP 상의 경로가 아니라 STOMP 프로토콜 상의 destination 경로입니다. 3. Web 설정StompJS 문서 1$ npm install @stomp/stompjs 1234567891011121314151617import * as StompJS from '@stomp/stompjs';const stomp = new StompJS.Client({ brokerURL: `wss://${window.location.host}/socket`,});// 아래와 같이 연결을 시작해야 메시지를 발송할 수 있습니다.stomp.activate();// 아래와 같이 메시지를 발송하시면 됩니다.stomp.publish(({ destination: '/ping', body: JSON.stringify({ message: 'Hello, World!' }),}));// 아래와 같이 연결을 종료 할 수 있습니다.stomp.deactivate(); 4. nginx 설정nginx.conf1234567location /socket { proxy_pass http://localhost:8080 proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_cache_bypass $http_upgrade;} 5. Interceptor 설정WebSocketInterceptor.java12345678910111213public class WebSocketInterceptor implements HandshakeInterceptor { @Override public boolean beforeHandshake(ServerHttpRequest request, ServerHttpResponse response, WebSocketHandler wsHandler, Map&lt;String, Object&gt; attributes) { String property = parseProperty(request); attributes.put(&quot;property&quot;, property); return true; } @Override public void afterHandshake(ServerHttpRequest request, ServerHttpResponse response, WebSocketHandler wsHandler, Exception exception) { }} 위와 같이 attributes 에 값을 넣으면 각 Controller 에서 해당 값을 꺼내볼 수 있습니다. beforeHandshake의 리턴 값을 false로 하면 연결을 거절할 수 있습니다. WebSocketConfiguration.java12345678910@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfiguration implements WebSocketMessageBrokerConfigurer { @Override public void registerStompEndpoints(StompEndpointRegistry registry) { registry.addEndpoint(&quot;/socket&quot;)+ .addInterceptors(WebSocketInterceptor()) .setAllowedOriginPatterns(&quot;*&quot;); }} 위와 같이 Interceptor를 추가합니다. WebSocketController.java12345678@Controllerpublic class WebSocketController { @MessageMapping(&quot;/ping&quot;) public String ping(WebSocketPayload payload, SimpMessageHeaderAccessor accessor) { String property = (String) accessor.getSessionAttributes().get(&quot;property&quot;); return payload.toString(); }} SimpMessageHeaderAccessor를 두 번째 파라미터에 추가한 다음에 위와 같이 Interceptor에서 추가한 값을 꺼내볼 수 있습니다.","link":"/Spring/Spring-WebSocket/"},{"title":"Terminal로 Spring Boot 다루기","text":"IntelliJ나 eclipse와 같은 IDE의 도움을 받지 않고 Spring을 빌드, 실행, 테스트 하는 방법을 알아보겠습니다. Maven과 Gradle 모두 다루고 있습니다. 1. 프로젝트 생성하기1.1. Spring Boot 프로젝트 템플릿 다운로드먼저 Spring Initializr에 들어간 다음 원하는 옵션을 입력한 후, 하단의 GENERATE 버튼을 누르면 zip 파일로 프로젝트 템플릿을 다운로드 받을 수 있습니다. 본 문서는 Gradle과 Java 8을 기반으로 작성되어 있으므로 해당 옵션으로 변경합니다. 매번 사이트에 접속해서 다운로드 받는 것이 번거롭다면 아래와 같이 curl 명령어를 이용해 터미널로 바로 다운로드 받을 수 있습니다. 1234567891011121314151617181920# default값과 다른 옵션만 명시$ curl https://start.spring.io/starter.zip \\ -d 'type=gradle-project' \\ -d 'baseDir=demo' \\ -d 'javaVersion=1.8' \\ --output starter.zip# 모든 옵션 명시$ curl https://start.spring.io/starter.zip \\ -d 'type=gradle-project' \\ -d 'language=java' \\ -d 'bootVersion=2.4.0.RELEASE' \\ -d 'baseDir=demo' \\ -d 'groupId=com.example' \\ -d 'artifactId=demo' \\ -d 'name=demo' \\ -d 'packageName=com.example.demo' \\ -d 'packaging=jar' \\ -d 'javaVersion=1.8' \\ --output starter.zip 보다 자세한 정보는 문서를 참고해 주시길 바랍니다. 1.2. Spring Boot 프로젝트 실행아래 명령어를 통해 Spring Boot 프로젝트를 실행합니다. Gradle Maven 1$ ./gradlew bootRun 1$ ./mvnw spring-boot:run 아래와 같은 출력이 나왔으면 성공입니다. 1234567891011 . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.4.0)2020-12-03 01:34:07.779 INFO 2041 --- [ main] com.example.demo.DemoApplication : Starting DemoApplication using Java 15.0.1 on Hyunsub-MacBookPro.local with PID 2041 (/Users/hyunsub/Documents/Spring/demo/build/classes/java/main started by hyunsub in /Users/hyunsub/Documents/Spring/demo)2020-12-03 01:34:07.782 INFO 2041 --- [ main] com.example.demo.DemoApplication : No active profile set, falling back to default profiles: default2020-12-03 01:34:08.236 INFO 2041 --- [ main] com.example.demo.DemoApplication : Started DemoApplication in 1.07 seconds (JVM running for 1.403) 2. 프로젝트 빌드하기 Gradle Maven build.gradle pom.xml 에 변경된 부분이 있으면 반영하고 java파일을 class파일로 컴파일하는 작업입니다. 1$ ./gradlew build 1$ ./mvnw compile 3. 프로젝트를 jar 파일로 빌드하기 Gradle Maven 1$ ./gradlew bootJar 1$ ./mvnw package 위 명령어를 실행하면 build/libs target 폴더에 demo-0.0.1-SNAPSHOT.jar 와 같은 이름으로 jar 파일이 생성됩니다. 이 파일은 아래의 명령어로 실행하면 됩니다. 1$ java -jar demo-0.0.1-SNAPSHOT.jar 4. Spring Boot devtoolsSpring Boot Devtools가 어떤 기능을 제공하는지에 대한 설명은 이 외부 블로그 글을 읽어 주길 바랍니다. 우리는 Terminal에서 어떻게 사용해야 하는지 알아보겠습니다. 4.1. 설치 및 확인 Gradle Maven build.gradle에 아래와 같이 Dependency를 추가합니다. build.gradle123dependencies { developmentOnly 'org.springframework.boot:spring-boot-devtools'} pom.xml에 아래와 같이 Dependency를 추가합니다. pom.xml12345678910&lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt; 추가한 후 실행해서 나오는 출력에서 아래와 같이 main이 restartedMain으로 바뀌어서 나오면 성공입니다. 12345# devtools 추가 하기 전2020-12-03 16:59:59.532 INFO 20300 --- [ main]# devtools 추가 한 후2020-12-03 16:59:59.532 INFO 20300 --- [ restartedMain] 4.2. Automatic Restartdevtools의 가장 큰 기능 중에 하나는 코드를 수정하면 자동으로 서버를 재시작 해주는 Automatic Restart 입니다. 하지만 devtools는 src 폴더의 java 파일을 watch 하는 것이 아니라 classpath 경로의 class 파일을 watch 합니다. 따라서 우리가 아무리 java 파일을 수정해봤자 devtools는 별다른 반응을 하지 않습니다. 이 때문에 우리는 아래와 같이 두 명령어를 동시에 실행시켜 놓아야 이 기능이 정상적으로 동작합니다. Gradle Maven 두 터미널에 각각 다음과 같은 명령어를 입력합니다. 12345# Terminal 1$ ./gradlew build -continuous# Terminal 2$ ./gradlew bootRun Maven에는 Gradle과 달리 watch를 하면서 빌드를 하는 기능이 없습니다. 따라서 외부 플러그인을 사용해야 합니다. 다음의 소스를 pom.xml에 추가합니다. 123456789101112131415161718192021222324&lt;build&gt; &lt;plugins&gt; ... &lt;plugin&gt; &lt;groupId&gt;com.fizzed&lt;/groupId&gt; &lt;artifactId&gt;fizzed-watcher-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.4&lt;/version&gt; &lt;configuration&gt; &lt;files&gt; &lt;param&gt;src/main/java&lt;/param&gt; &lt;param&gt;src/main/resources&lt;/param&gt; &lt;/files&gt; &lt;goals&gt; &lt;param&gt;clean&lt;/param&gt; &lt;param&gt;compile&lt;/param&gt; &lt;/goals&gt; &lt;profiles&gt; &lt;param&gt;optional-profile-to-activate-while-running-goals&lt;/param&gt; &lt;/profiles&gt; &lt;/configuration&gt; &lt;/plugin&gt; ... &lt;/plugins&gt;&lt;/build&gt; 그 후 두 터미널에 각각 다음과 같은 명령어를 입력합니다. 12345# Terminal 1$ ./mvnw fizzed-watcher:run# Terminal 2$ ./mvnw spring-boot:run 자세한 정보는 여기를 참고해주세요. 5. 테스트하기 Gradle Maven 아래의 명령어를 입력하면 @Test 어노테이션으로 등록된 테스트를 실행합니다. 1$ ./gradlew test 만약에 특정 클래스나 특정 메서드만 테스트하고 싶다면 아래와 같이 whildcard를 사용하면 됩니다. 1234567891011121314# MyTest 클래스의 모든 메서드를 테스트$ ./gradlew test --tests MyTest# MyTest 클래스의 testFunc 메서드만 테스트$ ./gradlew test --tests MyTest.testFunc# 이름이 Test로 끝나는 클래스의 모든 메서드를 테스트$ ./gradlew test --tests *Test# 이름이 Test로 끝나는 클래스의 test로 시작하는 메서드를 테스트$ ./gradlew test --tests *Test.test*# 패키지가 kim.hyunsub.demo.service인 클래스의 모든 메서드를 테스트$ ./gradlew test --tests kim.hyunsub.demo.service.* 1$ ./mvnw test 만약에 특정 클래스나 특정 메서드만 테스트하고 싶다면 아래와 같이 whildcard를 사용하면 됩니다. 1234567891011121314# MyTest 클래스의 모든 메서드를 테스트$ ./mvnw -Dtest=MyTest test# MyTest 클래스의 testFunc 메서드만 테스트$ ./mvnw -Dtest=MyTest#testFunc test# 이름이 Test로 끝나는 클래스의 모든 메서드를 테스트$ ./mvnw -Dtest=*Test test# 이름이 Test로 끝나는 클래스의 test로 시작하는 메서드를 테스트$ ./mvnw -Dtest=*Test#test* test# 패키지가 kim.hyunsub.demo.service인 클래스의 모든 메서드를 테스트$ ./mvnw -Dtest=kim/hyunsub/demo/service/* test","link":"/Spring/Spring-with-Terminal/"},{"title":"Awesome Web Site for Developer","text":"유용한 개발자용 웹 사이트 모음 crontab 시간, 날짜https://cron.help css로 삼각형http://apps.eky.hk/css-triangle-generator/ 정규표현식https://regex101.com/ - Java, Python, Go, JavaScript 지원 AWS 기반 아키텍처 시각화https://www.cloudcraft.co/ 국내 테크 기업 현황https://techcompanies.kr/ 완전 무료 이미지 모음https://unsplash.com/ 색 조합https://colorhunt.co/ https://color.adobe.com/ko/create/color-wheel/ https://coolors.co/palettes/trending 기타보안 취약점 검색: https://www.shodan.io/explore 발표 자료 만드는 사이트: https://slides.com/ 블로그에 넣을 코드 HTML 생성기: https://colorscripter.com/ 인터넷 과거 아카이브: https://archive.org/web/web.php","link":"/etc/Awesome-Developer-Site/"},{"title":"Awesome Github Repository","text":"쓸만한 Github Repository 모음 Web Development Tools fullPage.js - 한 화면에서 스크롤 바만 이용해서 화면 전환을 하게 해주는 도구 (데모) Tabler - Admin Page를 만들 수 있게 해주는 도구 (데모) Typed.js - 타이핑 하는 애니메이션을 만들 수 있게 하는 도구 (데모) Airframe React - Dashboard / Admin / Analytics 템플릿 (데모) WebTorrent - 웹 브라우저와 Node에서 사용할 수 있는 Torrent 라이브러리 Swiper - 스와이핑 라이브러리의 끝판왕 (데모) SweetAlert2 - 팝업 창 라이브러리 (데모) Gitfolio - Github 사용자를 위한 개인 홈페이지/블로그 생성기 (데모) DropzoneJS - 드래그 앤 드롭으로 파일 업로드하는 라이브러리 (데모) react-admin - 어드민 페이지 생성 도구 (데모) React-Select - Dropdown Select 라이브러리 (데모) selectize.js - Dropdown Select 라이브러리 (데모) Leaflet - OpenStreetMap 기반 지도 라이브러리 (데모) Sortable - 드래그 앤 드롭으로 순서를 바꿀 수 있는 리스트 라이브러리 (데모) Dplayer - Web Video Player (데모) Aplayer - Web Music Player (데모) Mapbox GL JS - OpenStreetMap 기반 지도 라이브러리 (데모) react-dates - 날짜 선택 라이브러리 (데모) CodeceptJS - End-to-End 프론트엔드 테스팅 라이브러리 Matter.js - 2D 물리엔진 (데모) Villain - 만화/소설 리더 라이브러리 vue-interactive-paycard - 신용카드 정보 입력 폼 라이브러리 (데모) react-interactive-paycard - 신용카드 정보 입력 폼 라이브러리 Quill - 텍스트 에디터 (데모) Slate - 텍스트 에디터 (데모) Editor.js - 텍스트 에디터 is-website-vulnerable - 웹사이트의 취약점을 찾아주는 라이브러리 Magnific Popup Repository - 이미지 확대 팝업 라이브러리 (데모) React Sortable HOC - React Sortable HOC (데모) GSAP (GreenSock Animation Platform) - 애니메이션 라이브러리 Lite YouTube Embed - 더 가벼운 Youtube Embed 태그 umi-request - axios와 같은 네트워크 요청 라이브러리 Text Mask - 입력 마스크 라이브러리 (데모) RxDB - Pub/Sub이 가능한 NoSQL-database react-use - 다양한 Custom React Hook 모음 Monaco Editor - 코드 에디터 (데모) JW Player - 동영상 플레이어 (데모) Cleave.js - 입력 마스크 라이브러리 (데모) Docusaurus - 문서 사이트 생성기 UI Components Elastic UI - Elastic Stack에서 사용하는 UI Components chakra Semantic UI Fomantic UI - Semantic UI의 커뮤니티 버전. 업데이트가 활발함 UIkit Awesome UIkit React Rainbow WixStyleReact - Wix 스타일의 React Component 모음 Grommet Reach UI Material Components - Material Design Web Components React Suite Rebass Boilerplates &amp; Samples React Boilerplate POS - Sample Application DDD Reactive Microservices with CQRS(Command Query Responsibility Segregation) &amp; Event Sourcing Suspense Demo for Library Authors - ‘Suspense for Data Fetching’과 ‘Concurrent UI Patterns’ 예제 Relay Examples - Relay 예제 Awesome Seires Awesome Selfhosted - 본인의 머신에서 구동할 수 있는 유용한 패키지들 모음 Awesome Mac - 맥에서 설치할 수 있는 프로그램 모음 Awesome React - React 도구/라이브러리/강의 등등 모음 Awesome Node.js - Node.js 패키지 및 리소스 모음 Awesome VSCode - VSCode 플로그인 모음 Awesome Design Tools - 디자인 도구 모음 Awesome Piracy - 저작권을 무시하는 링크/데이터/프로그램 모음 Public APIs - 무료 공공 API 모음 Study What the f*ck JavaScript? - Javascript에 재밌고 이해하기 힘든 예제 모음 주니어 개발자 채용정보 - 한국 주니어 개발자를 위한 취업 정보 모음 33 Javascript Concepts - Javascript 개발자가 알아야하는 Javascript 컨셉 모음 Front-End Checklist - 프론트엔드 개발할 때 체크해야 하는 사항 모음 AWS Serverless Workshop - AWS를 이용한 Serverless 아키텍처 예제 모음 Node.js Best Practices - Node.js 좋은 예제 모음 clean-code-javascript - 클린코드를 만들기 위한 예제 모음 (한국어판) React Developer Roadmap - React 개발자의 로드맵 Didact - Step-by-step 으로 나만의 리액트를 만들 수 있음 Front-End Developer Handbook 2019 (사이트) Study - 면접 대비 [영어] Tech Interview Handbook (사이트) [영어] 30 Seconds of Interviews (사이트) [영어] Front End Interview Handbook (한국어판) [한글] 취업 준비를 위해 공부한 내용을 정리하는 레포 [한글] Technical Interview Guidelines for Beginners [한글] 신입 개발자 전공 지식 &amp; 기술 면접 백과사전 [한글] 기술 냉장고 Visulization ApexCharts.js - Interactive SVG Charts (데모) nivo - (데모) Chart.js - canvas 태그를 이용한 시각화 도구 (데모) roughViz.js - 스케치 느낌이 나는 차트 라이브러리 react-vis - (데모) G2Plot - (데모) Cube.js - 차트 생성 플랫폼 (백엔드/프론트엔드) (데모) mermaid - 다이어그램/차트/그래프/흐름 생성기 (데모) plotly.js - (데모) vis-network - 네트워크 생성기 (데모) React Financial Charts - 금융 주식 차트 (데모) Programs &amp; Websites &amp; Self-hosted Hyper - Electron 베이스 터미널 프로그램 nuclear - 공공 무료 소스를 이용한 음악 플레이어 eDEX-UI - sci-fi 느낌이 나는 터미널 프로그램 Motrix - 다운로드 매니저 LosslessCut - 손실 없이 비디오/오디오 자르기 Mark Text - 마크다운 에디터 Boostnote - 마크다운 에디터 Another Redis DeskTop Manager - Redis 데스크톱 매니저 DrawIO - 온라인 그림 도구 (사이트) STREAMA - 설치형 미디어 스트리밍 서버 (데모) Bitwarden - 비밀정보 관리 도구 PreMID - 온라인 프로그램 스토어 (사이트) Terminus - 터미널 프로그램 YouTube-Music - Youtube Music Mac Desktop Application Util Libraries Day.js - Moment.js의 대체재로 2KB의 작은 시간/날짜 라이브러리 Yup - Javascript Object 스키마 검증 라이브러리 Nano ID - 작고, 안전하고, URL 친화적인 Unique ID 생성기 (데모) faker.js - 더미 데이터 생성기 (데모) Etc WebGL Fluid Simulation - WebGL을 이용하여 유체를 시뮬레이션한 예제 (데모) Github Trending Archive JSON server react-native-snap-carousel Emoji Screen - 이모지를 이용해서 표현한 영화, TV쇼, 뮤지컬 모음 (데모) Ghost - 블로깅 플랫폼 (Showcase) Meteor - Javascript 기반 서버/웹 개발 플랫폼 (설명) Svelte - 웹 개발 프레임워크 KeystoneJS - Node.js 기반 CMS Create Social Network - SNS 생성기 carbon - 코드 스크린샷 생성기 (데모) tsdx - Typescript 패키기 개발 도우미 strapi - 커스텀 API를 만들 수 있는 Node.js 기반 CMS (데모) VvvebJs - 드래그 앤 드롭 사이트 생성기 (데모) GrapesJS - 드래그 앤 드롭 사이트 생성기 (데모) TypeDoc - Typescript를 이용한 문서 생성기 (데모) Fantasy Map Generator - 판타지 지도 생성기 (데모) gif.js - gif 생성기 (데모) node-red - 이벤트 기반 어플리케이션을 위한 저레벨 프로그래밍 툴 (스크래치 같은거) Mostly adequate guide to FP (Book) - 함수형 프로그래밍 책 Fingerprint.js - 사용자 브라우저 정보를 이용하여 쿠키 없이 사용자 식별 (데모) Relay - React 애플리케이션을 위한 데이터 관리 프레임워크 React Query - 비동기 데이터를 얻고, 캐싱하고, 업데이트 하기 위한 React Hook Git History - 아무 Repository의 파일의 history를 빠르게 볼 수 있음 (데모) Firefox Send - 서버 설치형. 링크로 파일 공유할 수 있는 기능 (데모) fastify - express같은 web framework Zero - terminal output으로 3D 렌더링을 할 수 있는 라이브러리 Leaa - monorepo CMS built with Nest.js, Next.js, GraphQL, and Ant Design (어드민 페이지 데모)) (사용자 페이지 데모) Blocks - 드래그 앤 드롭 사이트 생성기 (데모) CSS pseudo element를 이용한 태양계 6 GitHub Repos For Instant Knowledge Boost","link":"/etc/Awesome-Github-Repository/"},{"title":"크롬에서 나만의 검색 엔진 사용하기","text":"크롬에서 나만의 검색 엔진 사용하기 개발자 여러분들은 구글링음 엄청 하고 계시리라 믿어 의심치 않습니다.저도 그렇거든요 ㅎㅎ 많은 사람들이 구글 홈페이지에 들어가서 검색 하는 것이 아니라 바로 크롬 주소창에 키워드를 입력해서 검색하시는데요 우리는 요걸 이용해서 더 편한 검색 라이프를 즐겨보도록 하겠습니다. 여기서는 예시를 유튜브로 하도록 하겠습니다. 크롬 설정에 들어갑니다. 기타 검색엔진에서 추가를 누릅니다. 위와 같이 입력하고 URL에 아래와 같이 입력합니다https://www.youtube.com/results?search_query=%s 키워드는 우리가 이 엔진을 쓴다고 크롬에게 알리는 문구 입니다.자세한건 밑의 gif 영상을 봐주세요 URL에서 %s는 우리가 검색할 키워드가 들어가는 부분입니다. 위의 두 옵션을 이용해서 다양한 검색엔진을 만드시면 됩니다. 검색창에 유튜브를 입력하고 탭을 누르면 유튜브 검색 모드로 넘어갑니다. 저는 이 기능을 이용하여 Github, 영어사전, 나무위키 등에 사용하고 있습니다.","link":"/etc/Chrome-Custom-Search-Engine/"},{"title":"CircleCI 사용해보기","text":"CircleCI를 이용하여 내 Node 프로젝트를 자동으로 테스트하고 Docker Image로 빌드해서 원격 서버에 배포해 보자! 1. 일단 해보기Github에 CircleCI-Example 이라는 레파지토리를 만들고 아래의 파일을 master 브랜치에 집어넣습니다. index.js 1console.log('hello, world!'); package.json 1234567{ &quot;name&quot;: &quot;CircleCI-Example&quot;, &quot;private&quot;: true, &quot;scripts&quot;: { &quot;test&quot;: &quot;node index.js&quot; }} CircleCI에 가서 회원가입을 한 후 CircleCI App으로 들어갑니다. 회원가입은 Github 아이디로 할 수 있습니다. CircleCI를 사용하기 원하는 프로젝트인 CircleCI-Example의 Setup Project 버튼을 누릅니다. 선택한 프로젝트가 Node 프로젝트이면 CircleCI는 이를 자동으로 인식하고 템플릿을 Node로 선택합니다. 템플릿 선택 버튼 밑에 있는 내용은 나중에 설명드릴테니 Start building 버튼을 누릅니다. 위와 같은 창이 뜰텐데 circleci-project-setup이라는 브랜치를 새로 만들어서 .circleci/config.yml 이라는 파일을 푸쉬하겠다는 의미입니다. Add Config 버튼을 눌러서 진행합니다. 1분 정도 기다리면 Success 라고 뜨면 우리의 첫 CircleCI 설정이 성공적으로 실행되었다는 의미입니다. 2. config.yml 설명위의 과정에서 생성된 config.yml 파일을 설명하도록 하겠습니다. 12345678910111213141516171819202122232425262728293031323334353637# CircleCI 2.1 버전을 사용한다는 의미입니다.# 각 버전에 따라 사용할 수 있는 속성이 다릅니다.version: 2.1# `node`라는 이름으로 `circleci/node@1.1.6` Orb를 사용한다는 의미입니다.orbs: node: circleci/node@1.1.6# `build-and-test`라는 이름을 가진 Job의 속성을 나열합니다.jobs: build-and-test: # 이 Job의 executor는 node Orb의 default executor 입니다. executor: name: node/default # 이 Job에서 수행할 명령어 모음입니다. steps: # 이 프로젝트 코드를 executor에 다운로드 합니다. - checkout # node Orb의 with-cache 명령어 입니다. # node_modules 폴더를 package.json을 키로 가지는 캐시에 생성합니다. - node/with-cache: # 캐싱되어 있던 node_modules 폴더를 불러 온 후 실행할 명령어를 입력합니다. # 이 steps가 끝난 후의 node_modules를 캐시에 저장합니다. steps: - run: npm install - run: npm test# build-and-test라는 workflow에는 build-and-test라는 Job을 가지고 있다는 의미입니다.workflows: build-and-test: jobs: - build-and-test 3. 용어 설명위의 config.yml 설명에서 익숙치 않은 용어들을 많이 접했을 텐데 그 용어를 밑에서 자세히 설명하겠습니다. 자세한 설명은 문서를 참고해 주길 바랍니다. Orbexecutor, 명령들이 재사용할 수 있게 구성되어 있는 패키지입니다. 본인이 자주 사용하는 명령이나 세팅들을 Orb로 만들어서 다른 프로젝트에 사용할 수 있습니다. CircleCI에서 제공하는 검증된 Orb도 있고 다른 사용자들이 올린 Orb도 사용할 수 있습니다. 자세한 정보는 Orb 사용법 문서와 Orb 리스트를 참고해 주길 바랍니다. JobStep들의 모음. Job의 모든 Step들은 하나의 머신 위에서 실행됩니다. 여기서 머신은 Docker container가 될 수도 있고 실제 물리적인 머신이 될 수도 있습니다. Steps 말고 Job에 올 수 있는 속성들은 여기에서 확인할 수 있습니다. StepJob을 수행하는 동안 실행할 수 있는 명령입니다. checkout이나 cache같이 CircleCI에 미리 준비되어 있는 명령도 있고 run과 같이 executor에 직접 입력할 수 있는 명령도 있습니다. Step에 올 수 있는 명령들은 여기에서 확인할 수 있습니다. 4. Docker Image 빌드1234567FROM node:12.16.1-alpineWORKDIR /appCOPY . /appENTRYPOINT node index.js 위의 Dockerfile을 프로젝트에 추가합니다. 123 orbs: node: circleci/node@1.1.6+ docker: circleci/docker@1.0.0 docker Orb를 불러옵니다. 123456789101112 jobs: build-and-test: ...+ docker:+ executor: docker/machine+ steps:+ - checkout+ - node/with-cache:+ steps:+ - run: npm install+ - run: docker build -t circleci/example .+ - run: docker save -o circleci-example.tar circleci/example 그런 다음 docker Job을 추가합니다. circleci/example 이라는 이름으로 docker image를 빌드하여 tar 파일로 압축한다는 의미입니다. node/with-cache 명령어를 이용하여 npm install로 다운로드 받는 node_modules 폴더를 캐싱합니다. 1234567 workflows: build-and-test: jobs: - build-and-test + - docker:+ requires:+ - test build-and-test workflow에 docker job을 추가합니다. 테스트가 끝난 후에 도커 이미지 빌드가 실행되도록 requires를 통하여 의존성을 추가합니다. 5. rsync로 원격 서버에 배포하기scp 말고 rsync를 사용하는 이유는 rsync가 더 빠르기 때문입니다. 보낼 파일들과 서버에 존재하는 파일을 비교하여 다른 파일들만 보내기 때문입니다. rsync를 이용하여 원격 서버에 파일을 보내기 위해서는 원격 서버에 접속할 수 있는 SSH 키를 등록해야 합니다. 5.1. SSH 키 생성일반적으로 서버에 접속할 때는 비밀번호를 씁니다. 하지만 SSH 키를 이용하여 비밀번호를 입력하지 않고도 접속할 수 있습니다. 1$ ssh-keygen -m PEM -t rsa -C &quot;server address&quot; 위의 명령어를 입력하고 나오는 모든 입력을 엔터로 넘어가면 ~/.ssh 폴더에 id_rsa 파일과 id_rsa.pub 파일이 생성됩니다. 이 두 파일들이 SSH Private Key와 SSH Public Key입니다. 5.2. CircleCI 프로젝트에 SSH 키 등록 Project Settings - SSH Keys에 들어가서 위에서 생성된 파일 중 id_rsa 파일의 내용을 복사하여 SSH 키를 추가합니다. 123456 - run: docker save -o circleci-example.tar circleci/example+ - add_ssh_keys: + fingerprints: + - &quot;ab:cd:ef:...&quot; + - run: rsync -ah -e &quot;ssh -o StrictHostKeyChecking=no&quot; circleci-example.tar username@server.address:/path/to/send + - run: ssh username@server.address &quot;cd /path/to/send; docker load -i circleci-example.tar; docker run circleci/example&quot; 추가한 뒤 나오는 fingerprint 값을 가지고 config.yml에 위과 같은 내용을 추가합니다. 현재 머신에 SSH 키를 추가한 후 rsync를 이용해 docker image 파일을 보낸 후 ssh 를 이용해 해당 docker image를 실행시키는 코드 입니다. 6. 특정 branch에서만 동작하도록 하기12345678910 workflows: build-and-test: jobs: - build-and-test - docker: requires: - test+ filters: + branches: + only: master job 밑에 filter 속성을 추가하여 master 브랜치에서만 동작하도록 설정할 수 있습니다.","link":"/etc/Circle-CI/"},{"title":"Github Actions 사용해보기","text":"Github Actions를 이용하여 개발 PC에서 배포 서버까지 빌드, 테스트, 배포를 자동화 해보자! 일반적으로 개발을 할 때 개발 PC가 있고 배포 타겟이 있습니다. Android, iOS는 Play Store, App Store에 배포합니다. Web Frontend, Backend는 Heroku, AWS, 또는 본인 소유의 서버에 배포합니다. Docker 이미지는 Docker hub, NPM Package는 NPM Repository에 배포합니다. 우리는 배포를 할 때 테스트 하고 빌드 하고 패키징 해서 배포 타겟에 넣어줍니다. 그런데 이 것도 하루 이틀이지 업데이트 할 때 마다 하면 매우 귀찮은 작업입니다. 그래서 Github Actions를 통해 이를 자동화 할 수 있는 법을 알아보겠습니다. 1. 첫 Workflow 생성하기Workflow는 프로젝트를 빌드, 테스트, 패키징, 릴리즈, 배포 등을 할 수 있게 자동화된 프로세스 입니다. 단순히 Github Repository에서 .github/workflows/main.yml 파일을 생성해서 아래의 내용을 넣어주시면 됩니다. 123456789101112name: Mainon: push: branches: - masterjobs: hello: runs-on: ubuntu-latest steps: - run: echo &quot;hello, world!&quot; 위의 내용을 잠깐 설명해 보겠습니다. 1번째 줄: Workflow의 이름입니다. 이 부분이 생략되면 파일 이름을 Workflow 이름으로 사용합니다. 3번째 줄: master 브랜치에 푸시가 들어왔을 때 이 Workflow를 실행시키겠다는 의미입니다. 9번째 줄: hello라는 job을 명시 합니다. 10번째 줄: 이 job은 최신 ubuntu 머신에서 실행된다는 것을 나타냅니다. 12번째 줄: 머신에서 실행할 명령어를 명시합니다. 위의 내용을 입력한 뒤 master 브랜치에 푸시 하신 후 위와 같이 Actions 탭에 들어갑니다. 그러면 위와 같이 방금 실행된 Workflow가 있을 텐데 클릭해서 들어갑니다. 위와 같이 hello, world를 띄운 것을 확인 할 수 있습니다! 2. 다양한 Action들빌드, 테스트, 배포를 자동화 하기 위해서는 코드도 다운로드 받아야 하고 환경도 세팅해줘야 하고 배포 대상에 로그인 해서 파일을 푸쉬까지 해야 하는데 run에서 명령어를 명시 하는 것 만으로는 많이 부족하고 너무 복잡해 집니다. 따라서 action이라는 것을 사용할 수 있는데 이는 다른 사람들이 만들어 놓은 명령어들의 모음입니다. 아래의 workflow 파일은 제가 사용하고 있는 workflow 입니다. 12345678910111213141516171819202122232425262728293031323334353637name: Mainon: push: branches: - masterjobs: check: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - uses: actions/setup-node@v1 - uses: actions/cache@v1 with: path: node_modules key: ${{ runner.OS }}-node-${{ hashFiles('**/package-lock.json') }} - run: npm install - run: npm run eslint - run: docker build -t hyunflix/frontend . - run: docker save -o hyunflix-frontend.tar hyunflix/frontend - uses: appleboy/scp-action@master with: host: ${{ secrets.HOST }} username: ${{ secrets.USERNAME }} password: ${{ secrets.PASSWORD }} port: ${{ secrets.PORT }} source: &quot;hyunflix-frontend.tar&quot; target: &quot;/home/hyunsub/environment/hyunflix&quot; - uses: appleboy/ssh-action@master with: host: ${{ secrets.HOST }} username: ${{ secrets.USERNAME }} password: ${{ secrets.PASSWORD }} port: ${{ secrets.PORT }} script: cd ~/environment/hyunflix; docker load -i hyunflix-frontend.tar; docker-compose up -d --force-recreate frontend; rm hyunflix-frontend.tar 13번째 줄: 이 Workflow가 있는 Repository를 Runner에 다운로드 합니다. 14번째 줄: 현재 Runner에 node 환경을 세팅합니다. 15번째 줄: node_modules를 캐싱해서 다음에 이 Workflow가 실행될 때 node_modules 폴더를 캐시에서 다운로드 받아 npm install 명령어가 더 빨리 실행될 수 있게 합니다. 16번째 줄: 캐싱의 키는 runner의 OS와 package-lock.json 파일의 해시 값입니다. 이 두 값이 같다면 node_modules 폴더의 내용도 같다고 보는 것입니다. 23번째 줄: source의 파일을 target에 scp로 보내는 action 입니다. 보안이 필요한 값들은 Repository 설정에서 Secret에 저장해 두었습니다. 31번째 줄: 원격 서버에 ssh로 접속해서 script의 내용을 실행하는 action입니다.","link":"/etc/Github-Actions/"},{"title":"슬랙과 깃허브 연동하기","text":"슬랙과 깃허브 연동하기 1. Slack에 Github 앱 추가하기먼저 좌상단의 워크스페이스 이름을 클릭한 후 Administration - Manage apps를 클릭합니다. Browse the App Directory를 누릅니다. 만약 이전에 설치해 놓은 앱이 있으면 아래의 사진과 같이 뜨는데 그러면 App Directory를 누릅니다. github를 검색한 후 맨 처음에 나온 Github를 클릭합니다. Legacy가 적힌 것은 Enterprise 계정 전용이라고 합니다. Install을 누릅니다. Continue를 누릅니다. 이 Github 앱을 사용할 채널을 선택하는데 저와같이 원하는 채널만 선택하셔도 되고, 모든 채널을 선택하셔도 됩니다. 나중에 채널을 추가 할 수 있습니다. 여기서 선택 되지 않은 채널에서는 Github와의 연동을 할 수 없습니다. 설치를 하게 되면 위와 같이 좌하단의 Apps에 Github가 추가된 것을 알 수 있습니다. 2. Github Repository와 연동하기 /github subscribe [연동하고 싶은 Repository URL]원하는 채널에 가서 위와 같은 명령어를 입력합니다. 그러면 위와 같이 Github account를 연결해달라는 요청이 뜹니다. `Authorize Slack By Github`를 클릭하면 됩니다. 연결을 원하는 Repository의 계정을 클릭합니다. 원하는 Repository를 선택합니다. 그려면 위와 같이 Repository와의 연결이 성공했다는 메시지가 뜹니다. 이 Repository에 누군가 Commit을 하고 Push를 한다면 위와 같은 커멧 메시지가 뜹니다. 3. Slack에서 Github Issue 만들기 /github open [Issue를 만들고 싶은 Repository URL]Issue를 만들고 싶은 Repository를 위와 같이 입력합니다. 그러면 위와 같이 Issue를 만들 수 있는 창이 뜹니다. 원하는 정보를 입력한 후 `Open`을 누릅니다. 그러면 위와 같이 Issue가 만들어졌다고 뜹니다. 실제 Github에 가서도 정상적으로 Issue가 생성된 것을 알 수 있습니다. /github help 명령어를 통해 어떤 명령어가 있는지 알 수 있습니다.","link":"/etc/Slack-Github-Integration/"},{"title":"Today I Learned (TIL)","text":"개인적인 TIL을 모아두는 글입니다. 2022-03-10vim syntax highlighting 켜기 echo “syntax on” &gt;&gt; ~/.vimrc 2021-08-18query parameter가 있으면 proxy_pass로 보내고 없으면 바로 파일을 서빙 12345678910111213location ~* \\.(jpe?g|png)$ { auth_request /auth; proxy_set_header 'X-Forwarded-For' $remote_addr; proxy_set_header 'Host' $http_host; if ($args) { proxy_pass http://localhost:9001; } if ($args = '') { root /archive; add_header Access-Control-Allow-Origin 'https://video.hyunsub.kim'; add_header Access-Control-Allow-Credentials 'true'; }} 2021-08-17 모바일 사파리에서는 전화번호같이 생긴 문자열에 대해서는 자동으로 링크를 생성하는데 아래의 meta tag로 이를 방지할 수 있다.&lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;/&gt; 사용자가 링크를 눌렀을 때 해당 링크로 넘어가기 전에 무언가의 작업을 하고 싶다면 window.addEventListener('pagehide', () =&gt; {}) 를 추가하면 된다. 2021-08-13https://apps.timwhitlock.info/unicode/inspect?s=%E6%A8%82+%EF%A5%9C 똑같이 생긴 한자인데 유니코드가 다르다 2021-07-30 scp로 큰 파일을 보내려고 할 때 client_loop: send disconnect: Broken pipe 라는 에러가 발생하는 경우가 있는데 이는 파일을 받는 서버가 throughput을 감당하지 못 해서 그렇다. -l 2000 옵션을 붙여주면 해결된다. 여기서 2000은 2000kbps 속도로 보낸다는 의미이다.ref: https://stackoverflow.com/questions/30020519/broken-pipe-error-on-scp ubuntu에서 adoptopenjdk를 설치하고 jenv 에 추가하는 법sudo apt install adoptopenjdk-11-hotspotjenv add /usr/lib/jvm/adoptopenjdk-11-hotspot-amd64ref: https://blog.benelog.net/installing-jdk.html#jenv 2021-07-18@Aspect 어노테이션이 붙은 클래스는 상속할 수 없다. AopConfigException: cannot extend concrete aspect 에러가 발생한다. 2021-07-15ClassPathResource.getFile은 IDE에서 실행하면 잘 되지만 jar 파일로 실행하면 잘 되지 않는다. 파일시스템에서 해당 파일을 찾기 때문이다. 대신 ClassPathResource.getInputStream 을 써야한다. 2021-06-21linux에서 특정 명령어만 비밀번호 입력 없이 sudo 권한을 사용할 수 있게 하는 방법 sudo visudo 명령어를 입력하면 nano 편집기가 열린다. %sudo로 시작하는 라인 아래에 다음과 같이 적는다 [username] ALL=NOPASSWD: [command]예) hyunsub ALL=NOPASSWD: /usr/sbin/nginx -s reload 여기서 command는 절대경로로 적어야 한다.여기서 [username]과 ALL 사이는 탭 문자를 써야 한다. 저장하고 나가면 적용되어 있다. 2021-06-05ssh 명령어로 remote server에 background process를 실행시키려고 하는 경우 &gt; nohup.out &lt; /dev/null을 붙여주어야 한다. 그렇지 않으면 ssh 세션이 끊어지지 않고 계속 유지된다. 또는 ssh 명령어에 -f 옵션을 붙여주면 된다. 2021-05-23nginx binary를 그냥 실행히시키면 no daemon 모드로 실행이 되는데 이를 daemon 모드로 실행시키려고 하면nginx -c nginx.conf -g 'daemon off;' 2021-05-11Java에서 삼항연산자 사용시, 두번째, 세번째 피연산자 중 한 쪽은 Wrapper Class이고 다른 한 쪽은 원시 자료형일 경우, 무조건 Wrapper Class를 원시 자료형으로 변환한다. 예를 들어 아래의 코드는 무조건 NullPointerException이 발생한다. 123Integer A = null;Integer B = null;Integer chosenInteger = A != null ? A.intValue() : B; 여기서 두번째 피연산자의 타입은 int(원시자료형)이고, 세번째 피연산자의 타입은 Integer(Wrapper Class)이다. 따라서 B를 원시자료형으로 변환하려고 하고 이 때 NullPointerException이 발생하는 것이다. Ref: https://stackoverflow.com/a/15082983 2021-05-04nginx 에서는 dollar sign 을 escape 할 수 있는 방법이 없다. 따라서 아래와 같이 써야 한다. 12345678// http block 에서geo $dollar { default &quot;$&quot;;}location / { return 200 '$dollar.status_code';} 2021-04 13safari 에서는 이미지의 높이를 auto 로 하려면 align-self: flex-start 를 해야 한다. 2021-03-19nginx에서 header에 underscore가 있으면 underscores_in_headers on; 해줘야한다. 2021-02-14nginx proxy_pass로 socket.io를 동작하고 싶을 때 1234567location /socket.io { proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_buffering off; proxy_pass http://localhost:8003;}","link":"/etc/TIL/"},{"title":"Noverish의 기술 블로그 시작합니다!","text":"Noverish의 기술 블로그 시작합니다!","link":"/etc/init/"},{"title":"낮은 Swift 버전을 쓰는 라이브러리 CocoaPod에서 사용하기","text":"내 프로젝트의 swift 버전 보다 낮은 버전으로 작성 되어 있는 외부 라이브러리를 사용하고 싶을 때 어떻게 해야 하는 지 알아보자 123456789101112131415pod 'LibraryName1'pod 'LibraryName2'post_install do |installer| # Your list of targets here. myTargets = ['LibraryName1', 'LibraryName2'] installer.pods_project.targets.each do |target| if myTargets.include? target.name target.build_configurations.each do |config| config.build_settings['SWIFT_VERSION'] = '3.2' end end endend LibraryName1, LibraryName2를 자신이 원하는 라이브러리 이름으로 바꾸면 된다.","link":"/iOS/Cocoa-Pod-Swift-Version/"},{"title":"iOS 오토 레이아웃 파헤치기","text":"iOS 오토 레이아웃 파헤치기 1. Editor를 사용하면서 개꿀팁Zeplin 같이 View와 View 사이의 상대 길이를 알아내는 법 한 View를 선택하고 alt 키를 누른 상태로 커서를 다른 View 위로 올리면 위와 같이 그 View 와의 거리가 나타난다. 디버깅 중에 View Hierarchy 및 Size, Property가 보고 싶을 때 View UI Hierarchy 를 클릭하면 위와 같이 View Hierarchy를 확인 할 수 있고 각각의 View 들의 Property, Size, Position 등을 확인할 수 있다. 2. 코드로 Layout Constraint 생성 하기NSLayoutConstraint을 직접 생성하기1234567NSLayoutConstraint(item: view1, attribute: .leading, relatedBy: .equal, toItem: view2, attribute: .leading, multiplier: 1.0, constant: 0.0).isActive = true Anchor를 이용하여 생성하기1234567891011121314151617181920212223242526272829// NSLayoutYAxisAnchor : topAnchor, bottomAnchor, firstBaselineAnchor, lastBaselineAnchor, centerYAnchor// NSLayoutXAxisAnchor : leadingAnchor, trailingAnchor, leftAnchor, rightAnchor, centerXAnchor// NSLayoutDimension : heightAnchor, widthAnchor// NSLayoutYAxisAnchor, NSLayoutXAxisAnchorview1.leadingAnchor.constraint(equalTo: view2.leadingAnchor)view1.leadingAnchor.constraint(greaterThanOrEqualTo: view2.leadingAnchor)view1.leadingAnchor.constraint(lessThanOrEqualTo: view2.leadingAnchor)view1.leadingAnchor.constraint(equalTo: view2.leadingAnchor, constant: 20)view1.leadingAnchor.constraint(greaterThanOrEqualTo: view2.leadingAnchor, constant: 20)view1.leadingAnchor.constraint(lessThanOrEqualTo: view2.leadingAnchor, constant: 20)// NSLayoutDimensionview1.heightAnchor.constraint(equalToConstant: 30)view1.heightAnchor.constraint(greaterThanOrEqualToConstant: 30)view1.heightAnchor.constraint(lessThanOrEqualToConstant: 30)view1.heightAnchor.constraint(equalTo: view2.heightAnchor, multiplier: 2.0)view1.heightAnchor.constraint(greaterThanOrEqualTo: view2.heightAnchor, multiplier: 2.0)view1.heightAnchor.constraint(lessThanOrEqualTo: view2.heightAnchor, multiplier: 2.0)view1.heightAnchor.constraint(equalTo: view2.heightAnchor, constant: 20)view1.heightAnchor.constraint(greaterThanOrEqualTo: view2.heightAnchor, constant: 20)view1.heightAnchor.constraint(lessThanOrEqualTo: view2.heightAnchor, constant: 20)view1.heightAnchor.constraint(equalTo: view2.heightAnchor, multiplier: 2.0, constant: 20)view1.heightAnchor.constraint(greaterThanOrEqualTo: view2.heightAnchor, multiplier: 2.0, constant: 20)view1.heightAnchor.constraint(lessThanOrEqualTo: view2.heightAnchor, multiplier: 2.0, constant: 20) 2.3. Visual Format을 이용하여 생성하기1234567891011121314151617181920212223let redView = UIView(frame: view.bounds)let blueView = UIView(frame: view.bounds)let greenView = UIView(frame: view.bounds)let views = [&quot;redView&quot;: redView, &quot;blueView&quot;: blueView, &quot;greenView&quot;: greenView]let format1 = &quot;V:|-[redView]-8-[greenView]-|&quot;let format2 = &quot;H:|-[redView]-8-[blueView(==redView)]-|&quot;let format3 = &quot;H:|-[greenView]-|&quot;var constraints = NSLayoutConstraint.constraints(withVisualFormat: format1, metrics: nil, views: views)constraints += NSLayoutConstraint.constraints(withVisualFormat: format2, metrics: nil, views: views)constraints += NSLayoutConstraint.constraints(withVisualFormat: format3, metrics: nil, views: views)NSLayoutConstraint.activate(constraints) 3. Stack View 위와 같이 라벨들의 최대 길이 만큼 UIImageView가 밀리는 UI를 구현한다고 가정하자. 어떤 방식으로 구현해야 할까? 다양한 방법이 있겠지만 UIStackView를 사용하면 간단하게 구현할 수 있다. UIImageView의 위치를 UIStackView의 Trailing에 연결하면 된다. UIStackView의 Width는 별도의 Constraint없이 안의 View들의 크기에 따라 자동으로 결정되기 때문이다. Stack View 생성/해제 하기 위와 같이 여러 개의 View를 선택한 다음 아래의 저 버튼을 누르면 위와 같이 선택 했던 View를이 연결되면서 StackView로 감싸집니다. View hierarchy는 위와 같습니다. StackView를 해제하려면 alt 키를 누른 상태로 Stack 버튼을 누른 뒤, Unembed 버튼을 누르면 됩니다 Stack View의 ConstraintStack View도 다른 View들과 마찬가지로 X Position, Y Position, Width, Height가 Constraint로 지정이 되어야 한다. UILabel들로만 이루어진 UIStackView는 각각의 UILabel의 사이즈를 추정할 수 있기 때문에 X, Y Position Constraint만 있어도 되지만 UIView로 이루어진 UIStackView와 같은 경우에는 각각의 사이즈를 추정할 수 없기 때문에 위와 같이 UIStackView안의 각각의 View마다 Width와 Height Constraint를 넣어주어 StackView가 자신의 크기를 계산할 수 있게 해야 한다. StackView가 자신의 크기를 계산할 수 있다면 굳이 모든 View에 Width와 Height Constraint를 넣지 않아도 된다. 위와 같은 경우에는 첫 번째, 두 번째 View는 Width Constraint만 마지막 View는 Width, Height Constraint를 넣어주었다. Editor에서나 실제 앱을 구동할 때나 잘 동작한다. 만일 위처럼 각각의 View에 Width, Height Constraint를 넣지 않고 그냥 StackView에 Width, Height Constraint를 넣어주면 StackView에는 모호성이 없다고 나오지만 각각의 View들을 클릭하면 X Position의 모호함이 빨간색으로 나타나고 있고 실제로 앱을 구동하면 위와 같이 이상하게 나온다. 주어진 정보 만으로는 StackView의 위치와 크기는 정할 수 있어도 안의 각각의 View들의 크기와 위치를 계산할 수 없기 때문이다. Stack View Distribution 위와 같이 X, Y 위치 Constraint만 있는 UIStackView에 현재 길이보다 더 긴 Width Constraint를 넣어주면 어떻게 될까? 이를 위한 옵션이 5가지가 있다. Fill 위와 같이 늘어날 수 있는 View를 늘린다. Fill Equally 원래 크기와 상관 없이 무조건 모든 뷰에 같은 크기를 분배한다. Fill Proportionally 원래 크기에 비례하여 남는 공간을 분배한다. 예를 들어 원래 크기가 1:2:3 이고 남는 공간이 60일 경우 각각의 View에게 10, 20, 30의 공간을 부여하는 것이다. Equal Spacing 남는 공간을 균일하게 나눠서 View 사이에 Space를 둔다. Equal Centering 각각의 뷰들의 Center 사이의 길이를 모두 같게 한다. 4. Intrinsic Content Size 모든 뷰들은 고유의 Intrinsic Content Size 가 있습니다. UILabel과 같이 해당 View의 속성만을 가지고 크기를 예측할 수 있는 경우 위와 같이 그냥 UIView는 width와 height가 결정되지 않아 빨간색으로 오류가 뜨지만 내용과 폰트 크기 만으로 Size를 예측할 수 있는 UILabel의 경우 Intrinsic Content Size가 결정되어서 오토레이아웃의 에러가 뜨지 않습니다. 12345class CustomView: UIView { override var intrinsicContentSize: CGSize { return CGSize(width: 100, height: 100) }} Intrinsic Content Size는 UIView.intrinsicContentSize에 접근하여 알아낼 수 있습니다. 또한 위와 같이 CustomView를 만들 때 intrinsicContentSize를 override 할 수도 있습니다. 만약에 intrinsicContentSize가 변했을 경우 invalidateIntrinsicContentSize()를 호출해줘야 합니다. 5. Content Hugging Priority Size Insepector의 Content Hugging Priority 입니다. 더 높은 Priority을 준다는 의미는 이 View가 intrinsic size 보다 커지지 않는다는 것을 의미합니다. labe1의 Content Hugging Priority는 251labe2의 Content Hugging Priority는 251일 때 입니다. 빨간색이 되어 있는 것을 볼 수 있습니다. labe1의 Content Hugging Priority는 252로 올린 결과 입니다. 6. Content Compression Resistance Priority Size Insepector의 Content Hugging Priority 입니다. 더 높은 Priority을 준다는 의미는 이 View가 intrinsic size 보다 작아지 않는다는 것을 의미합니다. UILabel에 horizontal 위치, vertical 위치, width 이 3개의 constraint를 주고 내용을 width보다 길게 했을 경우에는 당연히 내용이 잘려서 끝에 …이 붙습니다. 하지만 width의 priority를 1로 만들어 horizontal Content Compression Resistance Priority 보다 작게 만들어줬을 경우에는 내용이 모두 보이는 것을 알 수 있습니다. 참고https://www.raywenderlich.com/174078/auto-layout-visual-format-language-tutorial-2https://www.letmecompile.com/advanced-auto-layout/http://rhammer.tistory.com/210","link":"/iOS/Auto-Layout/"},{"title":"Facebook으로 로그인하기","text":"Facebook으로 로그인하기 1. 앱 기본 설정 페이스북 개발자 사이트에 접속해서 새 앱을 추가합니다. 자신의 앱 이름을 적고 앱 ID를 만듭니다. Facebook으로 로그인을 누릅니다. `설정`을 누릅니다. iOS를 누릅니다. iOS SDK 다운로드를 누릅니다. 다운로드 받은 뒤 압축을 풀면 위와 같은 파일이 들어있습니다. Bolts.framework, FBSDKCoreKit.framework, FBSDKLoginKit.framework 을 드래그 앤 드롭해서 Frameworks 폴더에 넣어줍니다. 만약에 이 폴더가 없으면 만들어줍니다. 이 창이 뜰텐데 꼭 Copy items if needed 를 체크해줍니다. 자기 앱의 번들ID를 적고 Save를 누른 뒤 넘어갑니다. 저 버튼을 클릭해서 활성화 해주고 Save를 누른 뒤 넘어갑니다. 라고 합니다. 밑에 사진으로 쉽게 설명 드리겠습니다. 이렇게 해서 `Info.plist` 를 코드로 엽니다. `` 태그 안에 제일 밑에 사진 같이 붙여 넣습니다. objective-c 코드로 되어 있네요. swift 코드는 밑에 써두겠습니다. AppDelegate.swift 에 아래 코드를 써주시면 됩니다. 123456789101112131415161718192021222324// AppDelegate.swiftimport FBSDKCoreKitfunc application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplicationLaunchOptionsKey : Any]? = nil) -&gt; Bool { FBSDKApplicationDelegate.sharedInstance().application(application, didFinishLaunchingWithOptions: launchOptions) // Add any custom logic here. return true}func application(_ application: UIApplication, open url: URL, options: [UIApplicationOpenURLOptionsKey: Any] = [:]) -&gt; Bool { let handled: Bool = FBSDKApplicationDelegate .sharedInstance() .application(application, open: url, sourceApplication: options[.sourceApplication] as? String, annotation: options[.annotation]) // Add any custom logic here. return handled} 위의 두 함수중 밑의 함수는 iOS 10 이상에서만 사용 할 수 있습니다. 따라서 그 밑 버전에서 사용하려면 아래의 코드를 사용해야 합니다. 1234567891011121314func application(_ application: UIApplication, open url: URL, sourceApplication: String?, annotation: Any) -&gt; Bool { let handled: Bool = FBSDKApplicationDelegate .sharedInstance() .application(application, open: url, sourceApplication: sourceApplication, annotation: annotation) // Add any custom logic here. return handled} 여기까지 앱의 기본 설정을 마쳤습니다.위의 7, 8, 9, 10번 단계는 밑에서 설명 하므로 넘어가셔도 됩니다. 2. Facebook 권한 요청Facebook 아주 기본적인 로그인(userID)을 제외하고 다른 기능을 사용 하려면 로그인 할 때 따로 권한 요청을 해야 합니다. 사용자의 프로필(프로필 사진 등)과 이메일을 얻으려고 하면 따로 권한 요청을 해야 합니다. 사용 가능한 권한은 다음과 같습니다. 로그인 할 때 권한 요청 하는 방법은 밑에서 다루도록 하겠습니다.자세한 사항은 여기 를 참고해 주세요. 3. Facebook SDK 버튼으로 로그인 하기Facebook SDK에서 제공하는 기본 버튼으로 로그인 하는 방법을 살펴보겠습니다.커스텀 버튼으로 로그인 하길 원하시는 분은 건너 뛰셔도 됩니다. 1234567891011121314import FBSDKCoreKitimport FBSDKLoginKitclass UIViewController { func viewDidLoad() { super.viewDidLoad() let loginButton = FBSDKLoginButton() loginButton.center = view.center loginButton.delegate = self loginButton.readPermissions = [&quot;public_profile&quot;] view.addSubview(loginButton) }} 위와 같이 코드로 버튼을 만들어 주셔도 되고 storyboard로 하셔도 됩니다.권한 요청 리스트는 FBSDKLoginButton의 readPermissions에 String Array로 넣어 주시면 됩니다.FBSDKLoginButton의 delegate는 FBSDKLoginButtonDelegate입니다. 자세한 코드는 밑에 있습니다. 12345678910111213141516171819202122extension UIViewController: FBSDKLoginButtonDelegate { func loginButton(_ loginButton: FBSDKLoginButton!, didCompleteWith result: FBSDKLoginManagerLoginResult!, error: Error?) { if let error = error { print(&quot;error : \\(error)&quot;) return } if result.isCancelled { print(&quot;cancelled&quot;) } else { print(&quot;token \\(result.token.userID!)&quot;) } } func loginButtonDidLogOut(_ sdkButton: FBSDKLoginButton!) { print(&quot;loginButtonDidLogOut&quot;) } func loginButtonWillLogin(_ sdkButton: FBSDKLoginButton!) { print(&quot;loginButtonWillLogin&quot;) }} loginButton 함수는 로그인 프로세스가 끝나면 호출 되는 함수입니다.loginButtonDidLogOut 함수는 로그아웃이 되면 호출 되는 함수입니다.loginButtonWillLogin 함수는 로그인 버튼이 클릭될 때 호출 되는 함수입니다. 자세한 정보는 FBSDKLoginButtonDelegate 와 FBSDKLoginManagerLoginResult 를 확인해 주세요 앱을 실행 하면 위와 같은 버튼이 뜹니다. 이 버튼을 클릭하면 `loginButtonWillLogin` 함수가 호출 됩니다. 버튼을 클릭하면 이런 AlertDialog가 뜹니다. 그리고 로그인 화면으로 연결 됩니다. 로그인하면 이 화면이 뜹니다. 그리고 나면 다시 앱으로 돌아와서 로그인 버튼이 로그아웃 버튼으로 바뀌어 있습니다. 이 버튼을 클릭하면 로그아웃이 되고 `loginButtonDidLogOut` 함수가 호출 됩니다. 4. Custom 버튼으로 로그인 하기12345678910111213let fbLoginManager = FBSDKLoginManager()fbLoginManager.logIn(withReadPermissions: [&quot;public_profile&quot;], from: self) { (result, error) -&gt; Void in if let error = error { print(&quot;error : \\(error)&quot;) return } if result.isCancelled { print(&quot;cancelled&quot;) } else { print(&quot;token \\(result.token.userID!)&quot;) }} 커스텀 버튼을 만들고 클릭 되면 위의 코드를 실행시키면 됩니다.권한 요청 리스트는 withReadPermissions에 String Array로 넣어 주시면 됩니다.로그아웃은 단순하게 FBSDKLoginManager의 logOut() 함수를 실행 시키면 됩니다. 1typealias FBSDKLoginManagerRequestTokenHandler = (_ result: FBSDKLoginManagerLoginResult, _ error: Error?) -&gt; Void callback함수인 FBSDKLoginManagerRequestTokenHandler은 위와 같이 정의되어 있습니다. 자세한 정보는 FBSDKLoginManager 와 FBSDKLoginManagerLoginResult 를 확인해 주세요. 5. 로그인 여부 확인하기12345678import FBSDKLoginKitif let token = FBSDKAccessToken.current() { print(&quot;token \\(token.userID!)&quot;) // 로그인 되어있음} else { // 로그인 안 되어있음} 위 코드를 사용하면 로그인 여부를 확인 할 수 있습니다.","link":"/iOS/Facebook-Login/"},{"title":"커스텀 뷰 만드는 법","text":"iOS에서 커스텀 뷰를 만드는 방법을 알아보자 먼저 Cocoa Touch Class 새로운 파일을 하나 만듭니다. 원하는 커스텀 뷰의 이름을 넣습니다. 여기서는 TestView라고 했습니다. 그 다음 새로운 xib 파일을 하나 만듭니다. 이 xib 파일의 이름도 아까와 같은 이름으로 해줍시다. 반드시 그래야 되는 건 아닌데 안 헷갈리기 위해 같게 해줍시다. File’s Owner를 클릭한 다음에 아까 만들었던 Cocoa Touch Class 파일의 이름을 적습니다 그런데 지금은 root 뷰의 크기를 바꿀 수가 없습니다. root 뷰의 Property의 Size를 Freedom으로 바꿔줍시다. 그러면 root뷰의 크기를 바꿀 수 있습니다. 그 다음 자유롭게 root 뷰의 내용을 채워줍시다. 1234567891011121314151617181920212223import UIKitclass TestView: UIView { //코드에서 뷰를 생성할 때 호출 됨 override init(frame: CGRect) { super.init(frame: frame) setup() } //storyboard에 뷰를 사용할 때 호출 됨 required init?(coder aDecoder: NSCoder) { super.init(coder: aDecoder) setup() } func setup() { let view = Bundle.main.loadNibNamed(&quot;TestView&quot;, owner: self, options: nil)?.first as! UIView view.frame = bounds addSubview(view) //여기에 필요한 작업을 하세요 }} 아까만든 swift 파일을 다음과 같이 채워줍시다. instanceFromNib 메서드의 nibName는 우리가 만든 xib파일의 이름과 같게 해줍시다. 이러면 커스텀 뷰 만들기 끝입니다. storyboard에서 우리가 만든 뷰를 사용 하려면 위와 같이 해줍시다. 확인 TableViewCell에서 했듯이 이렇게 화면 2개로 해서 코드에서 UI들을 다룰 수 있습니다.","link":"/iOS/How-To-Create-Custom-View/"},{"title":"Cell의 높이가 변하는 테이블 뷰 만들기","text":"Cell의 내용에 따라 높이가 변하는 TableView를 만드는 방법을 알아보자 이렇게 각 셀의 높이가 서로 다른 테이블 뷰를 만드려고 합니다. item 파일 만들기각각의 셀의 정보를 담고 있는 item파일을 만듭시다 Swift파일을 하나 만듭니다 2. TestItem 이라고 이름을 붙였습니다 123456789class TestItem { var title:String var content:String init(title:String, content:String) { self.title = title self.content = content }} Cell.swift 파일 만들기각각의 셀의 swift파일 만듭시다 Cocoa Touch Class를 하나 만든 다음에 2. Subclass of는 UICollectionViewCell을 선택하고 TestCell을 사용했습니다. 1234567891011121314151617181920212223class TestCell: UICollectionViewCell { static var measureView = Bundle.main.loadNibNamed(String(describing: TestCell.self), owner: nil, options: nil)?.first as! TestCell @IBOutlet weak var titleLabel: UILabel! @IBOutlet weak var contentLabel: UILabel! @IBOutlet weak var rootWidth: NSLayoutConstraint! var item: TestItem! func setItem(_ item: TestItem) { self.item = item titleLabel.text = item.title contentLabel.text = item.content } static func getDynamicHeight(of width: CGFloat, item:TestItem) -&gt; CGFloat { measureView.setItem(item) measureView.rootWidth.constant = width let newSize = measureView.systemLayoutSizeFitting(UILayoutFittingCompressedSize) return newSize.height }} measureView는 각의 cell의 높이를 구할 때 사용하는 테스트용 Cell입니다. 이를 이용해서 getDynamicHeight 함수에서 width와 item이 주어지면 이를 measureView에 넣어서 나오는 height를 반환해 줍니다. Cell.xib 파일 만들기각각의 셀의 xib파일 만듭시다 Empty xib파일을 하나 만듭니다 2. TestCell이라고 swift파일의 이름과 똑같이 붙였습니다 3. CollectionViewCell을 넣습니다 4. 그 안에 View를 넣습니다 5. 이 View의 constraint를 위와 같이 합니다 6. label을 2개 넣습니다 7. 두 label의 폰트와 폰트 색과 폰트 크기와 배경등을 적당히 하고 Lines를 0으로 해야 합니다! 8. Title label의 constraint를 위와 같이 합니다 9. Content label의 constraint를 위와 같이 합니다 10. 그러면 위와 같이 빨간색으로 conflict가 생겼다고 할 겁니다 11. CollectionViewCell을 선택해서 적당히 conflict가 없어지는 Height로 설정합니다. 12. 가장 root인 View의 width constraint를 넣습니다. 13. CollectionViewCell을 선택하고 Class를 TestCell로 합니다. 14. titleLabel과 contentLabel과 width를 outlet과 연결합니다 TestTableView 만들기이제 이 셀들을 보여주는 TableView 가 필요합니다. 우리는 PagingTableView를 사용 할 겁니다. 이건 제가 만든 건데 어떻게 구현 되어 있는지는 이해 안 하셔도 됩니다. 2. Swift파일을 만듭니다 3. TestTableView라고 이름을 붙였습니다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import UIKitimport Foundationclass TestTableView: PagingTableView, PagingTableViewDelegate, PagingTableViewDataSource { func initialize(nowVC: UIViewController) { super.columnNum = 1 //테이블의 열의 개수를 나타냄 super.sectionInset = CGFloat(8) //Cell과 바깥과의 여백 super.itemSpacing = CGFloat(8) //각 Cell사이의 간격 super.delegate = self super.initialize(nowVC: nowVC, dataSource: self) } func setItem(cell: UICollectionViewCell, item: Any) -&gt; UICollectionViewCell { if let cell = cell as? TestCell { if let item = item as? TestItem { cell.setItem(item) } } return cell } func loadMoreItems(page: Int, callback: @escaping ([Any]) -&gt; Void) { var items:[TestItem] = [] items.append(TestItem(title:&quot;title&quot;, content:&quot;content&quot;)) items.append(TestItem(title:&quot;랜섬웨어 7천만원 행방은? 배후 꼬리잡힐 가능성 있나&quot;, content:&quot;물밑 사이버 추격전 예고…\\&quot;현실화폐 교환 땐 추적될 수도\\&quot;비트코인 간판[EPA=연합뉴스](서울=연합뉴스) 김수진 기자&quot;)) items.append(TestItem(title:&quot;517건→2863건→1256건…기세 꺾인 랜섬웨어&quot;, content:&quot;관련 문의 15일 정점 찍고 하락세\\&quot;2차 공격 가능성 있어 안심은 일러\\&quot;[아시아경제 김동표 기자]랜섬웨어 워너크라이(WannaCry)의 기세가 대폭 꺾였다&quot;)) items.append(TestItem(title:&quot;오픈소스 저작권 위반혐의 한컴, 美 법정행… \\&quot;문제 간단치 않다\\&quot; 우려&quot;, content:&quot;한글과컴퓨터(www.hancom.com 대표 김상철·이원필, 이하 한컴)가 오픈소스 라이선스 위반으로 국제소송에 휘말려 미국 법정에 서게 될 위기에 빠졌다.미국 온라인매체 쿼츠는 지난&quot;)) items.append(TestItem(title:&quot;title title title title&quot;, content:&quot;content content content content&quot;)) callback(items) } func getNibName() -&gt; String { return String(describing: TestCell.self) } func didSelected(item: Any) { print(&quot;selected! \\(item)&quot;) } func calcHeight(width:CGFloat, item:Any) -&gt; CGFloat { return TestCell.getDynamicHeight(of: width, item: item as! TestItem) }} func initialize(nowVC: UIViewController) 초기화 하는 함수입니다 여기서 columnNum은 테이블의 열의 갯수 sectionInset은 셀과 바깥 테이블과의 간격 itemSpacing은 셀 사이의 간격을 나타냅니다 func setItem(cell: UICollectionViewCell, item: Any) -&gt; UICollectionViewCell 주어진 item을 가지고 주어진 cell에 데이터를 넣습니다 func loadMoreItems(page: Int, callback: @escaping ([Any]) -&gt; Void) page가 주어지면 이 page가지고 Item을 만들어서 callback함수를 실행시킵니다 func getNibName() -&gt; String 이 테이블에 사용하는 xib파일의 이름을 반환해 주는 함수 입니다. func didSelected(item: Any) 만약에 셀이 클릭되었을 때 실행되는 함수입니다. func calcHeight(width:CGFloat, item:Any) -&gt; CGFloat 주어진 width와 item이 있을 때 셀의 높이를 반환해 주는 함수입니다. ViewController에서 사용하기이제 이 셀들을 보여주는 TableView 가 필요합니다. Main.storyboard에 적당히 색을 칠한 view를 넣고 CustomClass를 TestTableView로 합니다. 123456789101112131415161718import UIKitclass ViewController: UIViewController { @IBOutlet weak var testTableView: TestTableView! override func viewDidLoad() { super.viewDidLoad() // Do any additional setup after loading the view, typically from a nib. testTableView.initialize(nowVC: self) } override func didReceiveMemoryWarning() { super.didReceiveMemoryWarning() // Dispose of any resources that can be recreated. }} ViewController파일에 위와 같이 하고 testTabelView와 storyboard상의 그 것과 연결해 줍니다 결과 columnNum이 2일 경우 columnNum이 1일 경우","link":"/iOS/How-to-Create-Paging-Table-View/"},{"title":"iOS에서 Naver Clova Speech Synthesis(CSS) 사용하기","text":"iOS에서 Naver Clova Speech Synthesis(CSS) 사용해서 audio를 text로 바꾸는 방법을 알아보자 CSSTest라는 이름의 프로젝트를 만듭니다. 여기서 Bundle Identifier는 뒤에서 쓰이니 기억해둡시다. 네이버 개발자 센터에 가서 새로운 어플리케이션을 등록하는 페이지로 갑니다. 휴대폰 인증을 하고 회사 이름은 아무거나 적어줍시다. 어플리케이션 이름은 아무거나 적어도 상관 없습니다. 저는 CSSTest 라고 적었습니다. 그리고 사용 API에 Clova Speech Synthesis를 선택하고 iOS환경을 추가해서 1번의 Bundle Identifier를 적습니다. 그러고 나면 이렇게 Client ID와 Client Secret이 보이는 페이지로 넘어갑니다. 이 두 개는 뒤에서 쓰이니 기억해둡시다. 12345678910111213141516171819202122232425262728293031323334353637383940import UIKitimport Alamofireimport AVFoundationclass ViewController: UIViewController { let URL = &quot;https://openapi.naver.com/v1/voice/tts.bin&quot; let headers: HTTPHeaders = [ &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded; charset=UTF-8&quot;, &quot;X-Naver-Client-Id&quot;: &quot;아까 봤던 Client ID&quot;, &quot;X-Naver-Client-Secret&quot;: &quot;아까 봤던 Client Secret&quot; ] let parameters: Parameters = [ &quot;speaker&quot;: &quot;clara&quot;, &quot;speed&quot;: 0, &quot;text&quot;: &quot;hello, world.&quot; ] var player = AVPlayer() override func viewDidLoad() { super.viewDidLoad() Alamofire.request(URL, method: .post, parameters: parameters, headers: headers).response { response in guard let data = response.data as NSData? else { return } let audioFileURL = self.save(data: data, fileName: &quot;css.mp3&quot;) let playerItem = AVPlayerItem(url: audioFileURL as URL) self.player = AVPlayer(playerItem:playerItem) self.player.play() } } func save(data: NSData, fileName: String) -&gt; NSURL { let path = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true)[0] as String let fullPath:String = (path as NSString).appendingPathComponent(fileName) data.write(toFile: fullPath, atomically: true) return NSURL(fileURLWithPath: fullPath) }} 아까 만들었던 프로젝트의 ViewController에 위와 같이 적어줍시다. X-Naver-Client-Id와 X-Naver-Client-Secret에 위에서 봤던 Client ID와 Client Secret을 각각 적어줍니다. 저는 Alamofire를 이용해서 mp3파일을 받아왔습니다. 다른 방법을 사용해도 상관 없습니다. 위 코드는 받은 mp3파일을 css.mp3로 앱 내 documentDirectory에 저장 후 재생하는 코드입니다. 이제 앱을 실행하면 “hello, world” 라는 clara의 목소리가 들릴 것입니다.","link":"/iOS/Naver-Clova-Speech-Synthesis/"},{"title":"웹에 있는 오디오 재생하기","text":"웹에 있는 오디오를 재생하는 방법을 알아보자 프로젝트의 Info.plist에 App Transport Security Settings를 추가합니다. 그리고 그 안에 Allow Arbitrary Loads를 추가하고 그 값을 YES로 바꿔줍니다. 12345678910111213import UIKitimport AVFoundationclass ViewController: UIViewController { var player = AVPlayer() override func viewDidLoad() { let url = &quot;http://radio.spainmedia.es/wp-content/uploads/2015/12/tailtoddle_lo4.mp3&quot; let playerItem = AVPlayerItem(url: NSURL(string: url)! as URL) player = AVPlayer(playerItem: playerItem) player.play() }} 위와 같이 하면 저 URL의 mp3파일이 재생됩니다.","link":"/iOS/Play-Audio-From-Web/"}],"tags":[{"name":"bitcoin","slug":"bitcoin","link":"/tags/bitcoin/"},{"name":"blockchain","slug":"blockchain","link":"/tags/blockchain/"},{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","link":"/tags/hdfs/"},{"name":"aws","slug":"aws","link":"/tags/aws/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"android","slug":"android","link":"/tags/android/"},{"name":"kotlin","slug":"kotlin","link":"/tags/kotlin/"},{"name":"firebase","slug":"firebase","link":"/tags/firebase/"},{"name":"fcm","slug":"fcm","link":"/tags/fcm/"},{"name":"naver","slug":"naver","link":"/tags/naver/"},{"name":"map","slug":"map","link":"/tags/map/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"node","slug":"node","link":"/tags/node/"},{"name":"sse","slug":"sse","link":"/tags/sse/"},{"name":"npm","slug":"npm","link":"/tags/npm/"},{"name":"eslint","slug":"eslint","link":"/tags/eslint/"},{"name":"typescript","slug":"typescript","link":"/tags/typescript/"},{"name":"morgan","slug":"morgan","link":"/tags/morgan/"},{"name":"winston","slug":"winston","link":"/tags/winston/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"webpack","slug":"webpack","link":"/tags/webpack/"},{"name":"babels","slug":"babels","link":"/tags/babels/"},{"name":"bash","slug":"bash","link":"/tags/bash/"},{"name":"cloud9","slug":"cloud9","link":"/tags/cloud9/"},{"name":"server","slug":"server","link":"/tags/server/"},{"name":"vscode","slug":"vscode","link":"/tags/vscode/"},{"name":"code-server","slug":"code-server","link":"/tags/code-server/"},{"name":"zsh","slug":"zsh","link":"/tags/zsh/"},{"name":"terminal","slug":"terminal","link":"/tags/terminal/"},{"name":"ohmyzsh","slug":"ohmyzsh","link":"/tags/ohmyzsh/"},{"name":"ghostty","slug":"ghostty","link":"/tags/ghostty/"},{"name":"ec2","slug":"ec2","link":"/tags/ec2/"},{"name":"proxy","slug":"proxy","link":"/tags/proxy/"},{"name":"ecs","slug":"ecs","link":"/tags/ecs/"},{"name":"elk","slug":"elk","link":"/tags/elk/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"letsencrypt","slug":"letsencrypt","link":"/tags/letsencrypt/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"lambda","slug":"lambda","link":"/tags/lambda/"},{"name":"apex","slug":"apex","link":"/tags/apex/"},{"name":"charset","slug":"charset","link":"/tags/charset/"},{"name":"smtp","slug":"smtp","link":"/tags/smtp/"},{"name":"postfix","slug":"postfix","link":"/tags/postfix/"},{"name":"dkim","slug":"dkim","link":"/tags/dkim/"},{"name":"spring-boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"graphql","slug":"graphql","link":"/tags/graphql/"},{"name":"jwt","slug":"jwt","link":"/tags/jwt/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"tomcat","slug":"tomcat","link":"/tags/tomcat/"},{"name":"websocket","slug":"websocket","link":"/tags/websocket/"},{"name":"chrome","slug":"chrome","link":"/tags/chrome/"},{"name":"tip","slug":"tip","link":"/tags/tip/"},{"name":"slack","slug":"slack","link":"/tags/slack/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"swift","slug":"swift","link":"/tags/swift/"},{"name":"cocoapod","slug":"cocoapod","link":"/tags/cocoapod/"},{"name":"ios","slug":"ios","link":"/tags/ios/"},{"name":"facebook","slug":"facebook","link":"/tags/facebook/"},{"name":"view","slug":"view","link":"/tags/view/"},{"name":"uitableview","slug":"uitableview","link":"/tags/uitableview/"},{"name":"clova-speech-synthesis","slug":"clova-speech-synthesis","link":"/tags/clova-speech-synthesis/"},{"name":"audio","slug":"audio","link":"/tags/audio/"}],"categories":[{"name":"Blockchain","slug":"Blockchain","link":"/categories/Blockchain/"},{"name":"Hadoop","slug":"Hadoop","link":"/categories/Hadoop/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"Android","slug":"Android","link":"/categories/Android/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Node","slug":"Node","link":"/categories/Node/"},{"name":"Server","slug":"Server","link":"/categories/Server/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"기타","slug":"기타","link":"/categories/%EA%B8%B0%ED%83%80/"},{"name":"CI&#x2F;CD","slug":"CI-CD","link":"/categories/CI-CD/"},{"name":"iOS","slug":"iOS","link":"/categories/iOS/"}]}